<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[Generative Adversarial Networks]]></title>
      <url>http://buptldy.github.io/2016/11/27/2016-11-27-gans/</url>
      <content type="html"><![CDATA[<p><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-27/13513902.jpg" alt=""><br><a id="more"></a></p>
<p>人工智能目前的核心目标应该是赋予机器自主理解我们所在世界的能力。对于人类来说，我们对这个世界所了解的知识可能很快就会忘记，比如我们所处的三维环境中，物体能够交互，移动，碰撞；什么动物会飞，什么动物吃草等等。这些巨大的并且不断扩大的信息现在是很容易被机器获取的，问题的关键是怎么设计模型和算法让机器更好的去分析和理解这些数据中所蕴含的宝藏。</p>
<p><code>Generative models</code>(生成模型)现在被认为是能够实现这一目标的最有前景的方法之一。<code>Generative models</code>通过输入一大堆特定领域的数据进行训练（比如图像，句子，声音等）来使得模型能够产生和输入数据相似的输出。这一直觉的背后可以由下面名言阐述。</p>
<blockquote>
<p>“What I cannot create, I do not understand.”<br>—Richard Feynman</p>
</blockquote>
<p>生成模型由一个参数数量比训练数据少的多神经网络构成，所以生成模型为了能够产生和训练数据相似的输出就会迫使自己去发现数据中内在的本质内容。训练<code>Generative models</code>的方法有几种，在这里我们主要阐述其中的<code>Adversarial Training</code>（对抗训练）方法。</p>
<h2 id="Adversarial-Training"><a href="#Adversarial-Training" class="headerlink" title="Adversarial Training"></a>Adversarial Training</h2><p>上文说过Adversarial Training是训练生成模型的一种方法。为了训练生成模型，Adversarial Training提出一种<code>Discriminative Model</code>(判别模型)来和生成模型产生对抗，下面来说说<code>Generative models</code> $G(z)$ 和 <code>Discriminative Model</code> $D(x)$ 是如何相互作用的。</p>
<ul>
<li>生成模型的目标是模仿输入训练数据, 通过输入一个随机噪声来产生和训练数据相似的样本；</li>
<li>判别模型的目标就是判断生成模型产生的样本和真实的输入样本之间的相似性。</li>
</ul>
<p>其中生成模型和判别模型合起来的框架被称为<code>GAN</code>网络。通过下图我们来理清判别模型和生成模型之间的输入输出关系：生成模型通过输入随机噪声 $z(z \sim p_z)$ 产生合成样本；而判别模型通过分别输入真实的训练数据和生成模型的训练数据来判断输入的数据是否真实。</p>
<center><br><img src="https://culurciello.github.io/assets/unsup/gan_simple.svg" alt=""><br></center>

<p>描述了<code>GAN</code>的网络结构，但它的优化目标是什么？怎么就可以通过训练使得生成模型能够产生和真实数据相似的输出？优化的目标其实很简单，简单来说就是：</p>
<ul>
<li>判别模型努力的想把真实的数据预测为<code>1</code>，把生成的数据预测为<code>0</code>；</li>
<li>而生成模型的奋斗目标则为‘我’要尽力的让判别模型对‘我’生成的数据预测为<code>1</code>，让判别模型分不清‘我’产生的数据和真实数据之间的区别，从而达到‘以假乱真’的效果。</li>
</ul>
<p>下面用形式化说明下如果训练GAN网络, 先定义一些参数：</p>
<table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$p_z$</td>
<td style="text-align:center">输入随机噪声 $z$ 的分布</td>
</tr>
<tr>
<td style="text-align:center">$p_{data}$</td>
<td style="text-align:center">未知的输入样本的数据分布</td>
</tr>
<tr>
<td style="text-align:center">$p_g$</td>
<td style="text-align:center">生成模型的输出样本的数据分布，GAN的目标就是要$p_g=p_{data}$</td>
</tr>
</tbody>
</table>
<p>训练判别模型 $D(x)$ 的目标：</p>
<ol>
<li>对每一个输入数据 $x \sim p_{data}$ 要使得 $D(x)$ 最大；</li>
<li>对每一个输入数据 $x \nsim p_{data}$ 要使得 $D(x)$ 最小。</li>
</ol>
<p>训练生成模型 $G(z)$ 的目标是来产生样本来欺骗判别模型 $D$, 因此目标为最大化 $D(G(z))$，也就是把生成模型的输出输入到判别模型，然后要让判别模型预测其为真实数据。同时，最大化 $D(G(z))$ 等同于最小化 $1-D(G(z))$，因为 $D$ 的输出是介于0到1之间的，真实数据努力预测为1，否则为0。</p>
<p>所以把生成模型和判别模型的训练目标结合起来，就得到了<code>GAN</code>的优化目标：</p>
<p>$$\min_G \max_D {\mathbb E}_{x\sim p_{\rm data}} \log D(x)+{\mathbb E}_{z\sim p_z}[\log (1-D(G(z)))] $$</p>
<p>总结一下上面的内容，GAN启发自博弈论中的二人零和博弈，在二人零和博弈中，两位博弈方的利益之和为零或一个常数，即一方有所得，另一方必有所失。GAN模型中的两位博弈方分别由生成模型和判别模型充当。生成模型G捕捉样本数据的分布，判别模型是一个二分类器，估计一个样本来自于训练数据（而非生成数据）的概率。G和D一般都是非线性映射函数，例如多层感知机、卷积神经网络等。生成模型的输入是一些服从某一简单分布（例如高斯分布）的随机噪声z，输出是与训练图像相同尺寸的生成图像。向判别模型D输入生成样本，对于D来说期望输出低概率（判断为生成样本），对于生成模型G来说要尽量欺骗D，使判别模型输出高概率（误判为真实样本），从而形成竞争与对抗。</p>
<h2 id="GAN实现"><a href="#GAN实现" class="headerlink" title="GAN实现"></a>GAN实现</h2><p>一个简单的一维数据GAN网络的tensorflow实现:<a href="https://github.com/ericjang/genadv_tutorial" target="_blank" rel="external">genadv_tutorial</a><br>其一维训练数据分布如下所示，是一个均值-1， $\sigma =1$ 的正态分布。</p>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-24/4360345.jpg" alt=""><br></center>

<p>我们结合代码和上面的理论内容来分析下GAN的具体实现，判别模型的优化目标为最大化下式，其中 $D_1(x)$ 表示判别真实数据, $D_2(G(z))$ 表示对生成的数据进行判别， 其中 $D_1$ 和 $D_2$ 是共享参数的， 也就是说是同一个判别模型。</p>
<p>$$\log(D_1(x))+\log(1-D_2(G(z)))$$</p>
<p>对应的python代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch=tf.Variable(<span class="number">0</span>)</span><br><span class="line">obj_d=tf.reduce_mean(tf.log(D1)+tf.log(<span class="number">1</span>-D2))</span><br><span class="line">opt_d=tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</span><br><span class="line">              .minimize(<span class="number">1</span>-obj_d,global_step=batch,var_list=theta_d)</span><br></pre></td></tr></table></figure></p>
<p>为了优化 $G$, 我们想要最大化 $D_2(x’)$(成功欺骗 $D$ )，因此 $G$ 的优化函数为：</p>
<p>$$\log(D_2(G(z)))$$</p>
<p>对应的python代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch=tf.Variable(<span class="number">0</span>)</span><br><span class="line">obj_g=tf.reduce_mean(tf.log(D2))</span><br><span class="line">opt_g=tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</span><br><span class="line">              .minimize(<span class="number">1</span>-obj_g,global_step=batch,var_list=theta_g)</span><br></pre></td></tr></table></figure>
<p>定义好优化目标后，下面就是训练的主要代码了：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Algorithm 1, GoodFellow et al. 2014</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(TRAIN_ITERS):</span><br><span class="line">    x= np.random.normal(mu,sigma,M) <span class="comment"># sample minibatch from p_data</span></span><br><span class="line">    z= np.random.random(M)  <span class="comment"># sample minibatch from noise prior</span></span><br><span class="line">    sess.run(opt_d, &#123;x_node: x, z_node: z&#125;) <span class="comment"># update discriminator D</span></span><br><span class="line">    z= np.random.random(M) <span class="comment"># sample noise prior</span></span><br><span class="line">    sess.run(opt_g, &#123;z_node: z&#125;) <span class="comment"># update generator G</span></span><br></pre></td></tr></table></figure></p>
<p>下面是实验的结果，左图是训练之间的数据，可以看到生成数据的分布和训练数据相差甚远；右图是训练后的数据分析，生成数据和训练数据的分布接近了很多，且此时判别模型的输出分布在0.5左右，说明生成模型顺利的欺骗到判别模型。</p>
<p><div></div></p>
<p><figure class="half"><br>    <img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-24/44877660.jpg" width="300"><br>    <img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-24/72678087.jpg" width="300"><br></figure><br></p>
<h2 id="DCGAN"><a href="#DCGAN" class="headerlink" title="DCGAN"></a>DCGAN</h2><p>GAN的一个改进模型就是DCGAN。这个网络的生成模型的输入为一个100个符合均匀分布的随机数（通常被称为<code>code</code>），然后产生输出为64x64x3的输出图像(下图中 $G(z)$ ), 当<code>code</code>逐渐递增时，生成模型输出的图像也逐渐变化。下图中的生产模型主要由<a href="http://buptldy.github.io/2016/10/29/2016-10-29-deconv/">反卷积层</a>构成, 判别模型就由简单的卷积层组成，最后输出一个判断输入图片是否为真实数据的概率 $P(x)$ 。</p>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-27/33141448.jpg"><br></center>

<p>下图为随着迭代次数，DCGAN产生图像的变化过程。</p>
<center><br><img src="https://openai.com/assets/research/generative-models/learning-gan-ffc4c09e6079283f334b2485ae663a6587d937a45ebc1d8aeac23a67889a3cf5.gif"><br></center>

<p>训练好网络之后，其中的生成模型和判别模型都有其他的作用。一个训练好的判别模型能够用来对数据提取特征然后进行分类任务。通过输入随机向量生成模型可以产生一些非常有意思的的图片，如下图所示，当输入空间平滑变化时，输出的图片也在平滑转变。</p>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-25/42718244.jpg"><br></center>

<p>还有一个非常有意思的属性就是如果对生产模型的输入向量做一些简单的数学运算，那么学习的特征输出也有同样的性质，如下图所示。</p>
<center><br><img src="https://fb-s-a-a.akamaihd.net/h-ak-xfp1/t39.2365-6/13438466_275356996149902_2140145659_n.jpg"><br></center>

<h2 id="GAN的训练及其改进"><a href="#GAN的训练及其改进" class="headerlink" title="GAN的训练及其改进"></a>GAN的训练及其改进</h2><p>上面使用GAN产生的图像虽然效果不错，但其实GAN网络的训练过程是非常不稳定的。<br>通常在实际训练GAN中所碰到的一个问题就是判别模型的收敛速度要比生成模型的收敛速度要快很多，通常的做法就是让生成模型多训练几次来赶上生成模型，但是存在的一个问题就是通常生成模型和判别模型的训练是相辅相成的，理想的状态是让生成模型和判别模型在每次的训练过程中同时变得更好。判别模型理想的minimum loss应该为0.5，这样才说明判别模型分不出是真实数据还是生成模型产生的数据。</p>
<h3 id="Improved-GANs"><a href="#Improved-GANs" class="headerlink" title="Improved GANs"></a>Improved GANs</h3><p><a href="https://arxiv.org/pdf/1606.03498v1.pdf" target="_blank" rel="external">Improved techniques for training GANs</a>这篇文章提出了很多改进GANs训练的方法，其中提出一个想法叫<code>Feature matching</code>，之前判别模型只判别输入数据是来自真实数据还是生成模型。现在为判别模型提出了一个新的目标函数来判别生成模型产生图像的统计信息是否和真实数据的相似。让 $f(x)$ 表示判别模型中间层的输出， 新的目标函数被定义为 $|| \mathbb{E}_{x \sim p_{data}}f(x)  -  \mathbb{E}_{z \sim p_z}f(G(z))||^2_2$, 其实就是要求真实图像和合成图像在判别模型中间层的距离要最小。这样可以防止生成模型在当前判别模型上过拟合。</p>
<h3 id="InfoGAN"><a href="#InfoGAN" class="headerlink" title="InfoGAN"></a>InfoGAN</h3><p>到这可能有些同学会想到，我要是想通过GAN产生我想要的特定属性的图片改怎么办？普通的GAN输入的是随机的噪声，输出也是与之对应的随机图片，我们并不能控制输出噪声和输出图片的对应关系。这样在训练的过程中也会倒置生成模型倾向于产生更容易欺骗判别模型的某一类特定图片，而不是更好的去学习训练数据的分布，这样对模型的训练肯定是不好的。InfoGAN的提出就是为了解决这一问题，通过对输入噪声添加一些类别信息以及控制图像特征(如mnist数字的角度和厚度)的隐含变量来使得生成模型的输入不在是随机噪声。虽然现在输入不再是随机噪声，但是生成模型可能会忽略这些输入的额外信息还是把输入当成和输出无关的噪声，所以需要定义一个生成模型输入输出的互信息，互信息越高，说明输入输出的关联越大。</p>
<p>下面三张图片展示了通过分别控制输入噪声的类别信息，数字角度信息，数字笔画厚度信息产生指定输出的图片，可以看出InfoGAN产生图片的效果还是很好的。</p>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-29/839516.jpg"><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-29/10937636.jpg"><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-29/2995738.jpg"><br></center>

<h3 id="其他应用"><a href="#其他应用" class="headerlink" title="其他应用"></a>其他应用</h3><p>GAN网络还有很多其他的有趣应用，比如下图所示的根据<code>一句话来产生对应的图片</code>，可能大家都有了解karpathy大神的<a href="https://github.com/karpathy/neuraltalk2" target="_blank" rel="external"><code>看图说话</code></a>, 但是GAN有能力把这个过程给反过来。</p>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-29/51572272.jpg"><br></center>

<p>还有下面这个“<a href="https://github.com/bamos/dcgan-completion.tensorflow" target="_blank" rel="external">图像补全</a>”, 根据图像剩余的信息来匹配最佳的补全内容。</p>
<center><br><img src="https://github.com/bamos/dcgan-completion.tensorflow/raw/master/completion.compressed.gif"><br></center>

<p>还有下面这个<a href="https://swarbrickjones.wordpress.com/2016/01/13/enhancing-images-using-deep-convolutional-generative-adversarial-networks-dcgans/" target="_blank" rel="external">图像增强</a>的例子，有点去马赛克的意思，效果还是挺不错的:-D。</p>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/public/16-11-29/71438836.jpg
"><br></center>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>颜乐存说过，2016年深度学习领域最让他兴奋技术莫过于对抗学习。对抗学习确实是解决非监督学习的一个有效方法，而无监督学习一直都是人工智能领域研究者所孜孜追求的“终极目标”之一。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="external">Generative Adversarial Networks</a></p>
<p><a href="https://arxiv.org/abs/1511.06434" target="_blank" rel="external">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a></p>
<p><a href="https://arxiv.org/abs/1606.03498" target="_blank" rel="external">Improved Techniques for Training GANs</a></p>
<p><a href="https://arxiv.org/abs/1606.03657" target="_blank" rel="external">InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Transposed Convolution, Fractionally Strided Convolution or Deconvolution]]></title>
      <url>http://buptldy.github.io/2016/10/29/2016-10-29-deconv/</url>
      <content type="html"><![CDATA[<center><br><img src="http://ww3.sinaimg.cn/large/8c2b2f6fjw1f9ac3snmk6j21130a7wj5.jpg"><br></center><br><a id="more"></a><br><br>反卷积（Deconvolution）的概念第一次出现是Zeiler在2010年发表的论文<a href="http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf" target="_blank" rel="external">Deconvolutional networks</a>中，但是并没有指定反卷积这个名字，反卷积这个术语正式的使用是在其之后的工作中(<a href="https://www.cs.nyu.edu/~gwtaylor/publications/zeilertaylorfergus_iccv2011.pdf" target="_blank" rel="external">Adaptive deconvolutional networks for mid and high level feature learning</a>)。随着反卷积在神经网络可视化上的成功应用，其被越来越多的工作所采纳比如：场景分割、生成模型等。其中反卷积（Deconvolution）也有很多其他的叫法，比如：Transposed Convolution，Fractional Strided Convolution等等。<br><br>这篇文章的目的主要有两方面：<br>1. 解释卷积层和反卷积层之间的关系；<br>2. 弄清楚反卷积层输入特征大小和输出特征大小之间的关系。<br><br>## 卷积层<br><br>卷积层大家应该都很熟悉了,为了方便说明，定义如下：<br>- 二维的离散卷积（$N = 2$）<br>- 方形的特征输入（$i_1 = i_2 = i$）<br>- 方形的卷积核尺寸（$k_1 = k_2 = k$）<br>- 每个维度相同的步长（$s_1 = s_2 = s$）<br>- 每个维度相同的padding ($p_1 = p_2 = p$)<br><br>下图表示参数为 $(i=5,k=3,s=2,p=1)$ 的卷积计算过程，从计算结果可以看出输出特征的尺寸为 $(o_1 = o_2 = o = 3)$。<br><center><br>    <img src="http://ww2.sinaimg.cn/large/8c2b2f6fjw1f99h94gnlbg20az0aldi4.gif" width="300"><br></center>

<p>下图表示参数为 $(i=6,k=3,s=2,p=1)$ 的卷积计算过程，从计算结果可以看出输出特征的尺寸为 $(o_1 = o_2 = o = 3)$。</p>
<center><br>    <img src="http://ww4.sinaimg.cn/large/8c2b2f6fjw1f99h9jeg5ig20cd0bf0v5.gif" width="300"><br></center>

<p>从上述两个例子我们可以总结出卷积层输入特征与输出特征尺寸和卷积核参数的关系为：<br>$$o = \left\lfloor \frac{i + 2p - k}{s} \right\rfloor + 1.$$<br>其中 $\lfloor x \rfloor$ 表示对 $x$ 向下取整。</p>
<h2 id="反卷积层"><a href="#反卷积层" class="headerlink" title="反卷积层"></a>反卷积层</h2><p>在介绍反卷积之前，我们先来看看卷积运算和矩阵运算之间的关系。</p>
<h3 id="卷积和矩阵相乘"><a href="#卷积和矩阵相乘" class="headerlink" title="卷积和矩阵相乘"></a>卷积和矩阵相乘</h3><p>考虑如下一个简单的卷积层运算，其参数为 $(i=4,k=3,s=1,p=0)$，输出 $o=2$。</p>
<center><br><img src="http://ww3.sinaimg.cn/large/8c2b2f6fjw1f99i1rv4jog206s0770t7.gif" width="300"><br></center>

<p>对于上述卷积运算，我们把上图所示的3×3卷积核展成一个如下所示的[4,16]的稀疏矩阵 $\mathbf{C}$， 其中非0元素 $w_{i,j}$ 表示卷积核的第 $i$ 行和第 $j$ 列。</p>
<p>\begin{pmatrix}<br>    w_{0,0} &amp; w_{0,1} &amp; w_{0,2} &amp; 0       &amp; w_{1,0} &amp; w_{1,1} &amp; w_{1,2} &amp; 0       &amp;<br>    w_{2,0} &amp; w_{2,1} &amp; w_{2,2} &amp; 0       &amp; 0       &amp; 0       &amp; 0       &amp; 0       \\<br>    0       &amp; w_{0,0} &amp; w_{0,1} &amp; w_{0,2} &amp; 0       &amp; w_{1,0} &amp; w_{1,1} &amp; w_{1,2} &amp;<br>    0       &amp; w_{2,0} &amp; w_{2,1} &amp; w_{2,2} &amp; 0       &amp; 0       &amp; 0       &amp; 0       \\<br>    0       &amp; 0       &amp; 0       &amp; 0       &amp; w_{0,0} &amp; w_{0,1} &amp; w_{0,2} &amp; 0       &amp;<br>    w_{1,0} &amp; w_{1,1} &amp; w_{1,2} &amp; 0       &amp; w_{2,0} &amp; w_{2,1} &amp; w_{2,2} &amp; 0       \\<br>    0       &amp; 0       &amp; 0       &amp; 0       &amp; 0       &amp; w_{0,0} &amp; w_{0,1} &amp; w_{0,2} &amp;<br>    0       &amp; w_{1,0} &amp; w_{1,1} &amp; w_{1,2} &amp; 0       &amp; w_{2,0} &amp; w_{2,1} &amp; w_{2,2} \\<br>\end{pmatrix}</p>
<p>我们再把4×4的输入特征展成[16,1]的矩阵 $\mathbf{X}$，那么 $\mathbf{Y = CX}$ 则是一个[4,1]的输出特征矩阵，把它重新排列2×2的输出特征就得到最终的结果，从上述分析可以看出卷积层的计算其实是可以转化成矩阵相乘的。值得注意的是，在一些深度学习网络的开源框架中并不是通过这种这个转换方法来计算卷积的，因为这个转换会存在很多无用的0乘操作，Caffe中具体实现卷积计算的方法可参考<a href="http://buptldy.github.io/2016/10/01/2016-10-01-im2col/">Implementing convolution as a matrix multiplication</a>。</p>
<p>通过上述的分析，我们已经知道卷积层的前向操作可以表示为和矩阵$\mathbf{C}$相乘，那么 <strong>我们很容易得到卷积层的反向传播就是和$\mathbf{C}$的转置相乘</strong>。</p>
<h3 id="反卷积和卷积的关系"><a href="#反卷积和卷积的关系" class="headerlink" title="反卷积和卷积的关系"></a>反卷积和卷积的关系</h3><p>全面我们已经说过反卷积又被称为Transposed(转置) Convolution，我们可以看出其实卷积层的前向传播过程就是反卷积层的反向传播过程，卷积层的反向传播过程就是反卷积层的前向传播过程。因为卷积层的前向反向计算分别为乘 $\mathbf{C}$ 和 $\mathbf{C^T}$,而反卷积层的前向反向计算分别为乘 $\mathbf{C^T}$ 和 $\mathbf{(C^T)^T}$ ，所以它们的前向传播和反向传播刚好交换过来。</p>
<p>下图表示一个和上图卷积计算对应的反卷积操作，其中他们的输入输出关系正好相反。如果不考虑通道以卷积运算的反向运算来计算反卷积运算的话，我们还可以通过离散卷积的方法来求反卷积（这里只是为了说明，实际工作中不会这么做）。</p>
<p>同样为了说明，定义反卷积操作参数如下：</p>
<ul>
<li>二维的离散卷积（$N = 2$）</li>
<li>方形的特征输入（$i’_1 = i’_2 = i’$）</li>
<li>方形的卷积核尺寸（$k’_1 = k’_2 = k’$）</li>
<li>每个维度相同的步长（$s’_1 = s’_2 = s’$）</li>
<li>每个维度相同的padding ($p’_1 = p’_2 = p’$)</li>
</ul>
<p>下图表示的是参数为( $i’=2,k’=3,s’=1,p’=2$)的反卷积操作，其对应的卷积操作参数为 $(i=4,k=3,s=1,p=0)$。我们可以发现对应的卷积和非卷积操作其 $(k=k’,s=s’)$，但是反卷积却多了$p’=2$。通过对比我们可以发现卷积层中左上角的输入只对左上角的输出有贡献，所以反卷积层会出现 $p’=k-p-1=2$。通过示意图，我们可以发现，反卷积层的输入输出在 $s=s’=1$ 的情况下关系为：</p>
<p>$$o’=i’-k’+2p’+1=i’+(k-1)-2p$$</p>
<center><br><img src="http://ww4.sinaimg.cn/large/8c2b2f6fjw1f99j2k89hlg209k0aq41i.gif" width="300"><br></center>

<h3 id="Fractionally-Strided-Convolution"><a href="#Fractionally-Strided-Convolution" class="headerlink" title="Fractionally Strided Convolution"></a>Fractionally Strided Convolution</h3><p>上面也提到过反卷积有时候也被叫做Fractionally Strided Convolution，翻译过来大概意思就是小数步长的卷积。对于步长 $s&gt;1$的卷积，我们可能会想到其对应的反卷积步长 $s’&lt;1$。 如下图所示为一个参数为 $i = 5, k = 3, s = 2 , p = 1$的卷积操作(就是第一张图所演示的)所对应的反卷积操作。对于反卷积操作的小数步长我们可以理解为：在其输入特征单元之间插入 $s-1$ 个0，插入0后把其看出是新的特征输入，然后此时步长 $s’$ 不再是小数而是为1。因此，结合上面所得到的结论，我们可以得出Fractionally Strided Convolution的输入输出关系为：</p>
<p>$$ o’ = s(i’ −1)+k −2p$$</p>
<center><br><img src="http://ww1.sinaimg.cn/large/8c2b2f6fjw1f9aba7bh2ig20az0chwjq.gif" width="300"><br></center>

<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank" rel="external">conv_arithmetic</a></p>
<p><a href="https://arxiv.org/abs/1609.07009" target="_blank" rel="external">Is the deconvolution layer the same as a convolutional layer?</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Caffe Source Code Analysis]]></title>
      <url>http://buptldy.github.io/2016/10/09/2016-10-09-Caffe_Code/</url>
      <content type="html"><![CDATA[<p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-10-9/83591184.jpg" alt=""><br><a id="more"></a></p>
<h2 id="Caffe简介"><a href="#Caffe简介" class="headerlink" title="Caffe简介"></a>Caffe简介</h2><p><a href="http://caffe.berkeleyvision.org/" target="_blank" rel="external">Caffe</a>作为一个优秀的深度学习框架网上已经有很多内容介绍了，这里就不在多说。作为一个C++新手，断断续续看Caffe源码一个月以来发现越看不懂的东西越多，因此在博客里记录和分享一下学习的过程。其中我把自己看源码的一些注释结合了网上一些同学的注释以及在学习源码过程中查到到的一些资源(包括怎么使用IDE单步调试以及一些Caffe中使用的第三方库的介绍)放在github上：<a href="https://github.com/BUPTLdy/Caffe_Code_Analysis" target="_blank" rel="external">Caffe_Code_Analysis</a>，感兴趣的同学可以看一看，希望能对你有帮助。</p>
<p>一般在介绍Caffe代码结构的时候，大家都会说Caffe主要由<code>Blob</code> <code>Layer</code> <code>Net</code> 和 <code>Solver</code>这几个部分组成。</p>
<ul>
<li>Blob 主要用来表示网络中的数据，包括训练数据，网络各层自身的参数(包括权值、偏置以及它们的梯度)，网络之间传递的数据都是通过 Blob 来实现的，同时 Blob 数据也支持在 CPU 与 GPU 上存储，能够在两者之间做同步。</li>
<li>Layer 是对神经网络中各种层的一个抽象，包括我们熟知的卷积层和下采样层，还有全连接层和各种激活函数层等等。同时每种 Layer 都实现了前向传播和反向传播，并通过 Blob 来传递数据。</li>
<li>Net 是对整个网络的表示，由各种 Layer 前后连接组合而成，也是我们所构建的网络模型。</li>
<li>Solver 定义了针对 Net 网络模型的求解方法，记录网络的训练过程，保存网络模型参数，中断并恢复网络的训练过程。自定义 Solver 能够实现不同的网络求解方式。</li>
</ul>
<p>不过在刚开始准备阅读Caffe代码的时候，就算知道了代码是由上面四部分组成还是感觉会无从下手，下面我们准备通过一个<a href="http://caffe.berkeleyvision.org/gathered/examples/mnist.html" target="_blank" rel="external">Caffe训练LeNet</a>的实例并结合代码来解释Caffe是如何初始化网络，然后正向传播、反向传播开始训练，最终得到训练好的模型这一过程。</p>
<h2 id="训练LeNet"><a href="#训练LeNet" class="headerlink" title="训练LeNet"></a>训练LeNet</h2><p>在Caffe提供的例子里，训练LeNet网络的命令为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $CAFFE_ROOT</span><br><span class="line">./build/tools/caffe train --solver=examples/mnist/lenet_solver.prototxt</span><br></pre></td></tr></table></figure></p>
<p>其中第一个参数<code>build/tools/caffe</code>是Caffe框架的主要框架，由<a href="https://github.com/BVLC/caffe/blob/master/tools/caffe.cpp" target="_blank" rel="external"><code>tools/caffe.cpp</code></a>文件编译而来，第二个参数<code>train</code>表示是要训练网络，第三个参数是 <a href="https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet_solver.prototxt" target="_blank" rel="external">solver的protobuf描述文件</a>。在Caffe中，网络模型的描述及其求解都是通过 <a href="https://developers.google.com/protocol-buffers/" target="_blank" rel="external">protobuf</a> 定义的，并不需要通过敲代码来实现。同时，模型的参数也是通过 protobuf 实现加载和存储，包括 CPU 与 GPU 之间的无缝切换，都是通过配置来实现的，不需要通过硬编码的方式实现，有关<br>protobuf的具体内容可参考这篇博文：<a href="http://alanse7en.github.io/caffedai-ma-jie-xi-2/" target="_blank" rel="external">http://alanse7en.github.io/caffedai-ma-jie-xi-2/</a>。</p>
<h2 id="网络初始化"><a href="#网络初始化" class="headerlink" title="网络初始化"></a>网络初始化</h2><p>下面我们从<code>caffe.cpp</code>的main函数入口开始观察Caffe是怎么一步一步训练网络的。在<code>caffe.cpp</code>中main函数之外通过<code>RegisterBrewFunction</code>这个宏在每一个实现主要功能的函数之后将这个函数的名字和其对应的函数指针添加到了<code>g_brew_map</code>中,具体分别为train()，test()，device_query()，time()这四个函数。</p>
<p>在运行的时候,根据传入的参数在main函数中，通过GetBrewFunction得到了我们需要调用的那个函数的函数指针，并完成了调用。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// caffe.cpp</span></span><br><span class="line"><span class="keyword">return</span> GetBrewFunction(caffe::<span class="built_in">string</span>(argv[<span class="number">1</span>])) ();</span><br></pre></td></tr></table></figure></p>
<p>在我们上面所说的训练LeNet的例子中，传入的第二个参数为<code>train</code>，所以调用的函数为<code>caffe.cpp</code>中的<code>int train()</code>函数，接下来主要看这个函数的内容。在<code>train</code>函数中有下面两行代码，下面的代码定义了一个指向Solver<float>的shared_ptr。其中主要是通过调用SolverRegistry这个类的静态成员函数CreateSolver得到一个指向Solver的指针来构造shared_ptr类型的solver。而且由于C++多态的特性，尽管solver是一个指向基类Solver类型的指针，通过solver这个智能指针来调用各个成员函数会调用到各个子类(SGDSolver等)的函数。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// caffe.cpp</span></span><br><span class="line"><span class="comment">// 其中输入参数solver_param就是上面所说的第三个参数：网络的模型及求解文件</span></span><br><span class="line"><span class="built_in">shared_ptr</span>&lt;caffe::Solver&lt;<span class="keyword">float</span>&gt; &gt;</span><br><span class="line">    solver(caffe::SolverRegistry&lt;<span class="keyword">float</span>&gt;::CreateSolver(solver_param);</span><br></pre></td></tr></table></figure></float></p>
<p>因为在<code>caffe.proto</code>文件中默认的优化<code>type</code>为<code>SGD</code>,所以上面的代码会实例化一个<code>SGDSolver</code>的对象，’SGDSolver’类继承于<code>Solver</code>类，在新建<code>SGDSolver</code>对象时会调用其构造函数如下所示：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">//sgd_solvers.hpp</span><br><span class="line">explicit SGDSolver(const SolverParameter&amp; param)</span><br><span class="line">    : Solver&lt;Dtype&gt;(param) &#123; PreSolve(); &#125;</span><br></pre></td></tr></table></figure></p>
<p>从上面代码可以看出，会先调用父类<code>Solver</code>的构造函数，如下所示。Solver类的构造函数通过<code>Init(param)</code>函数来初始化网络。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//solver.cpp</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line">Solver&lt;Dtype&gt;::Solver(<span class="keyword">const</span> SolverParameter&amp; param, <span class="keyword">const</span> Solver* root_solver)</span><br><span class="line">    : net_(), callbacks_(), root_solver_(root_solver),requested_early_exit_(<span class="literal">false</span>)</span><br><span class="line">&#123;</span><br><span class="line">  Init(param);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>而在<code>Init(paran)</code>函数中，又主要是通过<code>InitTrainNet()</code>和<code>InitTestNets()</code>函数分别来搭建训练网络结构和测试网络结构。</p>
<p>训练网络只能有一个,在<code>InitTrainNet()</code>函数中首先会设置一些基本参数，包括设置网络的状态为<code>TRAIN</code>，确定训练网络只有一个等，然会会通过下面这条语句新建了一个<code>Net</code>对象。<code>InitTestNets()</code>函数和<code>InitTrainNet()</code>函数基本类似，不再赘述。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//solver.cpp</span></span><br><span class="line">net_.reset(<span class="keyword">new</span> Net&lt;Dtype&gt;(net_param));</span><br></pre></td></tr></table></figure></p>
<p>上面语句新建了<code>Net</code>对象之后会调用<code>Net</code>类的构造函数，如下所示。可以看出构造函数是通过<code>Init(param)</code>函数来初始化网络结构的。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//net.cpp</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line">Net&lt;Dtype&gt;::Net(<span class="keyword">const</span> NetParameter&amp; param, <span class="keyword">const</span> Net* root_net)</span><br><span class="line">    : root_net_(root_net) &#123;</span><br><span class="line">  Init(param);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>下面是net.cpp文件里<code>Init()</code>函数的主要内容(忽略具体细节)，其中<code>LayerRegistry&lt;Dtype&gt;::CreateLayer(layer_param)</code>主要是通过调用LayerRegistry这个类的静态成员函数CreateLayer得到一个指向Layer类的shared_ptr类型指针。并把每一层的指针存放在<code>vector&lt;shared_ptr&lt;Layer&lt;Dtype&gt; &gt; &gt; layers_</code>这个指针容器里。这里相当于根据每层的参数<code>layer_param</code>实例化了对应的各个子类层，比如<code>conv_layer</code>(卷积层)和<code>pooling_layer</code>(池化层)。实例化了各层就会调用每个层的构造函数，但每层的构造函数都没有做什么大的设置。</p>
<p>接下来在Init()函数中主要由四部分组成：</p>
<ul>
<li><code>AppendBottom</code>：设置每一层的输入数据</li>
<li><code>AppendTop</code>：设置每一层的输出数据</li>
<li><code>layers_[layer_id]-&gt;SetUp</code>：对上面设置的输入输出数据计算分配空间，并设置每层的可学习参数(权值和偏置),下面会详细降到这个函数</li>
<li><code>AppendParam</code>：对上面申请的可学习参数进行设置，主要包括学习率和正则率等。<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//net.cpp Init()</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> layer_id = <span class="number">0</span>; layer_id &lt; param.layer_size(); ++layer_id) &#123;<span class="comment">//param是网络参数，layer_size()返回网络拥有的层数</span></span><br><span class="line">    <span class="keyword">const</span> LayerParameter&amp; layer_param = param.layer(layer_id);<span class="comment">//获取当前layer的参数</span></span><br><span class="line">    layers_.push_back(LayerRegistry&lt;Dtype&gt;::CreateLayer(layer_param));<span class="comment">//根据参数实例化layer</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//下面的两个for循环将此layer的bottom blob的指针和top blob的指针放入bottom_vecs_和top_vecs_,bottom blob和top blob的实例全都存放在blobs_中。相邻的两层，前一层的top blob是后一层的bottom blob，所以blobs_的同一个blob既可能是bottom blob，也可能使top blob。</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> bottom_id = <span class="number">0</span>; bottom_id &lt; layer_param.bottom_size();++bottom_id) &#123;</span><br><span class="line">       <span class="keyword">const</span> <span class="keyword">int</span> blob_id=AppendBottom(param,layer_id,bottom_id,&amp;available_blobs,&amp;blob_name_to_idx);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> top_id = <span class="number">0</span>; top_id &lt; num_top; ++top_id) &#123;</span><br><span class="line">       AppendTop(param, layer_id, top_id, &amp;available_blobs, &amp;blob_name_to_idx);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 调用layer类的Setup函数进行初始化，输入参数：每个layer的输入blobs以及输出blobs,为每个blob设置大小</span></span><br><span class="line">layers_[layer_id]-&gt;SetUp(bottom_vecs_[layer_id], top_vecs_[layer_id]);</span><br><span class="line"></span><br><span class="line"><span class="comment">//接下来的工作是将每层的parameter的指针塞进params_，尤其是learnable_params_。</span></span><br><span class="line">   <span class="keyword">const</span> <span class="keyword">int</span> num_param_blobs = layers_[layer_id]-&gt;blobs().size();</span><br><span class="line">   <span class="keyword">for</span> (<span class="keyword">int</span> param_id = <span class="number">0</span>; param_id &lt; num_param_blobs; ++param_id) &#123;</span><br><span class="line">       AppendParam(param, layer_id, param_id);</span><br><span class="line">       <span class="comment">//AppendParam负责具体的dirtywork</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>经过上面的过程，<code>Net</code>类的初始化工作基本就完成了，接着我们具体来看看上面所说的<code>layers_[layer_id]-&gt;SetUp</code>对每一具体的层结构进行设置，我们来看看<code>Layer</code>类的<code>Setup()</code>函数，对每一层的设置主要由下面三个函数组成：<br><code>LayerSetUp(bottom, top)</code>：由Layer类派生出的特定类都需要重写这个函数，主要功能是设置权值参数(包括偏置)的空间以及对权值参数经行随机初始化。<br><code>Reshape(bottom, top)</code>：根据输出blob和权值参数计算输出blob的维数，并申请空间。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">//layer.hpp</span><br><span class="line">// layer 初始化设置</span><br><span class="line">void SetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,   </span><br><span class="line">    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</span><br><span class="line">  InitMutex();</span><br><span class="line">  CheckBlobCounts(bottom, top);</span><br><span class="line">  LayerSetUp(bottom, top);</span><br><span class="line">  Reshape(bottom, top);</span><br><span class="line">  SetLossWeights(top);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>经过上述过程基本上就完成了初始化的工作，总体的流程大概就是新建一个<code>Solver</code>对象，然后调用<code>Solver</code>类的构造函数，然后在<code>Solver</code>的构造函数中又会新建<code>Net</code>类实例，在<code>Net</code>类的构造函数中又会新建各个<code>Layer</code>的实例,一直具体到设置每个<code>Blob</code>,大概就介绍完了网络初始化的工作，当然里面还有很多具体的细节，但大概的流程就是这样。</p>
<h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><p>上面介绍了网络初始化的大概流程，如上面所说的网络的初始化就是从下面一行代码新建一个<code>solver</code>指针开始一步一步的调用<code>Solver</code>，<code>Net</code>,<code>Layer</code>,<code>Blob</code>类的构造函数，完成整个网络的初始化。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">//caffe.cpp</span><br><span class="line">shared_ptr&lt;caffe::Solver&lt;float&gt; &gt; //初始化</span><br><span class="line">     solver(caffe::SolverRegistry&lt;float&gt;::CreateSolver(solver_param));</span><br></pre></td></tr></table></figure></p>
<p>完成初始化之后，就可以开始对网络经行训练了，开始训练的代码如下所示，指向<code>Solver</code>类的指针<code>solver</code>开始调用<code>Solver</code>类的成员函数<code>Solve()</code>，名称比较绕啊。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 开始优化</span></span><br><span class="line">solver-&gt;Solve();</span><br></pre></td></tr></table></figure></p>
<p>接下来我们来看看<code>Solver</code>类的成员函数<code>Solve()</code>,Solve函数其实主要就是调用了<code>Solver</code>的另一个成员函数<code>Step（）</code>来完成实际的迭代训练过程。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//solver.cpp</span><br><span class="line">template &lt;typename Dtype&gt;</span><br><span class="line">void Solver&lt;Dtype&gt;::Solve(const char* resume_file) &#123;</span><br><span class="line">  ...</span><br><span class="line">  int start_iter = iter_;</span><br><span class="line">  ...</span><br><span class="line">  // 然后调用了&apos;Step&apos;函数，这个函数执行了实际的逐步的迭代过程</span><br><span class="line">  Step(param_.max_iter() - iter_);</span><br><span class="line">  ...</span><br><span class="line">  LOG(INFO) &lt;&lt; &quot;Optimization Done.&quot;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>顺着来看看这个<code>Step()</code>函数的主要代码,首先是一个大循环设置了总的迭代次数，在每次迭代中训练<code>iter_size</code> x <code>batch_size</code>个样本，这个设置是为了在GPU的显存不够的时候使用，比如我本来想把batch_size设置为128，iter_size是默认为1的，但是会out_of_memory，借助这个方法，可以设置batch_size=32，iter_size=4，那实际上每次迭代还是处理了128个数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">//solver.cpp</span><br><span class="line">template &lt;typename Dtype&gt;</span><br><span class="line">void Solver&lt;Dtype&gt;::Step(int iters) &#123;</span><br><span class="line">  ...</span><br><span class="line">  //迭代</span><br><span class="line">  while (iter_ &lt; stop_iter) &#123;</span><br><span class="line">    ...</span><br><span class="line">    // iter_size也是在solver.prototxt里设置，实际上的batch_size=iter_size*网络定义里的batch_size，</span><br><span class="line">    // 因此每一次迭代的loss是iter_size次迭代的和，再除以iter_size，这个loss是通过调用`Net::ForwardBackward`函数得到的</span><br><span class="line">    // accumulate gradients over `iter_size` x `batch_size` instances</span><br><span class="line">    for (int i = 0; i &lt; param_.iter_size(); ++i) &#123;</span><br><span class="line">    /*</span><br><span class="line">     * 调用了Net中的代码，主要完成了前向后向的计算，</span><br><span class="line">     * 前向用于计算模型的最终输出和Loss，后向用于</span><br><span class="line">     * 计算每一层网络和参数的梯度。</span><br><span class="line">     */</span><br><span class="line">      loss += net_-&gt;ForwardBackward();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">     * 这个函数主要做Loss的平滑。由于Caffe的训练方式是SGD，我们无法把所有的数据同时</span><br><span class="line">     * 放入模型进行训练，那么部分数据产生的Loss就可能会和全样本的平均Loss不同，在必要</span><br><span class="line">     * 时候将Loss和历史过程中更新的Loss求平均就可以减少Loss的震荡问题。</span><br><span class="line">     */</span><br><span class="line">    UpdateSmoothedLoss(loss, start_iter, average_loss);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">    // 执行梯度的更新，这个函数在基类`Solver`中没有实现，会调用每个子类自己的实现</span><br><span class="line">    //，后面具体分析`SGDSolver`的实现</span><br><span class="line">    ApplyUpdate();</span><br><span class="line"></span><br><span class="line">    // 迭代次数加1</span><br><span class="line">    ++iter_;</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面<code>Step()</code>函数主要分为三部分：</p>
<h3 id="loss-net-gt-ForwardBackward"><a href="#loss-net-gt-ForwardBackward" class="headerlink" title="loss += net_-&gt;ForwardBackward();"></a><code>loss += net_-&gt;ForwardBackward();</code></h3><p>这行代码通过<code>Net</code>类的<code>net_</code>指针调用其成员函数<code>ForwardBackward()</code>，其代码如下所示,分别调用了成员函数<code>Forward(&amp;loss)</code>和成员函数<code>Backward()</code>来进行前向传播和反向传播。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// net.hpp</span><br><span class="line">// 进行一次正向传播，一次反向传播</span><br><span class="line">Dtype ForwardBackward() &#123;</span><br><span class="line">  Dtype loss;</span><br><span class="line">  Forward(&amp;loss);</span><br><span class="line">  Backward();</span><br><span class="line">  return loss;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>前面的<code>Forward(&amp;loss)</code>函数最终会执行到下面一段代码,<code>Net</code>类的<code>Forward()</code>函数会对网络中的每一层执行<code>Layer</code>类的成员函数<code>Forward()</code>，而具体的每一层<code>Layer</code>的派生类会重写<code>Forward()</code>函数来实现不同层的前向计算功能。上面的<code>Backward()</code>反向求导函数也和<code>Forward()</code>类似，调用不同层的<code>Backward()</code>函数来计算每层的梯度。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">//net.cpp</span><br><span class="line">for (int i = start; i &lt;= end; ++i) &#123;</span><br><span class="line">// 对每一层进行前向计算，返回每层的loss，其实只有最后一层loss不为0</span><br><span class="line">  Dtype layer_loss = layers_[i]-&gt;Forward(bottom_vecs_[i], top_vecs_[i]);</span><br><span class="line">  loss += layer_loss;</span><br><span class="line">  if (debug_info_) &#123; ForwardDebugInfo(i); &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="UpdateSmoothedLoss"><a href="#UpdateSmoothedLoss" class="headerlink" title="UpdateSmoothedLoss();"></a><code>UpdateSmoothedLoss();</code></h3><p> 这个函数主要做Loss的平滑。由于Caffe的训练方式是SGD，我们无法把所有的数据同时放入模型进行训练，那么部分数据产生的Loss就可能会和全样本的平均Loss不同，在必要时候将Loss和历史过程中更新的Loss求平均就可以减少Loss的震荡问题</p>
<h3 id="ApplyUpdate"><a href="#ApplyUpdate" class="headerlink" title="ApplyUpdate();"></a><code>ApplyUpdate();</code></h3><p>这个函数是<code>Solver</code>类的纯虚函数，需要派生类来实现，比如<code>SGDSolver</code>类实现的<code>ApplyUpdate();</code>函数如下，主要内容包括：设置参数的学习率；对梯度进行Normalize；对反向求导得到的梯度添加正则项的梯度；最后根据SGD算法计算最终的梯度；最后的最后把计算得到的最终梯度对权值进行更新。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">template &lt;typename Dtype&gt;</span><br><span class="line">void SGDSolver&lt;Dtype&gt;::ApplyUpdate() &#123;</span><br><span class="line">  CHECK(Caffe::root_solver());</span><br><span class="line"></span><br><span class="line">  // GetLearningRate根据设置的lr_policy来计算当前迭代的learning rate的值</span><br><span class="line">  Dtype rate = GetLearningRate();</span><br><span class="line"></span><br><span class="line">  // 判断是否需要输出当前的learning rate</span><br><span class="line">  if (this-&gt;param_.display() &amp;&amp; this-&gt;iter_ % this-&gt;param_.display() == 0) &#123;</span><br><span class="line">    LOG(INFO) &lt;&lt; &quot;Iteration &quot; &lt;&lt; this-&gt;iter_ &lt;&lt; &quot;, lr = &quot; &lt;&lt; rate;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // 避免梯度爆炸，如果梯度的二范数超过了某个数值则进行scale操作，将梯度减小</span><br><span class="line">  ClipGradients();</span><br><span class="line"></span><br><span class="line">  // 对所有可更新的网络参数进行操作</span><br><span class="line">  for (int param_id = 0; param_id &lt; this-&gt;net_-&gt;learnable_params().size();</span><br><span class="line">       ++param_id) &#123;</span><br><span class="line">	// 将第param_id个参数的梯度除以iter_size，</span><br><span class="line">	// 这一步的作用是保证实际的batch_size=iter_size*设置的batch_size</span><br><span class="line">    Normalize(param_id);</span><br><span class="line"></span><br><span class="line">    // 将正则化部分的梯度降入到每个参数的梯度中</span><br><span class="line">    Regularize(param_id);</span><br><span class="line"></span><br><span class="line">    // 计算SGD算法的梯度(momentum等)</span><br><span class="line">    ComputeUpdateValue(param_id, rate);</span><br><span class="line">  &#125;</span><br><span class="line">  // 调用`Net::Update`更新所有的参数</span><br><span class="line">  this-&gt;net_-&gt;Update();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>等进行了所有的循环，网络的训练也算是完成了。上面大概说了下使用Caffe进行网络训练时网络初始化以及前向传播、反向传播、梯度更新的过程，其中省略了大量的细节。上面还有很多东西都没提到，比如说Caffe中<code>Layer</code>派生类的注册及各个具体层前向反向的实现、<code>Solver</code>派生类的注册、网络结构的读取、模型的保存等等大量内容。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Implementing convolution as a matrix multiplication]]></title>
      <url>http://buptldy.github.io/2016/10/01/2016-10-01-im2col/</url>
      <content type="html"><![CDATA[<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-10-1/61814949.jpg" alt=""><br></center>

<a id="more"></a>
<h2 id="CNN中的卷积操作"><a href="#CNN中的卷积操作" class="headerlink" title="CNN中的卷积操作"></a>CNN中的卷积操作</h2><p>卷积层是CNNs网络中可以说是最重要的层了，卷积层的主要作用是对输入图像求卷积运算。如下图所示，输入图片的维数为$[c_0,h_0,w_0]$ ；卷积核的维数为$[c_1,c_0,h_k,w_k]$，其中$c_0$在图中没有表示出来，一个卷积核可以看成由$c_1$个维数为$[c_0,h_k,w_k]$的三维滤波器组成；除了这些参数通常在计算卷积运算的时候还有一些超参数比如：stride（步长）：$S$,padding（填充）：$P$。</p>
<p>根据上面所说的参数就可以求出输出特征的维数为$[c_1,h_1,w_1]$,其中$h_1 = (h_0-h_k+2P)/S+1$,$w_1 = (w_0-w_k+2P)/S+1$。</p>
<p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-10-1/28468053.jpg" alt=""></p>
<p>卷积的计算过程其实很简单，但不是很容易说清楚，下面通过代码来说明。</p>
<p>基本环境设置:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%load_ext cython  <span class="comment">#代码运行在jupyter-notebook中</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">10.0</span>, <span class="number">8.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br></pre></td></tr></table></figure>
<p>卷积层计算的代码如下，想象一副图像尺寸为MxM，卷积核mxm。在计算时，卷积核与图像中每个mxm大小的图像块做element-wise相乘，然后得到的结果相加得到一个值，然后再移动一个stride，做同样的运算，直到整副输入图像遍历完，上述过程得到的值就组成了输出特征，具体运算过程还是看代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward_naive</span><span class="params">(x, w, b, conv_param)</span>:</span></span><br><span class="line">  out = <span class="keyword">None</span></span><br><span class="line">  stride = conv_param[<span class="string">'stride'</span>]</span><br><span class="line">  pad = conv_param[<span class="string">'pad'</span>]</span><br><span class="line">  N, C, W, H = x.shape</span><br><span class="line">  F, C, HH, WW = w.shape</span><br><span class="line">  H_out = <span class="number">1</span> + (H + <span class="number">2</span> * pad - HH) / stride</span><br><span class="line">  W_out = <span class="number">1</span> + (W + <span class="number">2</span> * pad - WW) / stride</span><br><span class="line">  npad = ((<span class="number">0</span>,<span class="number">0</span>), (<span class="number">0</span>,<span class="number">0</span>), (pad,pad), (pad,pad))</span><br><span class="line">  x_pad = np.pad(x, pad_width=npad, mode=<span class="string">'constant'</span>, constant_values=<span class="number">0</span>)</span><br><span class="line">  out = np.zeros((N, F, H_out, W_out))</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> xrange(N):</span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> xrange(F):</span><br><span class="line">          <span class="keyword">for</span> k <span class="keyword">in</span> xrange(H_out):</span><br><span class="line">              <span class="keyword">for</span> z <span class="keyword">in</span> xrange(W_out):</span><br><span class="line">                  out[i, j, k, z] = np.sum(x_pad[i, :, k*stride:k*stride+HH,  z*stride:z*stride+WW]*w[j, :, :, :])+b[j]            </span><br><span class="line">  cache = (x, w, b, conv_param)</span><br><span class="line">  <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure>
<p>下面来检测下上面的卷积计算代码，我们人为的设置两个卷积核（分别为求灰度特征，和边缘特征），然后对两幅输入图像求卷积，观察输出的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.misc <span class="keyword">import</span> imread, imresize</span><br><span class="line">kitten, puppy = imread(<span class="string">'kitten.jpg'</span>), imread(<span class="string">'puppy.jpg'</span>)</span><br><span class="line"></span><br><span class="line">d = kitten.shape[<span class="number">1</span>] - kitten.shape[<span class="number">0</span>]</span><br><span class="line">kitten_cropped = kitten[:, d/<span class="number">2</span>:-d/<span class="number">2</span>, :]</span><br><span class="line"></span><br><span class="line">img_size = <span class="number">200</span>   <span class="comment"># Make this smaller if it runs too slow</span></span><br><span class="line">x = np.zeros((<span class="number">2</span>, <span class="number">3</span>, img_size, img_size))</span><br><span class="line">x[<span class="number">0</span>, :, :, :] = imresize(puppy, (img_size, img_size)).transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">x[<span class="number">1</span>, :, :, :] = imresize(kitten_cropped, (img_size, img_size)).transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set up a convolutional weights holding 2 filters, each 3x3</span></span><br><span class="line">w = np.zeros((<span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># The first filter converts the image to grayscale.</span></span><br><span class="line"><span class="comment"># Set up the red, green, and blue channels of the filter.</span></span><br><span class="line">w[<span class="number">0</span>, <span class="number">0</span>, :, :] = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0.3</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]]</span><br><span class="line">w[<span class="number">0</span>, <span class="number">1</span>, :, :] = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0.6</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]]</span><br><span class="line">w[<span class="number">0</span>, <span class="number">2</span>, :, :] = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Second filter detects horizontal edges in the blue channel.</span></span><br><span class="line">w[<span class="number">1</span>, <span class="number">2</span>, :, :] = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">-1</span>, <span class="number">-2</span>, <span class="number">-1</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Vector of biases. We don't need any bias for the grayscale</span></span><br><span class="line"><span class="comment"># filter, but for the edge detection filter we want to add 128</span></span><br><span class="line"><span class="comment"># to each output so that nothing is negative.</span></span><br><span class="line">b = np.array([<span class="number">0</span>, <span class="number">128</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the result of convolving each input in x with each filter in w,</span></span><br><span class="line"><span class="comment"># offsetting by b, and storing the results in out.</span></span><br><span class="line">out, _ = conv_forward_naive(x, w, b, &#123;<span class="string">'stride'</span>: <span class="number">1</span>, <span class="string">'pad'</span>: <span class="number">1</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow_noax</span><span class="params">(img, normalize=True)</span>:</span></span><br><span class="line">    <span class="string">""" Tiny helper to show images as uint8 and remove axis labels """</span></span><br><span class="line">    <span class="keyword">if</span> normalize:</span><br><span class="line">        img_max, img_min = np.max(img), np.min(img)</span><br><span class="line">        img = <span class="number">255.0</span> * (img - img_min) / (img_max - img_min)</span><br><span class="line">    plt.imshow(img.astype(<span class="string">'uint8'</span>))</span><br><span class="line">    plt.gca().axis(<span class="string">'off'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the original images and the results of the conv operation</span></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">imshow_noax(puppy, normalize=<span class="keyword">False</span>)</span><br><span class="line">plt.title(<span class="string">'Original image'</span>)</span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">imshow_noax(out[<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">plt.title(<span class="string">'Grayscale'</span>)</span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">imshow_noax(out[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.title(<span class="string">'Edges'</span>)</span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">imshow_noax(kitten_cropped, normalize=<span class="keyword">False</span>)</span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">imshow_noax(out[<span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>)</span><br><span class="line">imshow_noax(out[<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>图像经过卷积后，输入结果如下所示：</p>
<center>:<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-10-1/81198578.jpg" alt=""><br></center>

<h2 id="im2col"><a href="#im2col" class="headerlink" title="im2col"></a>im2col</h2><p>运行上面代码的时候，我们发现对这两张图片计算卷积还是比较慢的，而在CNN中是存在大量的卷积运算的，所以我们需要一个更加快速的计算卷积的方法。如下图所示为Caffe中计算卷积的示意图，通过上面普通卷积运算的实现我们可以发现，卷积操作实际上是在对输入特征的一定范围内和卷积核滤波器做点乘，如下图我们可以利用这一特性把卷积操作转换成两个大矩阵相乘。</p>
<p>把输入图像要经行卷积操作的这一区域展成列向量的操作通常称为<code>im2col</code>，具体过程如下图所示。</p>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-10-1/72204285.jpg" alt=""><br></center>

<p>下图为一个具体的例子，看懂下面这个图应该就会清楚上面的做法。</p>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-10-1/690672.jpg" alt=""><br></center>

<p>下面的<code>im2col_cython</code>是使用Cython代码来实现<code>im2col</code>功能，有关Cython在Python中的具体使用可参考：<a href="http://buptldy.github.io/2016/06/15/2016-06-15-Python%E9%80%9F%E5%BA%A6%E4%BC%98%E5%8C%96-Cython%E4%B8%ADnumpy%E4%BB%A5%E5%8F%8A%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%9A%84%E4%BD%BF%E7%94%A8/">Python速度优化-Cython中numpy以及多线程的使用</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">%%cython</span><br><span class="line"><span class="keyword">import</span> cython</span><br><span class="line">cimport numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">ctypedef fused DTYPE_t:</span><br><span class="line">    np.float32_t</span><br><span class="line">    np.float64_t</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">im2col_cython</span><span class="params">(np.ndarray[DTYPE_t, ndim=<span class="number">4</span>] x, int field_height,</span><br><span class="line">                  int field_width, int padding, int stride)</span>:</span></span><br><span class="line">    cdef int N = x.shape[<span class="number">0</span>]</span><br><span class="line">    cdef int C = x.shape[<span class="number">1</span>]</span><br><span class="line">    cdef int H = x.shape[<span class="number">2</span>]</span><br><span class="line">    cdef int W = x.shape[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">    cdef int HH = (H + <span class="number">2</span> * padding - field_height) / stride + <span class="number">1</span></span><br><span class="line">    cdef int WW = (W + <span class="number">2</span> * padding - field_width) / stride + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    cdef int p = padding</span><br><span class="line">    cdef np.ndarray[DTYPE_t, ndim=<span class="number">4</span>] x_padded = np.pad(x,</span><br><span class="line">            ((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>), (p, p), (p, p)), mode=<span class="string">'constant'</span>)</span><br><span class="line"></span><br><span class="line">    cdef np.ndarray[DTYPE_t, ndim=<span class="number">2</span>] cols = np.zeros(</span><br><span class="line">            (C * field_height * field_width, N * HH * WW),</span><br><span class="line">            dtype=x.dtype)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Moving the inner loop to a C function with no bounds checking works, but does</span></span><br><span class="line">    <span class="comment"># not seem to help performance in any measurable way.</span></span><br><span class="line"></span><br><span class="line">    cdef int c, ii, jj, row, yy, xx, i, col</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> range(C):</span><br><span class="line">        <span class="keyword">for</span> yy <span class="keyword">in</span> range(HH):</span><br><span class="line">            <span class="keyword">for</span> xx <span class="keyword">in</span> range(WW):</span><br><span class="line">                <span class="keyword">for</span> ii <span class="keyword">in</span> range(field_height):</span><br><span class="line">                    <span class="keyword">for</span> jj <span class="keyword">in</span> range(field_width):</span><br><span class="line">                        row = c * field_width * field_height + ii * field_height + jj</span><br><span class="line">                        <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">                            col = yy * WW * N + xx * N + i</span><br><span class="line">                            cols[row, col] = x_padded[i, c, stride * yy + ii, stride * xx + jj]</span><br><span class="line">    <span class="keyword">return</span> cols</span><br></pre></td></tr></table></figure>
<p>调用上面的<code>im2col_cython</code>函数来实现卷积操作：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward_im2col</span><span class="params">(x, w, b, conv_param)</span>:</span></span><br><span class="line">  <span class="string">"""</span><br><span class="line">  A fast implementation of the forward pass for a convolutional layer</span><br><span class="line">  based on im2col and col2im.</span><br><span class="line">  """</span></span><br><span class="line">  N, C, H, W = x.shape</span><br><span class="line">  num_filters, _, filter_height, filter_width = w.shape</span><br><span class="line">  stride, pad = conv_param[<span class="string">'stride'</span>], conv_param[<span class="string">'pad'</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Check dimensions</span></span><br><span class="line">  <span class="keyword">assert</span> (W + <span class="number">2</span> * pad - filter_width) % stride == <span class="number">0</span>, <span class="string">'width does not work'</span></span><br><span class="line">  <span class="keyword">assert</span> (H + <span class="number">2</span> * pad - filter_height) % stride == <span class="number">0</span>, <span class="string">'height does not work'</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Create output</span></span><br><span class="line">  out_height = (H + <span class="number">2</span> * pad - filter_height) / stride + <span class="number">1</span></span><br><span class="line">  out_width = (W + <span class="number">2</span> * pad - filter_width) / stride + <span class="number">1</span></span><br><span class="line">  out = np.zeros((N, num_filters, out_height, out_width), dtype=x.dtype)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># x_cols = im2col_indices(x, w.shape[2], w.shape[3], pad, stride)</span></span><br><span class="line">  x_cols = im2col_cython(x, w.shape[<span class="number">2</span>], w.shape[<span class="number">3</span>], pad, stride)</span><br><span class="line">  res = w.reshape((w.shape[<span class="number">0</span>], <span class="number">-1</span>)).dot(x_cols) + b.reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  out = res.reshape(w.shape[<span class="number">0</span>], out.shape[<span class="number">2</span>], out.shape[<span class="number">3</span>], x.shape[<span class="number">0</span>])</span><br><span class="line">  out = out.transpose(<span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">  cache = (x, w, b, conv_param, x_cols)</span><br><span class="line">  <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure></p>
<p>测试使用<code>im2col</code>方法的卷积操作，从输出的图片可以看出和原始卷积方法一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">out, _ = conv_forward_im2col(x, w, b, &#123;<span class="string">'stride'</span>: <span class="number">1</span>, <span class="string">'pad'</span>: <span class="number">1</span>&#125;)</span><br><span class="line"><span class="comment"># Show the original images and the results of the conv operation</span></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">imshow_noax(puppy, normalize=<span class="keyword">False</span>)</span><br><span class="line">plt.title(<span class="string">'Original image'</span>)</span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">imshow_noax(out[<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">plt.title(<span class="string">'Grayscale'</span>)</span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">imshow_noax(out[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.title(<span class="string">'Edges'</span>)</span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">imshow_noax(kitten_cropped, normalize=<span class="keyword">False</span>)</span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">imshow_noax(out[<span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>)</span><br><span class="line">imshow_noax(out[<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-10-1/81198578.jpg" alt=""><br></center>

<p>下面来测试一下使用两种方法的时间，使用原始的卷积操作每次循环需要2.19s，而使用<code>im2col</code>方法则只需要28.3ms，时间大概缩短了77倍，当然这其中也包括了使用Cython所降低的时间，但总体上来说还是大大加快了卷积的计算速度。</p>
<p>虽然使用<code>im2col</code>方法加快了计算速度，但也会使用更多的内存，因为把输入图像转换为col的时候，会有很多重复的元素。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%timeit conv_forward_naive(x, w, b, &#123;<span class="string">'stride'</span>: <span class="number">1</span>, <span class="string">'pad'</span>: <span class="number">1</span>&#125;)</span><br><span class="line">%timeit conv_forward_im2col(x, w, b, &#123;<span class="string">'stride'</span>: <span class="number">1</span>, <span class="string">'pad'</span>: <span class="number">1</span>&#125;)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1 loop, best of 3: 2.19 s per loop</span><br><span class="line">10 loops, best of 3: 28.3 ms per loop</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="external">Convolutional Neural Networks (CNNs / ConvNets)</a></p>
<p><a href="http://www.colinyan.com/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3caffe%E6%BA%90%E7%A0%81%EF%BC%88%E5%8D%B7%E7%A7%AF%E5%AE%9E%E7%8E%B0%E8%AF%A6%E7%BB%86%E5%88%86%E6%9E%90%EF%BC%89/" target="_blank" rel="external">深入理解Caffe源码（卷积实现详细分析</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Ways for Visualizing Convolutional Networks]]></title>
      <url>http://buptldy.github.io/2016/09/25/2016-09-25-cnn_vis/</url>
      <content type="html"><![CDATA[<p><img src="http://7xritj.com1.z0.glb.clouddn.com/%E9%80%89%E5%8C%BA_005.jpg" alt=""><br><a id="more"></a><br>近年来，卷积神经网络（CNN）在海量数据的物体分类、识别取得了巨大的成功，但是我们对CNN为什么能够取得这么好的效果以及其中间层所计算得到的特征的理解却是远远落后与CNN的应用。更多的时候CNN对于我们来说就像个黑盒子，输入数据和便签进行训练，然后就可以拟合出我们想要的结果。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-9-18/80287570.jpg" alt=""></p>
<p>如果不能弄明白CNN为什么能够工作的这么好，构建一个好的CNN模型就只能靠试错。为了对CNN有个直观的了解，近年来有许多工作围绕着CNN可视化来展开。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-9-18/38360122.jpg" alt=""></p>
<p>目前CNN的可视化方法主要分为两种：</p>
<p>(1) 前向计算可视化</p>
<p>通过前向计算直接可视化深度卷积网络每层的feature map，然后观察feature map的数值变化。一个训练成功的CNN网络，其feature map的值伴随网络深度的增加，会越来越稀疏。</p>
<p>(2)反向计算可视化</p>
<p>反向信号向后传播将低维的feature maps 还原到原图像空间，可视化该feature map被原图哪部分特征激活，从而理解该feature map从原图像学习了何种特征。</p>
<p>本文后面的内容也主要围绕这两方面展开。</p>
<h2 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h2><p>在介绍一些具体的可视化方法之前，我们先介绍一下我们使用的模型,我们使用的网络是经过CaffeNet微调，用来分类21类光学遥感图像的模型，具体内容可参考<a href="http://buptldy.github.io/2016/06/12/2016-06-12-CNN%E5%9C%A8%E5%85%89%E5%AD%A6%E9%81%A5%E6%84%9F%E5%9B%BE%E5%83%8F%E4%B8%8A%E7%9A%84%E5%BA%94%E7%94%A8/">CNN在光学遥感图像上的应用</a>。</p>
<p>CaffeNet其实就是AlexNet在Caffe上的实现，为了适应我们具体的分类任务，输出层改为21个节点。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-23/18035393.jpg" alt=""></p>
<p>其中要分类的21类光学遥感图像如下图所示：</p>
<p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-22/46888988.jpg" alt=""></p>
<h2 id="前向计算可视化"><a href="#前向计算可视化" class="headerlink" title="前向计算可视化"></a>前向计算可视化</h2><h3 id="特征可视化"><a href="#特征可视化" class="headerlink" title="特征可视化"></a>特征可视化</h3><p>通过可视化CNN计算得到的特征通常是大家都能想到的事情，通常第一层能提取到的特征能够和图像对应上，但是到了CNN的更高层，提取到的特征就变的更加抽象，不容易解释。</p>
<p>如下图所示，Input为输入图像，Filter为CNN第一层卷积层所学<br>习到的参数，可视化后其实就是一个个抽取边缘的滤波器，然后Output为CNN第一层卷积层所提取到的特征，从图中可以看出来输入图像经过CNN第一层卷积层之后得到了边缘特征。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-9-18/70836148.jpg" alt=""></p>
<p>但是CNN高层滤波器对前面输入特征的组合，提取得到的高维特征就不怎么好解释了，如下图所示，顺着箭头方向依次为上述输入图片通过CNN高层卷积层所提取到的特征，可以发现特征随着网络的加深，会越来越抽象、越来越稀疏。</p>
<p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-9-18/23256006.jpg" alt=""></p>
<h3 id="t-SNE-visualization"><a href="#t-SNE-visualization" class="headerlink" title="t-SNE visualization"></a>t-SNE visualization</h3><p>有时为了体现CNN提取到特征的相关性，我们可以把提取到的特征经行t-SNE降维，然后在二维平面显示出来，如下图所示。从下图可以看出，视觉上看上去相似的图片，在降维后在平面上也很靠近。我们提取的是fc7层的特征（也称为CNN-Code）,t-SNE降维为2维向量显示如下。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-9-18/89427613.jpg" alt=""></p>
<h3 id="遮挡实验"><a href="#遮挡实验" class="headerlink" title="遮挡实验"></a>遮挡实验</h3><p>如下图，左边的图为输入图像，注意上边的黑色遮挡区域，我们在输入图像上逐渐移动遮挡区域，然后记录对应输入图像所对应的正确类别的输出概率。很容易理解，当我们遮挡住输入图像的关键区域时，对应的正确输出概率会很低，从下图也可以看出来，当遮挡住飞机的关键部位时，CNN判别为飞机场的概率下降到0.2以下。说明CNN模型确实学习到了物体的关键部分，而不是只依靠一些上下文环境，遮挡实验的代码可参考：<a href="https://github.com/BUPTLdy/occlusion_experiments" target="_blank" rel="external">occlusion_experiments</a>。</p>
<p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-9-19/45188603.jpg" alt=""></p>
<h2 id="反向计算可视化"><a href="#反向计算可视化" class="headerlink" title="反向计算可视化"></a>反向计算可视化</h2><p>前面介绍的几种前向计算可视化的方法都比较好理解，但是还是不能解释CNN深层提取到的特征究竟是什么，究竟对应了输入图像的哪一部分。</p>
<h3 id="反向求导可视化"><a href="#反向求导可视化" class="headerlink" title="反向求导可视化"></a>反向求导可视化</h3><p>在探讨对图像反向求导可视化之前，我们先看看那一个线性分类器，公式如下：</p>
<p>$$f(x_i)=Wx_i+b$$</p>
<p>$W$为线性分类器权值、$x_i$表示一幅输入图像，$b$为偏置。如下图所示，$W$为线性分类器的权值维度为<code>[3×4]</code>，3表示要分类的数目，4表示为图片的每一个像素值打分;其中$x_i$为一幅图像展成的列向量，维度为<code>[4×1]</code>;$b$的维度为<code>[3×1]</code>,所以$Wx_i+b$得到一个<code>[3×1]</code>的向量表示当前输入图像$x_i$在每个类别上的打分，其中最高分预判为输入图像的类别。通过上述分析<br>可知$W$值决定了图像中的对应像素的重要性，某一类中某个像素越重要，则其对应的权值越大。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-9-21/40364595.jpg" alt=""></p>
<p>对于CNN来说因为有很多层非线性函数，$f(x_i)$为一个高度非线性话的分类器，不过我们可以把它看做一个整体，近似的等于一个线性分类器：</p>
<p>$$f(x_i) \approx Wx_i+b$$</p>
<p>然后我们可以对某个输入图片$x_0$上对上式求导，得打权值$W$，也就得到了对应输入图片的重要性大小。</p>
<p>$$W = \frac{\partial f(x_i)}{\partial I}\vert _{x_0}$$</p>
<p>产生的图像如下图所示，不是很明显，仔细看能看出飞机的轮廓。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-9-22/81010256.jpg" alt=""></p>
<h3 id="欺骗CNN网络"><a href="#欺骗CNN网络" class="headerlink" title="欺骗CNN网络"></a>欺骗CNN网络</h3><p>上面讨论了通过对图片求导来得到对应图片像素的重要性，我们可以利用上面求到的图像导数来欺骗CNN网络，如下图所示坐上图为输入图片类别为<code>airplane</code>，然后给定一个目标类别<code>denseresidential</code>,我们通过对输入图像求梯度上升来最大化目标类别的输出，求得的梯度累加到输入图像上，知道CNN判别为目标类别。下图中我们可以看出，上面的两个图人眼看起来都是<code>airplane</code>类别，差别看起来也不大，但是CNN判别第二张图为<code>denseresidential</code>类比，从某种意义上说我们欺骗了CNN。</p>
<p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-9-22/31659955.jpg" alt=""></p>
<h3 id="Class-Model-Visualisation"><a href="#Class-Model-Visualisation" class="headerlink" title="Class Model Visualisation"></a>Class Model Visualisation</h3><p>对于一个训练好的CNN模型，我们可以通过随机产生一张带噪声的图片然后在我们感兴趣的类别上通过梯度上升逐渐优化输入图片可以产生对应类别的图片。</p>
<p>更一般的, 让$I$ 表示随机产生的噪声图片， $y$ 表示我们感兴趣的类别， $s_y(I)$ 表示CNN 对图片 $I$ 在类别 $y$上的打分。 我们希望能够产生的图片 $I^*$ 使得在类别 $y$ 上打分最高。</p>
<p>$$<br>I^* = \arg\max_I s_y(I) - R(I)<br>$$</p>
<p>其中 $R$ 为正则项， 我们可以通过梯度上升法来求解 。</p>
<p>产生的图片如下所示，可以看出产生的图像对目标的分类又一定的旋转不变形和尺度不变性。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-9-22/60627433.jpg" alt=""><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-9-22/72860217.jpg" alt=""></p>
<h3 id="Feature-Inversion"><a href="#Feature-Inversion" class="headerlink" title="Feature Inversion"></a>Feature Inversion</h3><p>为了CNN怎么去学习和理解特征，最近也有文章提出通过提取到的特征重建原图像的方法。我们在训练好的CNN模型的基础上，可以通过对图像的求导来实现。具体来说，给定图片<br>$I$, 让$\phi_\ell(I)$ 表示卷积神经$\phi$中 $\ell$ 层所提取到的特征。我们想要求得一张图片$I^*$ 在网络$\phi$中的$\ell$ 层和图片 $I$有相同的特征。</p>
<p>$$<br>I^* = \arg\min_{I’} |\phi_\ell(I) - \phi_\ell(I’)|_2^2 + R(I’)<br>$$</p>
<p>其中 $|\cdot|_2^2$ 为欧式距离，$R$ 表示正则项。</p>
<p>下图展示了从不同层提取的特征重建原图的结果，可以看出层数越深，重建出的结果和原图差异越大，因为CNN在特取特征的过程中，还有一个压缩学习图片最本质特征的作用，所以越往后层，重建得到图片越是代表原图片的本质。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-9-23/93123483.jpg" alt=""></p>
<h3 id="DeepDream"><a href="#DeepDream" class="headerlink" title="DeepDream"></a>DeepDream</h3><p>2015年夏天，google发布了一种从神经网络产生图片的新方法，原理其实很简单，就是从神经网络中的某一层提取特征，然后让这一层的反向梯度等于这一层提取到的特征，然后在反向传导回图像，通常会选择在卷积层进行操作，所以可以产生任意分辨率的图像。</p>
<p>过程如下，我们先对CNN输入一张原图<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-9-23/40419664.jpg" alt=""><br>然后选择激活某一层的特征，如果选择的是高层特征，反向传递得到的结果如下，高层特征反向传递得到了一些复杂的模式；</p>
<p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-9-23/18649599.jpg" alt=""><br>如果是低层的特征，则得到的是一些线条，纹理特征。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-9-23/14566819.jpg" alt=""><br>如果我们把上述输出的结果当成输入再次传入，经过一定次数的循环，一些模式会得到增强，输出结果看起来有点惊悚:<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-9-23/11926968.jpg" alt=""></p>
<h3 id="反卷积可视化"><a href="#反卷积可视化" class="headerlink" title="反卷积可视化"></a>反卷积可视化</h3><p>反卷积顾名思义是和卷积相反的操作，使用反卷积进行特征的可视化，可以理解为把得到的特征映射回原图像的输入空间。反卷积网络如下图所示，其中下图左边为反卷积网络、右边为卷积网络。其中反卷积网络中的反卷积层和卷积网络中卷积层对应，Unpooling层和pooling层对应。卷积网络是输入图像提取特征，而反卷积网络是从特征映射到输入图像。</p>
<p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-9-19/64712899.jpg" alt=""></p>
<p>流程如上图所示。</p>
<h4 id="正常卷积过程convnet："><a href="#正常卷积过程convnet：" class="headerlink" title="正常卷积过程convnet："></a>正常卷积过程convnet：</h4><p>如图右侧黑框流程图部分，上一层pooled的特征图，通过本层的filter卷积后，形成本层的卷积特征，然后经过ReLU函数进行非线性变换的到Recitifed特征图，再经过本层的max-pooling操作，完成本层的卷积池化操作；之后传入下一层。本层需要记录在执行max-pooling操作时，每个pooing局域内最大值的位置</p>
<p>选择激活值：</p>
<p>为了理解某一个给定的pooling特征激活值，先把特征中其他的激活值设置为0；然后利用deconvnet把这个给定的激活值映射到初始像素层。</p>
<h4 id="反卷积过程deconvnet："><a href="#反卷积过程deconvnet：" class="headerlink" title="反卷积过程deconvnet："></a>反卷积过程deconvnet：</h4><p>Unpooling</p>
<p>顾名思义就是反pooling过程，由于pooling是不可逆的，所以unpooling只是正常pooling的一种近似；通过记录正常pooling时的位置，把传进来的特征按照记录的方式重新“摆放”，来近似pooling前的卷基层特征。如图中彩色部分</p>
<p>Filtering</p>
<p>利用卷积过程filter的转置（实际上就是水平和数值翻转filter）版本来计算卷积前的特征图；从而形成重构的特征。从一个单独的激活值获得的重构图片类似原始图片的一个部分。</p>
<p>反卷积反池化过程如下所示：<br><img src="http://7xritj.com1.z0.glb.clouddn.com/%E9%80%89%E5%8C%BA_006.jpg" alt=""></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过CNN可视化，我们可以看到底层卷积网络学习到的是一些边缘、颜色块等信息；高层网络通过对底层网络抽取到的特征经行组合，学习到了更加复杂以及具有不变性的特征。特征的可视化都是通过对图片方向求导来计算，通过设置不同的优化函数，梯度下降求导来达到可视化的目的。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/abs/1412.0035" target="_blank" rel="external">Understanding deep image representations by inverting them.</a></p>
<p><a href="https://arxiv.org/abs/1412.1897" target="_blank" rel="external">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images.</a></p>
<p><a href="https://arxiv.org/abs/1312.6034" target="_blank" rel="external">Deep inside convolutional networks: Visualising image classification models and saliency maps.</a></p>
<p><a href="http://arxiv.org/abs/1506.06579" target="_blank" rel="external">Understanding neural networks through deep visualization.</a></p>
<p><a href="https://arxiv.org/abs/1311.2901" target="_blank" rel="external">Visualizing and understanding convolutional networks.</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux Deepin Note]]></title>
      <url>http://buptldy.github.io/2016/08/30/2016-08-30-deepin/</url>
      <content type="html"><![CDATA[<p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-9-11/90739124.jpg" alt=""><br><a id="more"></a></p>
<h2 id="系统备份及还原"><a href="#系统备份及还原" class="headerlink" title="系统备份及还原"></a>系统备份及还原</h2><p>深度操作系统，是一个Linux发行版，由武汉深之度科技有限公司开发。Deepin系统不仅仅注重系统和桌面环境的开发，同时还注重配套的基础软件开发，目前Deepin系统已经拥有相当多深度特色应用并与许多第三方厂商合作推出热门应用的Linux版本。以上来自维基百科，总的来说Deepin界面很漂亮，对新手也很友好，但就是有点不稳定加上我又爱折腾，所以有时会崩溃，所以进行系统备份还是很有必要的。</p>
<p>备份系统前我们先了解下Linux文件系统的目录结构，清楚哪些文件夹需要备份，哪些不需要。</p>
<p><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-9-11/79598638.jpg" alt=""><br></center></p>
<h3 id="备份过程"><a href="#备份过程" class="headerlink" title="备份过程"></a>备份过程</h3><h4 id="切换到root"><a href="#切换到root" class="headerlink" title="切换到root"></a>切换到root</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo su</span><br></pre></td></tr></table></figure>
<h4 id="进入根目录"><a href="#进入根目录" class="headerlink" title="进入根目录"></a>进入根目录</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /</span><br></pre></td></tr></table></figure>
<h4 id="执行打包命令"><a href="#执行打包命令" class="headerlink" title="执行打包命令"></a>执行打包命令</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -cvpzf /media/ldy/6482108A82/backup1.tgz --exclude=/proc --exclude=/lost+found --exclude=/tmp --exclude=/sys --exclude=/media --exclude=/home /</span><br></pre></td></tr></table></figure>
<p>命令解释：</p>
<p>tar：linux常用的打包程序<br>cvpzf：式tar的参数</p>
<ul>
<li>c­创建新文档</li>
<li>v­处理过程中输出相关信息</li>
<li>p­表示保持相同的权限</li>
<li>z­调用gzip来压缩归档文件，与­x联用时调用gzip完成解压缩</li>
<li>f­对普通文件操作</li>
</ul>
<p>/media/ldy/6482108A82/backup1.tgz：表示打包到你挂载的硬盘里并命名为backup1.tgz</p>
<p>­­exclude=/proc：排除/proc目录，不打包这个目录，后面也同理，为什么排除参考上面的Linux文件系统的目录结构，为什么排除/home，因为我把/home新分了一区，在重装系统的时候选择不格式化/home分区即可保留数据</p>
<p>/：表示打包linux根目录所有文件，当然了排除的文件不包含在内</p>
<h3 id="恢复过程-还未实践"><a href="#恢复过程-还未实践" class="headerlink" title="恢复过程(还未实践)"></a>恢复过程(还未实践)</h3><h4 id="切换到root-1"><a href="#切换到root-1" class="headerlink" title="切换到root"></a>切换到root</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo su</span><br></pre></td></tr></table></figure>
<h4 id="进入根目录-1"><a href="#进入根目录-1" class="headerlink" title="进入根目录"></a>进入根目录</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /</span><br></pre></td></tr></table></figure>
<h4 id="解压恢复系统"><a href="#解压恢复系统" class="headerlink" title="解压恢复系统"></a>解压恢复系统</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar xvpfz linuxbackup.tgz -C /</span><br></pre></td></tr></table></figure>
<p>等执行完后，别急着重启系统，要记得创建刚才在备份时候排除的目录，手动创建，例如上面我们排除，我们需创建<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mkdir proc  </span><br><span class="line">mdkir lost+found  </span><br><span class="line">mkdir mnt   </span><br><span class="line">mkdir sys  </span><br><span class="line">mkdir tmp</span><br><span class="line">mkdir media</span><br></pre></td></tr></table></figure></p>
<h2 id="fsck命令"><a href="#fsck命令" class="headerlink" title="fsck命令"></a>fsck命令</h2><p>前两天由于笔记本突然掉电，导致/home分区损坏，开机出现：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Cannot open access to console , the root account is locked.</span><br></pre></td></tr></table></figure></p>
<h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>用deepin安装u盘启动，出现选择安装语言的界面时，按ctrl+alt+F1，进入tty，然后输入startx，进入live cd模式，挂载硬盘的根分区，然后修改/etc/fstab文件，把里面的/home分区里的启动项注释掉，如下所示。mount 命令在开始时会读取这个文件，确定设备和分区的挂载选项，注释掉后开机就不会挂载/home分区。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># /dev/sda2</span><br><span class="line">UUID=79813e75-eab0-42e4-b77c-daba9a9b7d01	/         	ext4      	rw,relatime,data=ordered	0 1</span><br><span class="line"></span><br><span class="line"># /dev/sda6</span><br><span class="line">#UUID=8b23af2a-2fd6-426e-8e63-f791378d8485	/home     	ext4      	rw,relatime,data=ordered	0 2</span><br><span class="line"></span><br><span class="line"># /dev/sda5</span><br><span class="line">UUID=730d40c7-946a-478e-bde9-9501ba156103	none      	swap      	defaults  	0 0</span><br></pre></td></tr></table></figure></p>
<p>修改后退出liveCD模式进入原系统，因为没有挂载损坏的/home分区，所以能进入系统，但是是不能进入图形界面的，进入文字界面执行下述命令修护损坏的/home分区，其中/dev/sda6为/home分区所在的设备名，设备名可以通过<code>fdisk -l</code>查看。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo fsck -y /dev/sda6</span><br></pre></td></tr></table></figure></p>
<p>修复成功后，取消/etc/fstab的注释，重启即可。</p>
<h2 id="双硬盘开机挂载"><a href="#双硬盘开机挂载" class="headerlink" title="双硬盘开机挂载"></a>双硬盘开机挂载</h2><p>前面已经介绍过/etc/fstab文件，要开机加载其他硬盘修改这个文件就可以。</p>
<h3 id="UUID"><a href="#UUID" class="headerlink" title="UUID"></a>UUID</h3><p>所有分区和设备都有唯一的 UUID。它们由文件系统生成工具 (mkfs.*) 在创建文件系统时生成。<br><code>lsblk -f</code>命令将显示所有设备的 UUID 值。/etc/fstab 中使用 UUID= 前缀:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">/etc/fstab</span><br><span class="line"># &lt;file system&gt;                           &lt;dir&gt;         &lt;type&gt;    &lt;options&gt;             &lt;dump&gt; &lt;pass&gt;</span><br><span class="line"></span><br><span class="line">tmpfs                                     /tmp          tmpfs     nodev,nosuid          0      0</span><br><span class="line"></span><br><span class="line">UUID=24f28fc6-717e-4bcd-a5f7-32b959024e26 /     ext4              defaults,noatime      0      1</span><br><span class="line">UUID=03ec5dd3-45c0-4f95-a363-61ff321a09ff /home ext4              defaults,noatime      0      2</span><br><span class="line">UUID=4209c845-f495-4c43-8a03-5363dd433153 none  swap              defaults              0      0</span><br></pre></td></tr></table></figure></p>
<h3 id="各段含义"><a href="#各段含义" class="headerlink" title="各段含义"></a>各段含义</h3><p><code>&lt;file systems&gt;</code> ：要挂载的分区或存储设备.</p>
<p><code>&lt;dir&gt;</code> ： <code>&lt;file systems&gt;</code>的挂载位置。</p>
<p><code>&lt;type&gt;</code>  要挂载设备或是分区的文件系统类型，支持许多种不同的文件系统：ext2, ext3, ext4, reiserfs, xfs, jfs, smbfs, iso9660, vfat, ntfs, swap 及 auto。 设置成auto类型，mount 命令会猜测使用的文件系统类型，对 CDROM 和 DVD 等移动设备是非常有用的。</p>
<p><code>&lt;options&gt;</code> 挂载时使用的参数，使用默认参数<code>defaults</code>即可。</p>
<p><code>&lt;dump&gt;</code>dump 工具通过它决定何时作备份. dump 会检查其内容，并用数字来决定是否对这个文件系统进行备份。 允许的数字是 0 和 1 。0 表示忽略， 1 则进行备份。大部分的用户是没有安装 dump 的 ，对他们而言 <dump> 应设为 0。</dump></p>
<p><code>&lt;pass&gt;</code> fsck 读取 <pass> 的数值来决定需要检查的文件系统的检查顺序。允许的数字是0, 1, 和2。 根目录应当获得最高的优先权 1, 其它所有需要被检查的设备设置为 2. 0 表示设备不会被 fsck 所检查。</pass></p>
<h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><p>比如我要开机自动挂载/dev/sdb5这个设备，在/etc/fstab后面加入下面内容即可。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># /dev/sdb5</span><br><span class="line">UUID=6482108A821062BA           /media/ldy/6482108A82   ntfs     	defaults  	0 0</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Implementation of Batch Normalization Layer]]></title>
      <url>http://buptldy.github.io/2016/08/18/2016-08-18-Batch_Normalization/</url>
      <content type="html"><![CDATA[<p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-8-5/21841995.jpg" alt=""></p>
<a id="more"></a>
<h2 id="数据归一化"><a href="#数据归一化" class="headerlink" title="数据归一化"></a>数据归一化</h2><p>通常在神经网络训练开始前,都要对输入数据做一个归一化处理,那么具体为什么需要归一化呢?归一化后有什么好处呢?原因在于神经网络学习过程本质就是为了学习数据分布,一旦训练数据与测试数据的分布不同,那么网络的泛化能力也大大降低;另外一方面,一旦每批训练数据的分布各不相同(batch 梯度下降),那么网络就要在每次迭代都去学习适应不同的分布,这样将会大大降低网络的训练速度,这也正是为什么我们需要对数据都要做一个归一化预处理的原因。对于深度网络的训练是一个复杂的过程,只要网络的前面几层发生微小的改变,那么后面几层就会被累积放大下去。一旦网络某一层的输入数据的分布发生改变,那么这一层网络就需要去适应学习这个新的数据分布,所以如果训练过程中,训练数据的分布一直在发生变化,那么将会影响网络的训练速度。</p>
<center><br><img src="http://i2.buimg.com/567571/70c7efd885eb08f3.png" alt=""><br></center>

<p>举例说明进行数据预处理能够加速训练过程，上图中红点代表2维的数据点，由于图像数据的每一维一般都是0-255之间的数字，因此数据点只会落在第一象限，而且图像数据具有很强的相关性，比如第一个灰度值为30，比较黑，那它旁边的一个像素值一般不会超过100，否则给人的感觉就像噪声一样。由于强相关性，数据点仅会落在第一象限的很小的区域中，形成类似上图所示的狭长分布。</p>
<p>而神经网络模型在初始化的时候，权重W是随机采样生成的，一个常见的神经元表示为：ReLU(Wx+b) = max(Wx+b,0)，即在Wx+b=0的两侧，对数据采用不同的操作方法。具体到ReLU就是一侧收缩，一侧保持不变。</p>
<p>随机的Wx+b=0表现为上图中的随机虚线，注意到，两条绿色虚线实际上并没有什么意义，在使用梯度下降时，可能需要很多次迭代才会使这些虚线对数据点进行有效的分割，就像紫色虚线那样，这势必会带来求解速率变慢的问题。更何况，我们这只是个二维的演示，数据占据四个象限中的一个，如果是几百、几千、上万维呢？而且数据在第一象限中也只是占了很小的一部分区域而已，可想而知不对数据进行预处理带来了多少运算资源的浪费，而且大量的数据外分割面在迭代时很可能会在刚进入数据中时就遇到了一个局部最优，导致overfit的问题。</p>
<p>这时，如果我们将数据减去其均值，数据点就不再只分布在第一象限，这时一个随机分界面落入数据分布的概率增加了多少呢？2^n倍！如果我们使用去除相关性的算法，例如PCA和ZCA白化，数据不再是一个狭长的分布，随机分界面有效的概率就又大大增加了。</p>
<p>不过计算协方差矩阵的特征值太耗时也太耗空间，我们一般最多只用到z-score处理，即每一维度减去自身均值，再除以自身标准差，这样能使数据点在每维上具有相似的宽度，可以起到一定的增大数据分布范围，进而使更多随机分界面有意义的作用。</p>
<h2 id="batch-normalization-算法"><a href="#batch-normalization-算法" class="headerlink" title="batch normalization 算法"></a>batch normalization 算法</h2><p>算法基本流程：</p>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-8-5/92943869.jpg" alt=""><br></center>

<p>如果在ReLU=max(Wx+b,0)之后，对数据进行归一化。然而，文章中说这样做在训练初期，分界面还在剧烈变化时，计算出的参数不稳定，所以退而求其次，在Wx+b之后进行归一化。因为初始的W是从标准高斯分布中采样得到的，而W中元素的数量远大于x，Wx+b每维的均值本身就接近0、方差接近1，所以在Wx+b后使用Batch Normalization能得到更稳定的结果。</p>
<p>文中使用了类似z-score的归一化方式：每一维度减去自身均值，再除以自身标准差，由于使用的是随机梯度下降法，这些均值和方差也只能在当前迭代的batch中计算，故作者给这个算法命名为Batch Normalization。</p>
<p>在Normalization完成后，Google的研究员仍对数值稳定性不放心，又加入了两个参数gamma和beta，使得</p>
<p>$$y_i=\gamma \hat{x}_i+ \beta$$</p>
<p>注意到，如果我们令gamma等于之前求得的标准差，beta等于之前求得的均值，则这个变换就又将数据还原回去了。在他们的模型中，这两个参数与每层的W和b一样，是需要迭代求解的。为什么进行归一化之后又添加两个可学习的参数对数据进行变化：<strong>实际上BN可以看作是在原模型上加入的“新操作”，这个新操作很大可能会改变某层原来的输入。当然也可能不改变，不改变的时候就是“还原原来输入”。如此一来，既可以改变同时也可以保持原输入，那么模型的容纳能力（capacity）就提升了。</strong></p>
<h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-8-5/21841995.jpg" alt=""></p>
<p>根据链式求导法则，我们可以把复杂的运算分解成一步一步能够简单求导的运算，然后根据链式求导法则来求得最终的导数，参考<a href="http://cs231n.github.io/optimization-2/" target="_blank" rel="external">cs231n</a>。</p>
<h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_forward</span><span class="params">(x, gamma, beta, eps)</span>:</span></span><br><span class="line"></span><br><span class="line">  N, D = x.shape</span><br><span class="line"></span><br><span class="line">  <span class="comment">#step1: calculate mean</span></span><br><span class="line">  mu = <span class="number">1.</span>/N * np.sum(x, axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#step2: subtract mean vector of every trainings example</span></span><br><span class="line">  xmu = x - mu</span><br><span class="line"></span><br><span class="line">  <span class="comment">#step3: following the lower branch - calculation denominator</span></span><br><span class="line">  sq = xmu ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">#step4: calculate variance</span></span><br><span class="line">  var = <span class="number">1.</span>/N * np.sum(sq, axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#step5: add eps for numerical stability, then sqrt</span></span><br><span class="line">  sqrtvar = np.sqrt(var + eps)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#step6: invert sqrtwar</span></span><br><span class="line">  ivar = <span class="number">1.</span>/sqrtvar</span><br><span class="line"></span><br><span class="line">  <span class="comment">#step7: execute normalization</span></span><br><span class="line">  xhat = xmu * ivar</span><br><span class="line"></span><br><span class="line">  <span class="comment">#step8: Nor the two transformation steps</span></span><br><span class="line">  gammax = gamma * xhat</span><br><span class="line"></span><br><span class="line">  <span class="comment">#step9</span></span><br><span class="line">  out = gammax + beta</span><br><span class="line"></span><br><span class="line">  <span class="comment">#store intermediate</span></span><br><span class="line">  cache = (xhat,gamma,xmu,ivar,sqrtvar,var,eps)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure>
<h3 id="反向求导"><a href="#反向求导" class="headerlink" title="反向求导"></a>反向求导</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">#unfold the variables stored in cache</span></span><br><span class="line">  xhat,gamma,xmu,ivar,sqrtvar,var,eps = cache</span><br><span class="line"></span><br><span class="line">  <span class="comment">#get the dimensions of the input/output</span></span><br><span class="line">  N,D = dout.shape</span><br><span class="line"></span><br><span class="line">  <span class="comment">#step9</span></span><br><span class="line">  dbeta = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">  dgammax = dout <span class="comment">#not necessary, but more understandable</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">#step8</span></span><br><span class="line">  dgamma = np.sum(dgammax*xhat, axis=<span class="number">0</span>)</span><br><span class="line">  dxhat = dgammax * gamma</span><br><span class="line"></span><br><span class="line">  <span class="comment">#step7</span></span><br><span class="line">  divar = np.sum(dxhat*xmu, axis=<span class="number">0</span>)</span><br><span class="line">  dxmu1 = dxhat * ivar</span><br><span class="line"></span><br><span class="line">  <span class="comment">#step6</span></span><br><span class="line">  dsqrtvar = <span class="number">-1.</span> /(sqrtvar**<span class="number">2</span>) * divar</span><br><span class="line"></span><br><span class="line">  <span class="comment">#step5</span></span><br><span class="line">  dvar = <span class="number">0.5</span> * <span class="number">1.</span> /np.sqrt(var+eps) * dsqrtvar</span><br><span class="line"></span><br><span class="line">  <span class="comment">#step4</span></span><br><span class="line">  dsq = <span class="number">1.</span> /N * np.ones((N,D)) * dvar</span><br><span class="line"></span><br><span class="line">  <span class="comment">#step3</span></span><br><span class="line">  dxmu2 = <span class="number">2</span> * xmu * dsq</span><br><span class="line"></span><br><span class="line">  <span class="comment">#step2</span></span><br><span class="line">  dx1 = (dxmu1 + dxmu2)</span><br><span class="line">  dmu = <span class="number">-1</span> * np.sum(dxmu1+dxmu2, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#step1</span></span><br><span class="line">  dx2 = <span class="number">1.</span> /N * np.ones((N,D)) * dmu</span><br><span class="line"></span><br><span class="line">  <span class="comment">#step0</span></span><br><span class="line">  dx = dx1 + dx2</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure>
<h2 id="卷积层batch-normalization"><a href="#卷积层batch-normalization" class="headerlink" title="卷积层batch normalization"></a>卷积层batch normalization</h2><p>这里有一点需要注意，像卷积层这样具有权值共享的层，Wx+b的均值和方差是对整张map求得的，在batch_size * channel * height * width这么大的一层中，对总共batch_size*height*width个像素点统计得到一个均值和一个标准差，共得到channel组参数。</p>
<p>也就是说把每个channel看出一批数据，然后就可以调用全连接层的batch normalization 算法了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spatial_batchnorm_forward</span><span class="params">(x, gamma, beta, bn_param)</span>:</span></span><br><span class="line">  <span class="string">"""</span><br><span class="line">  Computes the forward pass for spatial batch normalization.</span><br><span class="line"></span><br><span class="line">  Inputs:</span><br><span class="line">  - x: Input data of shape (N, C, H, W)</span><br><span class="line">  - gamma: Scale parameter, of shape (C,)</span><br><span class="line">  - beta: Shift parameter, of shape (C,)</span><br><span class="line">  - bn_param: Dictionary with the following keys:</span><br><span class="line">    - mode: 'train' or 'test'; required</span><br><span class="line">    - eps: Constant for numeric stability</span><br><span class="line">    - momentum: Constant for running mean / variance. momentum=0 means that</span><br><span class="line">      old information is discarded completely at every time step, while</span><br><span class="line">      momentum=1 means that new information is never incorporated. The</span><br><span class="line">      default of momentum=0.9 should work well in most situations.</span><br><span class="line">    - running_mean: Array of shape (D,) giving running mean of features</span><br><span class="line">    - running_var Array of shape (D,) giving running variance of features</span><br><span class="line"></span><br><span class="line">  Returns a tuple of:</span><br><span class="line">  - out: Output data, of shape (N, C, H, W)</span><br><span class="line">  - cache: Values needed for the backward pass</span><br><span class="line">  """</span></span><br><span class="line">  out, cache = <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line">  N, C, H, W = x.shape</span><br><span class="line">  x_flat = x.transpose(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).reshape(<span class="number">-1</span>, C)</span><br><span class="line">  out_flat, cache = batchnorm_forward(x_flat, gamma, beta, bn_param)</span><br><span class="line">  out = out_flat.reshape(N, H, W, C).transpose(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spatial_batchnorm_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">  <span class="string">"""</span><br><span class="line">  Computes the backward pass for spatial batch normalization.</span><br><span class="line"></span><br><span class="line">  Inputs:</span><br><span class="line">  - dout: Upstream derivatives, of shape (N, C, H, W)</span><br><span class="line">  - cache: Values from the forward pass</span><br><span class="line"></span><br><span class="line">  Returns a tuple of:</span><br><span class="line">  - dx: Gradient with respect to inputs, of shape (N, C, H, W)</span><br><span class="line">  - dgamma: Gradient with respect to scale parameter, of shape (C,)</span><br><span class="line">  - dbeta: Gradient with respect to shift parameter, of shape (C,)</span><br><span class="line">  """</span></span><br><span class="line">  dx, dgamma, dbeta = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>                                   </span><br><span class="line">  N, C, H, W = dout.shape</span><br><span class="line">  dout_flat = dout.transpose(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).reshape(<span class="number">-1</span>, C)</span><br><span class="line">  dx_flat, dgamma, dbeta = batchnorm_backward(dout_flat, cache)</span><br><span class="line">  dx = dx_flat.reshape(N, H, W, C).transpose(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)                        </span><br><span class="line">  <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://blog.csdn.net/hjimce/article/details/50866313" target="_blank" rel="external">Batch Normalization 学习笔记</a></p>
<p><a href="http://blog.csdn.net/happynear/article/details/44238541" target="_blank" rel="external">《Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift》阅读笔记与实现</a></p>
<p><a href="http://zhihu.com/question/38102762/answer/85238569" target="_blank" rel="external">深度学习中 Batch Normalization为什么效果好？ - 回答作者: 魏秀参</a></p>
<p><a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html" target="_blank" rel="external">Understanding the backward pass through Batch Normalization Layer</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Shopping Reviews sentiment analysis]]></title>
      <url>http://buptldy.github.io/2016/07/20/2016-07-20-sentiment%20analysis/</url>
      <content type="html"><![CDATA[<p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-7-18/19781698.jpg" alt=""><br><a id="more"></a><br>情感分析是一种常见的自然语言处理（NLP）方法的应用，特别是在以提取文本的情感内容为目标的分类方法中。通过这种方式，情感分析可以被视为利用一些情感得分指标来量化定性数据的方法。尽管情绪在很大程度上是主观的，但是情感量化分析已经有很多有用的实践，比如企业分析消费者对产品的反馈信息，或者检测在线评论中的差评信息。</p>
<p>最简单的情感分析方法是利用词语的正负属性来判定。句子中的每个单词都有一个得分，乐观的单词得分为+1，悲观的单词则为-1。然后我们对句子中所有单词得分进行加总求和得到一个最终的情感总分。很明显，这种方法有许多局限之处，最重要的一点在于它忽略了上下文的信息。例如，在这个简易模型中，因为“not”的得分为-1，而“good”的得分为 +1，所以词组“not good”将被归类到中性词组中。但是“not good”通常是消极的。</p>
<p>另外一个常见的方法是将文本视为一个“词袋”。我们将每个文本看出一个<code>1xN</code>的向量，其中N表示文本词汇的数量。该向量中每一列都是一个单词，其对应的值为该单词出现的频数。例如，词组“bag of bag of words”可以被编码为<code>[2, 2, 1]</code>。这些数据可以被应用到机器学习分类算法中（比如罗吉斯回归或者支持向量机），从而预测未知数据的情感状况。需要注意的是，这种有监督学习的方法要求利用已知情感状况的数据作为训练集。虽然这个方法改进了之前的模型，但是它仍然忽略了上下文的信息和数据集的规模情况。</p>
<h2 id="Word2Vec-and-Doc2Vec"><a href="#Word2Vec-and-Doc2Vec" class="headerlink" title="Word2Vec and Doc2Vec"></a>Word2Vec and Doc2Vec</h2><p>谷歌开发了一个叫做Word2Vec的方法，该方法可以在捕捉语境信息的同时压缩数据规模。Word2Vec实际上是两种不同的方法：Continuous Bag of Words (CBOW) 和 Skip-gram。CBOW的目标是根据上下文来预测当前词语。Skip-gram刚好相反：根据当前词语来预测上下文。这两种方法都利用人工神经网络作为它们的分类算法。起初，每个单词都是一个随机的 N 维向量。经过训练之后，该算法利用 CBOW 或者 Skip-gram 的方法获得了每个单词的最优向量。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-24/80870876.jpg" alt=""><br>在上图中 $w(t)$ 表示当前的词汇，$w(t-2)$ ， $w(t-1)$ 等表示上下文词汇。</p>
<p>现在这些词向量已经捕捉到上下文的信息。我们可以利用基本代数公式来发现单词之间的关系（比如，“国王”-“男人”+“女人”=“王后”）。这些词向量可以代替词袋用来预测未知数据的情感状况。该模型的优点在于不仅考虑了语境信息还压缩了数据规模（通常情况下，词汇量规模大约在300个单词左右而不是之前模型的100000个单词）。因为神经网络可以替我们提取出这些特征的信息，所以我们仅需要做很少的手动工作。</p>
<h2 id="使用SVM和Word2Vec进行情感分类"><a href="#使用SVM和Word2Vec进行情感分类" class="headerlink" title="使用SVM和Word2Vec进行情感分类"></a>使用SVM和Word2Vec进行情感分类</h2><p>我们使用的训练数据是网友<a href="http://spaces.ac.cn/archives/3414/" target="_blank" rel="external">苏剑林</a>收集分享的两万多条中文标注语料，涉及六个领域的评论数据。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-25/48028586.jpg" alt=""><br>我们随机正负这两组数据中抽取样本，构建比例为8：2的训练集和测试集。随后，我们对训练集数据构建Word2Vec模型，其中分类器的输入值为推文中所有词向量的加权平均值。word2vec工具和svm分类器分别使用python中的<a href="https://radimrehurek.com/gensim/" target="_blank" rel="external">gensim</a>库和<a href="http://scikit-learn.org/" target="_blank" rel="external">sklearn</a>库。</p>
<p>加载文件，并分词</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载文件，导入数据,分词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadfile</span><span class="params">()</span>:</span></span><br><span class="line">    neg=pd.read_excel(<span class="string">'data/neg.xls'</span>,header=<span class="keyword">None</span>,index=<span class="keyword">None</span>)</span><br><span class="line">    pos=pd.read_excel(<span class="string">'data/pos.xls'</span>,header=<span class="keyword">None</span>,index=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">    cw = <span class="keyword">lambda</span> x: list(jieba.cut(x))</span><br><span class="line">    pos[<span class="string">'words'</span>] = pos[<span class="number">0</span>].apply(cw)</span><br><span class="line">    neg[<span class="string">'words'</span>] = neg[<span class="number">0</span>].apply(cw)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#print pos['words']</span></span><br><span class="line">    <span class="comment">#use 1 for positive sentiment, 0 for negative</span></span><br><span class="line">    y = np.concatenate((np.ones(len(pos)), np.zeros(len(neg))))</span><br><span class="line"></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(np.concatenate((pos[<span class="string">'words'</span>], neg[<span class="string">'words'</span>])), y, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">    np.save(<span class="string">'svm_data/y_train.npy'</span>,y_train)</span><br><span class="line">    np.save(<span class="string">'svm_data/y_test.npy'</span>,y_test)</span><br><span class="line">    <span class="keyword">return</span> x_train,x_test</span><br></pre></td></tr></table></figure>
<p>计算词向量，并对每个评论的所有词向量取均值作为每个评论的输入<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对每个句子的所有词向量取均值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">buildWordVector</span><span class="params">(text, size,imdb_w2v)</span>:</span></span><br><span class="line">    vec = np.zeros(size).reshape((<span class="number">1</span>, size))</span><br><span class="line">    count = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> text:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            vec += imdb_w2v[word].reshape((<span class="number">1</span>, size))</span><br><span class="line">            count += <span class="number">1.</span></span><br><span class="line">        <span class="keyword">except</span> KeyError:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">if</span> count != <span class="number">0</span>:</span><br><span class="line">        vec /= count</span><br><span class="line">    <span class="keyword">return</span> vec</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算词向量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_train_vecs</span><span class="params">(x_train,x_test)</span>:</span></span><br><span class="line">    n_dim = <span class="number">300</span></span><br><span class="line">    <span class="comment">#Initialize model and build vocab</span></span><br><span class="line">    imdb_w2v = Word2Vec(size=n_dim, min_count=<span class="number">10</span>)</span><br><span class="line">    imdb_w2v.build_vocab(x_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Train the model over train_reviews (this may take several minutes)</span></span><br><span class="line">    imdb_w2v.train(x_train)</span><br><span class="line"></span><br><span class="line">    train_vecs = np.concatenate([buildWordVector(z, n_dim,imdb_w2v) <span class="keyword">for</span> z <span class="keyword">in</span> x_train])</span><br><span class="line">    <span class="comment">#train_vecs = scale(train_vecs)</span></span><br><span class="line"></span><br><span class="line">    np.save(<span class="string">'svm_data/train_vecs.npy'</span>,train_vecs)</span><br><span class="line">    <span class="keyword">print</span> train_vecs.shape</span><br><span class="line">    <span class="comment">#Train word2vec on test tweets</span></span><br><span class="line">    imdb_w2v.train(x_test)</span><br><span class="line">    imdb_w2v.save(<span class="string">'svm_data/w2v_model/w2v_model.pkl'</span>)</span><br><span class="line">    <span class="comment">#Build test tweet vectors then scale</span></span><br><span class="line">    test_vecs = np.concatenate([buildWordVector(z, n_dim,imdb_w2v) <span class="keyword">for</span> z <span class="keyword">in</span> x_test])</span><br><span class="line">    <span class="comment">#test_vecs = scale(test_vecs)</span></span><br><span class="line">    np.save(<span class="string">'svm_data/test_vecs.npy'</span>,test_vecs)</span><br><span class="line">    <span class="keyword">print</span> test_vecs.shape</span><br></pre></td></tr></table></figure></p>
<p>训练svm模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##训练svm模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_train</span><span class="params">(train_vecs,y_train,test_vecs,y_test)</span>:</span></span><br><span class="line">    clf=SVC(kernel=<span class="string">'rbf'</span>,verbose=<span class="keyword">True</span>)</span><br><span class="line">    clf.fit(train_vecs,y_train)</span><br><span class="line">    joblib.dump(clf, <span class="string">'svm_data/svm_model/model.pkl'</span>)</span><br><span class="line">    <span class="keyword">print</span> clf.score(test_vecs,y_test)</span><br></pre></td></tr></table></figure>
<p>在没有创建任何类型的特性和最小文本预处理的情况下，我们利用Scikit-Learn构建的简单线性模型的预测精度为80%左右。有趣的是，删除标点符号会影响预测精度，这说明Word2Vec模型可以提取出文档中符号所包含的信息。处理单独的单词，训练更长时间，做更多的数据预处理工作，和调整模型的参数都可以提高预测精度。用svm分类有一个缺点是，我们把每个句子的词向量求平均丢失了句子词语之间的顺序信息。</p>
<h2 id="使用LSTM和Word2Vec进行情感分类"><a href="#使用LSTM和Word2Vec进行情感分类" class="headerlink" title="使用LSTM和Word2Vec进行情感分类"></a>使用LSTM和Word2Vec进行情感分类</h2><p>人类的思维不是每时每刻都是崭新的，就像你阅读一篇文章时，你理解当前词语的基础是基于对之前词语的理解，人类的思维是能保持一段时间的。传统的人工神经网络，并不能模拟人类思维具有记忆性这一特征，例如，你想要分类电影在某一时间点发生了什么事情，使用传统的人工神经网络并不能清楚的表现出之前出现的镜头对当前镜头的提示。循环神经网络能够很好的处理这个问题。</p>
<p><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-7-8/34980285.jpg" alt=""><br></center><br>RNN相对于传统的神经网络，它允许我们对向量序列进行操作：输入序列、输出序列、或大部分的输入输出序列。如下图所示，每一个矩形是一个向量，箭头则表示函数（比如矩阵相乘）。输入向量用红色标出，输出向量用蓝色标出，绿色的矩形是RNN的状态（下面会详细介绍）。从做到右：（1）没有使用RNN的Vanilla模型，从固定大小的输入得到固定大小输出（比如图像分类）。（2）序列输出（比如图片字幕，输入一张图片输出一段文字序列）。（3）序列输入（比如情感分析，输入一段文字然后将它分类成积极或者消极情感）。（4）序列输入和序列输出（比如机器翻译：一个RNN读取一条英文语句然后将它以法语形式输出）。（5）同步序列输入输出（比如视频分类，对视频中每一帧打标签）。我们注意到在每一个案例中，都没有对序列长度进行预先特定约束，因为递归变换（绿色部分）是固定的，而且我们可以多次使用。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-25/61310852.jpg" alt=""></p>
<p>单纯循环神经网络因为无法处理随着递归，权重指数级爆炸或消失的问题（Vanishing gradient problem），难以捕捉长期时间关联；而结合不同的LSTM可以很好解决这个问题。</p>
<p>LSTM 全称叫 Long Short Term Memory networks，它和传统 RNN 唯一的不同就在与其中的神经元（感知机）的构造不同。传统的 RNN 每个神经元和一般神经网络的感知机没啥区别，但在 LSTM 中，每个神经元是一个“记忆细胞”，细胞里面有一个“输入门”（input gate）, 一个“遗忘门”（forget gate）， 一个“输出门”（output gate）。</p>
<p>这个设计的用意在于，能够使得LSTM维持两条线，一条明线：当前时刻的数据流（包括其他细胞的输入和来自数据的输入）；一条暗线：这个细胞本身的记忆流。两条线互相呼应，互相纠缠，就像佛祖青灯里的两根灯芯。典型的工作流如下：在“输入门”中，根据当前的数据流来控制接受细胞记忆的影响；接着，在“遗忘门”里，更新这个细胞的记忆和数据流；然后在“输出门”里产生输出更新后的记忆和数据流。LSTM 模型的关键之一就在于这个“遗忘门”， 它能够控制训练时候梯度在这里的收敛性（从而避免了 RNN 中的梯度 vanishing/exploding问题），同时也能够保持长期的记忆性。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-25/14675212.jpg" alt=""></p>
<h3 id="实现步骤"><a href="#实现步骤" class="headerlink" title="实现步骤:"></a>实现步骤:</h3><p>加载训练文件并分词<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载训练文件</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadfile</span><span class="params">()</span>:</span></span><br><span class="line">    neg=pd.read_excel(<span class="string">'data/neg.xls'</span>,header=<span class="keyword">None</span>,index=<span class="keyword">None</span>)</span><br><span class="line">    pos=pd.read_excel(<span class="string">'data/pos.xls'</span>,header=<span class="keyword">None</span>,index=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">    combined=np.concatenate((pos[<span class="number">0</span>], neg[<span class="number">0</span>]))</span><br><span class="line">    y = np.concatenate((np.ones(len(pos),dtype=int), np.zeros(len(neg),dtype=int)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> combined,y</span><br><span class="line"></span><br><span class="line"><span class="comment">#对句子经行分词，并去掉换行符</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenizer</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="string">''' Simple Parser converting each document to lower-case, then</span><br><span class="line">        removing the breaks for new lines and finally splitting on the</span><br><span class="line">        whitespace</span><br><span class="line">    '''</span></span><br><span class="line">    text = [jieba.lcut(document.replace(<span class="string">'\n'</span>, <span class="string">''</span>)) <span class="keyword">for</span> document <span class="keyword">in</span> text]</span><br><span class="line">    <span class="keyword">return</span> text</span><br></pre></td></tr></table></figure></p>
<p>创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dictionaries</span><span class="params">(model=None,</span><br><span class="line">                        combined=None)</span>:</span></span><br><span class="line">    <span class="string">''' Function does are number of Jobs:</span><br><span class="line">        1- Creates a word to index mapping</span><br><span class="line">        2- Creates a word to vector mapping</span><br><span class="line">        3- Transforms the Training and Testing Dictionaries</span><br><span class="line"></span><br><span class="line">    '''</span></span><br><span class="line">    <span class="keyword">if</span> (combined <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>) <span class="keyword">and</span> (model <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>):</span><br><span class="line">        gensim_dict = Dictionary()</span><br><span class="line">        gensim_dict.doc2bow(model.vocab.keys(),</span><br><span class="line">                            allow_update=<span class="keyword">True</span>)</span><br><span class="line">        w2indx = &#123;v: k+<span class="number">1</span> <span class="keyword">for</span> k, v <span class="keyword">in</span> gensim_dict.items()&#125;<span class="comment">#所有频数超过10的词语的索引</span></span><br><span class="line">        w2vec = &#123;word: model[word] <span class="keyword">for</span> word <span class="keyword">in</span> w2indx.keys()&#125;<span class="comment">#所有频数超过10的词语的词向量</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">parse_dataset</span><span class="params">(combined)</span>:</span></span><br><span class="line">            <span class="string">''' Words become integers</span><br><span class="line">            '''</span></span><br><span class="line">            data=[]</span><br><span class="line">            <span class="keyword">for</span> sentence <span class="keyword">in</span> combined:</span><br><span class="line">                new_txt = []</span><br><span class="line">                <span class="keyword">for</span> word <span class="keyword">in</span> sentence:</span><br><span class="line">                    <span class="keyword">try</span>:</span><br><span class="line">                        new_txt.append(w2indx[word])</span><br><span class="line">                    <span class="keyword">except</span>:</span><br><span class="line">                        new_txt.append(<span class="number">0</span>)</span><br><span class="line">                data.append(new_txt)</span><br><span class="line">            <span class="keyword">return</span> data</span><br><span class="line">        combined=parse_dataset(combined)</span><br><span class="line">        combined= sequence.pad_sequences(combined, maxlen=maxlen)<span class="comment">#每个句子所含词语对应的索引，所以句子中含有频数小于10的词语，索引为0</span></span><br><span class="line">        <span class="keyword">return</span> w2indx, w2vec,combined</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'No data provided...'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word2vec_train</span><span class="params">(combined)</span>:</span></span><br><span class="line"></span><br><span class="line">    model = Word2Vec(size=vocab_dim,</span><br><span class="line">                     min_count=n_exposures,</span><br><span class="line">                     window=window_size,</span><br><span class="line">                     workers=cpu_count,</span><br><span class="line">                     iter=n_iterations)</span><br><span class="line">    model.build_vocab(combined)</span><br><span class="line">    model.train(combined)</span><br><span class="line">    model.save(<span class="string">'lstm_data/Word2vec_model.pkl'</span>)</span><br><span class="line">    index_dict, word_vectors,combined = create_dictionaries(model=model,combined=combined)</span><br><span class="line">    <span class="keyword">return</span>   index_dict, word_vectors,combined</span><br></pre></td></tr></table></figure>
<p>训练网络，并保存模型，其中LSTM的实现采用Python中的<a href="http://keras.io/" target="_blank" rel="external">keras</a>库</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">(index_dict,word_vectors,combined,y)</span>:</span></span><br><span class="line"></span><br><span class="line">    n_symbols = len(index_dict) + <span class="number">1</span>  <span class="comment"># 所有单词的索引数，频数小于10的词语索引为0，所以加1</span></span><br><span class="line">    embedding_weights = np.zeros((n_symbols, vocab_dim))<span class="comment">#索引为0的词语，词向量全为0</span></span><br><span class="line">    <span class="keyword">for</span> word, index <span class="keyword">in</span> index_dict.items():<span class="comment">#从索引为1的词语开始，对每个词语对应其词向量</span></span><br><span class="line">        embedding_weights[index, :] = word_vectors[word]</span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(combined, y, test_size=<span class="number">0.2</span>)</span><br><span class="line">    <span class="keyword">print</span> x_train.shape,y_train.shape</span><br><span class="line">    <span class="keyword">return</span> n_symbols,embedding_weights,x_train,y_train,x_test,y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">##定义网络结构</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_lstm</span><span class="params">(n_symbols,embedding_weights,x_train,y_train,x_test,y_test)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Defining a Simple Keras Model...'</span></span><br><span class="line">    model = Sequential()  <span class="comment"># or Graph or whatever</span></span><br><span class="line">    model.add(Embedding(output_dim=vocab_dim,</span><br><span class="line">                        input_dim=n_symbols,</span><br><span class="line">                        mask_zero=<span class="keyword">True</span>,</span><br><span class="line">                        weights=[embedding_weights],</span><br><span class="line">                        input_length=input_length))  <span class="comment"># Adding Input Length</span></span><br><span class="line">    model.add(LSTM(output_dim=<span class="number">50</span>, activation=<span class="string">'sigmoid'</span>, inner_activation=<span class="string">'hard_sigmoid'</span>))</span><br><span class="line">    model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line">    model.add(Dense(<span class="number">1</span>))</span><br><span class="line">    model.add(Activation(<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Compiling the Model...'</span></span><br><span class="line">    model.compile(loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">                  optimizer=<span class="string">'adam'</span>,metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"Train..."</span></span><br><span class="line">    model.fit(x_train, y_train, batch_size=batch_size, nb_epoch=n_epoch,verbose=<span class="number">1</span>, validation_data=(x_test, y_test),show_accuracy=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"Evaluate..."</span></span><br><span class="line">    score = model.evaluate(x_test, y_test,</span><br><span class="line">                                batch_size=batch_size)</span><br><span class="line"></span><br><span class="line">    yaml_string = model.to_yaml()</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'lstm_data/lstm.yml'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> outfile:</span><br><span class="line">        outfile.write( yaml.dump(yaml_string, default_flow_style=<span class="keyword">True</span>) )</span><br><span class="line">    model.save_weights(<span class="string">'lstm_data/lstm.h5'</span>)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Test score:'</span>, score</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型，并保存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Loading Data...'</span></span><br><span class="line">    combined,y=loadfile()</span><br><span class="line">    <span class="keyword">print</span> len(combined),len(y)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Tokenising...'</span></span><br><span class="line">    combined = tokenizer(combined)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Training a Word2vec model...'</span></span><br><span class="line">    index_dict, word_vectors,combined=word2vec_train(combined)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Setting up Arrays for Keras Embedding Layer...'</span></span><br><span class="line">    n_symbols,embedding_weights,x_train,y_train,x_test,y_test=get_data(index_dict, word_vectors,combined,y)</span><br><span class="line">    <span class="keyword">print</span> x_train.shape,y_train.shape</span><br><span class="line">    train_lstm(n_symbols,embedding_weights,x_train,y_train,x_test,y_test)</span><br></pre></td></tr></table></figure>
<h3 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h3><p> 使用LSTM网络在测试集上的准确率为92%，比用SVM分类提高了不少。</p>
<h2 id="代码地址"><a href="#代码地址" class="headerlink" title="代码地址"></a>代码地址</h2><p><a href="https://github.com/BUPTLdy/Sentiment-Analysis" target="_blank" rel="external">https://github.com/BUPTLdy/Sentiment-Analysis</a></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://www.15yan.com/story/huxAyyeuYAj/" target="_blank" rel="external">http://www.15yan.com/story/huxAyyeuYAj/</a></p>
<p><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
<p><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Solve Linear Classifier by SGD]]></title>
      <url>http://buptldy.github.io/2016/06/20/2016-06-20-SVM%20Softmax%20SGD/</url>
      <content type="html"><![CDATA[<p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-6-20/61070235.jpg" alt=""><br><a id="more"></a></p>
<h2 id="线性分类器"><a href="#线性分类器" class="headerlink" title="线性分类器"></a>线性分类器</h2><p>一个线性分类器的基本形式如下所示：<br>$$f(x_i,W,b)=Wx_i+b （1）$$<br>在上面的公式中，如果是对图像经行分类，$x_i$表示对一张图片展开成一个列向量维数为[D,1],矩阵<strong>W</strong>维数为[K,D],向量<strong>b</strong>维数为[K,1]。参数<strong>W</strong>通常成为权值，<strong>b</strong>为偏置。</p>
<h3 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is a bit of magic to make matplotlib figures appear inline in the</span></span><br><span class="line"><span class="comment"># notebook rather than in a new window.</span></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">10.0</span>, <span class="number">8.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">digits=load_digits()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=<span class="number">0.2</span>)</span><br><span class="line">X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">print</span> X_train.shape, y_train.shape</span><br><span class="line"><span class="keyword">print</span> X_val.shape, y_val.shape</span><br><span class="line"><span class="keyword">print</span> X_test.shape, y_test.shape</span><br></pre></td></tr></table></figure>
<pre><code>(1293, 64) (1293,)
(144, 64) (144,)
(360, 64) (360,)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">classes = [<span class="string">'0'</span>, <span class="string">'1'</span>, <span class="string">'2'</span>, <span class="string">'3'</span>, <span class="string">'4'</span>, <span class="string">'5'</span>, <span class="string">'6'</span>, <span class="string">'7'</span>, <span class="string">'8'</span>, <span class="string">'9'</span>]</span><br><span class="line">num_classes = len(classes)</span><br><span class="line">samples_per_class = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> y, cls <span class="keyword">in</span> enumerate(classes):</span><br><span class="line">    idxs = np.flatnonzero(y_train == y)</span><br><span class="line">    idxs = np.random.choice(idxs, samples_per_class, replace=<span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">for</span> i, idx <span class="keyword">in</span> enumerate(idxs):</span><br><span class="line">        plt_idx = i * num_classes + y + <span class="number">1</span></span><br><span class="line">        plt.subplot(samples_per_class, num_classes, plt_idx)</span><br><span class="line">        plt.imshow(X_train[idx].astype(<span class="string">'uint8'</span>).reshape(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">        plt.axis(<span class="string">'off'</span>)</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            plt.title(cls)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-6-20/14165529.jpg" alt=""></p>
<p>比如在数字手写识别数据集中，要分类[0-9]共十类数字手写图片，每张图片的像素为8×8，如上图所示。分类器的目的就是通过训练得到参数<strong>W,b</strong>，应为我们知道输入数据是 $(x_i,y_i)$ （$x_i$是输入图片像素值，$y_i$为对应类别号）是给定而且是固定的，我们的目标就是通过控制参数<strong>W,b</strong>来尽量拟合公式(1), 使得公式(1)能通过参数对输入数据$x_i$计算得到正确的$y_i$。</p>
<p><strong>Bias trick</strong>,在公式(1)中有两个参数<strong>W,b</strong>，通过一个小技巧可以把这两个参数组合在一个矩阵中，通过把$x_i$增加一维，设置值为1，就可以把公式(1)写为：<br>$$f(x_i,W)=Wx_i$$<br>原理如下图所示：<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-6-20/17970270.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_train = np.hstack([X_train, np.ones((X_train.shape[<span class="number">0</span>], <span class="number">1</span>))])</span><br><span class="line">X_val = np.hstack([X_val, np.ones((X_val.shape[<span class="number">0</span>], <span class="number">1</span>))])</span><br><span class="line">X_test = np.hstack([X_test, np.ones((X_test.shape[<span class="number">0</span>], <span class="number">1</span>))])</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> X_train.shape, X_val.shape, X_test.shape</span><br></pre></td></tr></table></figure>
<pre><code>(1293, 65) (144, 65) (360, 65)
</code></pre><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>上面说到使用公式$f(x_i,W)=Wx_i$对输入图片经行每一类的打分，但是开始时线性分类器预测的打分和我们真实的类别可能相差比较远，我们需要一个函数来表示真实的分数和分类器所计算到的分数之间的距离，这个函数就叫损失函数。</p>
<p>比如说我们输入一张图像像素值$x_i$，其真实类别为$y_i$,我们通过分类器计算每类的得分 $f(x_i,W)$ 。 例如 $s_j=f(x_i,W)_ j$ 表示分类器对输入数据 $x_i$ 预测为第j类的可能性，那么损失函数就可以定义为：</p>
<p>$$L_i=\sum _{j\neq y_i}max(0,s_j-s_{y_i}+\Delta) (2)$$</p>
<p>假如我们有三类通过分类器得到每类的分数为[13,-7,11],并假设第一类是正确的类别($y_i=0$), 并假设 $\Delta=10$ ，我们可以通过上述公式计算得到损失函数值为：<br>$$L_i=max(0,−7−13+10)+max(0,11−13+10)$$</p>
<p>我们可以看到第一个max函数求得的值为0，我们可以理解为对第一类的打分13和第二类的打分-7之间的距离为20已经超过我们设置的间隔10，所以不需要惩罚，即这一部分计算得到的损失函数值为0；第一类与第三类的打分距离为2，小于设定的间隔10，所以计算得到损失函数为8。通过上诉例子我们发现损失函数就是用来描述我们对预测的不满意程度，如下图所示，如果预测到的真实类别的分数与错误类别的分数之间的距离都大于我们设置的阈值，则损失函数的值为0。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-6-20/62536190.jpg" alt=""></p>
<p>这种损失函数就称为<strong>hinge loss</strong>，因为$s_j=w_j^Tx_i$ ， $w_j$为矩阵<strong>W</strong>的第$j$行展成的列向量，所以公式(2)可以写为：</p>
<p> $$L_i=\sum _ {j\neq y_i}max(0,w_j^Tx_i-w_{y_i}^Tx_i+\Delta) (3)$$</p>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>上述损失函数用来约束预测打分和真实打分之间的区别，我们好需要一些参数来约束参数矩阵<strong>W</strong>值的大小，L2正则如下所示，会惩罚过大的参数值：</p>
<p>$$R(W)=\sum _ k \sum _ lW _ {k,l}^2$$</p>
<p>所以对整个数据集总的损失函数如下所示：</p>
<p>$$L= \frac {1}{N} \sum _ i \sum _ {j \neq y_i} [max(0, f(x_i;W) _ j -f(x_i;W) _ { y_i} +\Delta)]+\lambda\sum _ k \sum _ lW _ {k,l}^2 $$</p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>对公式(2)的 $w_{y_i}$ 求导，可以得到：</p>
<p>$$\nabla _ {W _ {y _ i}}L_i =- (\sum _ {j \neq y_i}1(w_j^Tx_i-w _ {y_i}^Tx_i + \Delta &gt;0))x_i$$</p>
<p>其中<strong>1</strong>为指示函数，当括号里的条件成立是函数值为1，否则为0。所以上述<strong>对正确类别所对应分类器权值的求导结果就是把错误类别的打分与正确类别打分间距小于阈值的个数再乘以输入数据$x_i$</strong>。</p>
<p>对 $j\neq y_i$ 的其他行，求导结果如下所示，也就是如果这一行所对应的滤波器打分相对于正确的类别分数间隔小于阈值，则对这一行求导所得就是 $x_i$</p>
<p>$$\nabla _ {W _ {j}}L_i =1(w_j^Tx_i-w _ {y_i}^Tx_i + \Delta &gt;0)x_i$$</p>
<p>其中SVM的hinge loss以及梯度计算如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line"></span><br><span class="line">  loss = <span class="number">0.0</span></span><br><span class="line">  dW = np.zeros(W.shape) <span class="comment"># initialize the gradient as zero</span></span><br><span class="line"></span><br><span class="line">  N=X.shape[<span class="number">0</span>]</span><br><span class="line">  D=X.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">  scores = X.dot(W)</span><br><span class="line">  correct_scores = scores[np.arange(N),y]</span><br><span class="line">  margin = np.maximum(np.zeros(scores.shape),scores+<span class="number">1</span>-correct_scores.reshape(N,<span class="number">-1</span>))</span><br><span class="line">  margin[np.arange(N),y] = <span class="number">0</span></span><br><span class="line">  loss = np.sum(margin)</span><br><span class="line">  loss /= N</span><br><span class="line">  loss += <span class="number">0.5</span> * reg * np.sum(W * W)</span><br><span class="line"></span><br><span class="line">  binary = margin</span><br><span class="line">  binary[margin&gt;<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">  row_sum = np.sum(binary, axis=<span class="number">1</span>)</span><br><span class="line">  binary[np.arange(N), y] = -row_sum[np.arange(N)]</span><br><span class="line">  dW = X.T.dot(binary)</span><br><span class="line">  dW /= N</span><br><span class="line">  dW += reg * W</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure>
<h2 id="Softmax分类器"><a href="#Softmax分类器" class="headerlink" title="Softmax分类器"></a>Softmax分类器</h2><p> 分类器相对于SVM分类器来说，增加了一个计算概率的过程，SVM选择得分最大的一类输出，Softmax把所有的得分转换为每一类的概率，如下公式所示：<br>$$P(y_i|x_i;W)=\frac{e^{f _ {y_i}}}{\sum_je^{f_j}}$$<br>其中$f_j$为分类器对每一类的打分。</p>
<p>Softmax 分类器的损失函数为<strong>cross-entropy loss</strong>，如下所示，其实就是正确类别概率取对数再乘以-1。<br>$$L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.5in} \text{或等于} \hspace{0.5in} L_i = -f_{y_i} + \log\sum_j e^{f_j}$$</p>
<p>Softmax 和 SVM分类器的联系区别如下图所示：<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-6-20/8049276.jpg" alt=""></p>
<p>cross-entropy loss求导</p>
<p>对$w_{y_i}$:<br>$$\nabla _ {W _ {y _ i}}L_i =-x_i+p _ {y_i}x_i$$</p>
<p>对$w_j(j \neq y_i)$:<br>$$\nabla _ {W _ {y _ i}}L_i =p _ {j}x_i$$<br>其中 $p_{j}$ 为Softmax分类器输出为第 $j$ 类的概率。</p>
<p>Softmax的cross-entropy loss以及梯度计算如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">  <span class="string">"""</span><br><span class="line">  Softmax loss function, vectorized version.</span><br><span class="line"></span><br><span class="line">  Inputs and outputs are the same as softmax_loss_naive.</span><br><span class="line">  """</span></span><br><span class="line">  <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">  loss = <span class="number">0.0</span></span><br><span class="line">  dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">  N, D = X.shape</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  scores = X.dot(W) <span class="comment">#(N,C)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  p = np.exp(scores.T)/np.sum(np.exp(scores.T),axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">  p=p.T</span><br><span class="line"></span><br><span class="line">  loss = -np.sum(np.log(p[np.arange(N), y]))</span><br><span class="line"></span><br><span class="line">  p[np.arange(N), y] = p[np.arange(N), y]<span class="number">-1</span></span><br><span class="line"></span><br><span class="line">  dW = X.T.dot(p)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  loss /=N</span><br><span class="line">  dW /=N</span><br><span class="line">  loss += <span class="number">0.5</span> * reg * np.sum(W * W)</span><br><span class="line">  dW += reg * W</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure>
<h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><p>在大数据集的训练中，计算所有数据的损失函数只更新一次参数是很浪费的行为。一个通常的做法是计算一批训练数据的梯度然后更新，能用这个方法的是基于所以训练数据都是相关的假设，每一批数据的梯度是所有数据的一个近似估计。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearClassifier</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.W = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y, learning_rate=<span class="number">1e-3</span>, reg=<span class="number">1e-5</span>, num_iters=<span class="number">100</span>,</span><br><span class="line">            batch_size=<span class="number">200</span>, verbose=False)</span>:</span></span><br><span class="line"></span><br><span class="line">    num_train, dim = X.shape</span><br><span class="line">    num_classes = np.max(y) + <span class="number">1</span> <span class="comment"># assume y takes values 0...K-1 where K is number of classes</span></span><br><span class="line">    <span class="keyword">if</span> self.W <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">      <span class="comment"># lazily initialize W</span></span><br><span class="line">      self.W = <span class="number">0.001</span> * np.random.randn(dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Run stochastic gradient descent to optimize W</span></span><br><span class="line">    loss_history = []</span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> xrange(num_iters):</span><br><span class="line">      X_batch = <span class="keyword">None</span></span><br><span class="line">      y_batch = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">      index=np.random.choice(num_train,batch_size,replace=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">      X_batch=X[index]</span><br><span class="line">      y_batch = y[index]</span><br><span class="line"></span><br><span class="line">      loss, grad = self.loss(X_batch, y_batch, reg)</span><br><span class="line">      loss_history.append(loss)</span><br><span class="line"></span><br><span class="line">      self.W -= learning_rate*grad</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> verbose <span class="keyword">and</span> it % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'iteration %d / %d: loss %f'</span> % (it, num_iters, loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss_history</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    y_pred = np.zeros(X.shape[<span class="number">1</span>])</span><br><span class="line">    scores = X.dot(self.W)</span><br><span class="line"></span><br><span class="line">    y_pred = np.argmax(scores,axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X_batch, y_batch, reg)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearSVM</span><span class="params">(LinearClassifier)</span>:</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X_batch, y_batch, reg)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> svm_loss_vectorized(self.W, X_batch, y_batch, reg)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Softmax</span><span class="params">(LinearClassifier)</span>:</span></span><br><span class="line">  <span class="string">""" A subclass that uses the Softmax + Cross-entropy loss function """</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X_batch, y_batch, reg)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> softmax_loss_vectorized(self.W, X_batch, y_batch, reg)</span><br></pre></td></tr></table></figure>
<p>训练SVM：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">svm = LinearSVM()</span><br><span class="line">svm.train(X_train, y_train, learning_rate=<span class="number">1e-3</span>, reg=<span class="number">1e0</span>, num_iters=<span class="number">400</span>,verbose=<span class="keyword">True</span>)</span><br><span class="line">y_train_pred = svm.predict(X_train)</span><br><span class="line">acc_train = np.mean(y_train == y_train_pred)</span><br><span class="line">y_val_pred = svm.predict(X_val)</span><br><span class="line">acc_val = np.mean(y_val == y_val_pred)</span><br><span class="line">y_test_pred = svm.predict(X_test)</span><br><span class="line">acc_test = np.mean(y_test == y_test_pred)</span><br><span class="line"><span class="keyword">print</span> train_accuracy, val_accuracy, acc_test</span><br></pre></td></tr></table></figure>
<pre><code>iteration 0 / 400: loss 8.953550
iteration 100 / 400: loss 0.208299
iteration 200 / 400: loss 0.287735
iteration 300 / 400: loss 0.233046
0.940448569219 0.951388888889 0.952777777778
</code></pre><h2 id="对线性分类器的直观解释"><a href="#对线性分类器的直观解释" class="headerlink" title="对线性分类器的直观解释"></a>对线性分类器的直观解释</h2><p>参数<strong>W</strong>的每一行可以理解为一个图像模板，每个类别的得分就是输入图片与每一行的图片模板<strong>内积</strong>的结果，看输出的图片最符合哪个图片模板，也就是最可能符合哪一类。也就是说通过训练，分类器的每一行学习到了每类图片的模板，如下图所示，线性SVM分类器学习到的每一类数字的模板图片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">w = svm.W[:<span class="number">-1</span>,:] <span class="comment"># strip out the bias</span></span><br><span class="line"><span class="comment">#print w</span></span><br><span class="line">w = w.reshape(<span class="number">8</span>, <span class="number">8</span>, <span class="number">10</span>)</span><br><span class="line">w_min, w_max = np.min(w), np.max(w)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">10</span>):</span><br><span class="line">  plt.subplot(<span class="number">2</span>, <span class="number">5</span>, i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#Rescale the weights to be between 0 and 255</span></span><br><span class="line">  wimg = <span class="number">255.0</span> * (w[:, :, i].squeeze() - w_min) / (w_max - w_min)</span><br><span class="line">  <span class="comment">#wimg = w[:, :, i]</span></span><br><span class="line">  plt.imshow(wimg.astype(<span class="string">'uint8'</span>))</span><br><span class="line">  plt.axis(<span class="string">'off'</span>)</span><br><span class="line">  plt.title(classes[i])</span><br></pre></td></tr></table></figure>
<p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-6-20/94762256.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sf = Softmax()</span><br><span class="line">sf.train(X_train, y_train, learning_rate=<span class="number">1e-2</span>, reg=<span class="number">0.5</span>, num_iters=<span class="number">500</span>,verbose=<span class="keyword">True</span>)</span><br><span class="line">y_train_pred = sf.predict(X_train)</span><br><span class="line">acc_train = np.mean(y_train == y_train_pred)</span><br><span class="line">y_val_pred = sf.predict(X_val)</span><br><span class="line">acc_val = np.mean(y_val == y_val_pred)</span><br><span class="line">y_test_pred = sf.predict(X_test)</span><br><span class="line">acc_test = np.mean(y_test == y_test_pred)</span><br><span class="line"><span class="keyword">print</span> train_accuracy, val_accuracy, acc_test</span><br></pre></td></tr></table></figure>
<pre><code>iteration 0 / 500: loss 2.301230
iteration 100 / 500: loss 0.362278
iteration 200 / 500: loss 0.361298
iteration 300 / 500: loss 0.352944
iteration 400 / 500: loss 0.365702
0.940448569219 0.951388888889 0.95
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">w = sf.W[:<span class="number">-1</span>,:] <span class="comment"># strip out the bias</span></span><br><span class="line"><span class="keyword">print</span> w.shape</span><br><span class="line">w = w.reshape(<span class="number">8</span>, <span class="number">8</span>, <span class="number">10</span>)</span><br><span class="line">w_min, w_max = np.min(w), np.max(w)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">10</span>):</span><br><span class="line">  plt.subplot(<span class="number">2</span>, <span class="number">5</span>, i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#Rescale the weights to be between 0 and 255</span></span><br><span class="line">  wimg = <span class="number">255.0</span> * (w[:, :, i].squeeze() - w_min) / (w_max - w_min)</span><br><span class="line">  <span class="comment">#wimg = w[:, :, i]</span></span><br><span class="line">  plt.imshow(wimg.astype(<span class="string">'uint8'</span>))</span><br><span class="line">  plt.axis(<span class="string">'off'</span>)</span><br><span class="line">  plt.title(classes[i])</span><br></pre></td></tr></table></figure>
<pre><code>(64, 10)
</code></pre><p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-6-20/30387585.jpg" alt=""></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://cs231n.github.io/" target="_blank" rel="external">CS231n: Convolutional Neural Networks for Visual Recognition. </a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Speed-up with Cython and Numpy in Python]]></title>
      <url>http://buptldy.github.io/2016/06/15/2016-06-15-Python%20Cython/</url>
      <content type="html"><![CDATA[<p><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-6-15/20612896.jpg" alt=""><br></center><br><a id="more"></a></p>
<h2 id="Cython代码和Python代码区别"><a href="#Cython代码和Python代码区别" class="headerlink" title="Cython代码和Python代码区别"></a>Cython代码和Python代码区别</h2><p>代码运行在<a href="https://ipython.org/notebook.html" target="_blank" rel="external">IPython-Notebook</a>中，在IPython-Notebook中导入cython环境。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%load_ext cython</span><br></pre></td></tr></table></figure>
<p>Cython可以在Python中掺杂C和C++的静态类型，cython编译器可以把Cython源码编译成C或C++代码，编译后的代码可以单独执行或者作为Python中的模型使用。Cython中的强大之处在于可以把Python和C结合起来，它使得看起来像Python语言的Cython代码有着和C相似的运行速度。</p>
<p>我们使用一个简单的Fibonacci函数来比较下Python和Cython的区别：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#python</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fib1</span><span class="params">(n)</span>:</span></span><br><span class="line">    a,b=<span class="number">0.0</span>,<span class="number">1.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        a,b=a+b,a</span><br><span class="line">    <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure>
<p>下面代码使用<code>%%cython</code>标志表示下面的代码使用cython编译<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%%cython</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fib2</span><span class="params">(int n)</span>:</span></span><br><span class="line">    cdef double a=<span class="number">0.0</span>, b=<span class="number">1.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        a,b = a+b,a</span><br><span class="line">    <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure></p>
<p>通过比较上面的代码，为了把Python中的动态类型转换为Cython中的静态类型，我们用<code>cdef</code>来定义C语言中的变量<code>i</code>，<code>a</code>，<code>b</code>。<br>我们用C语言实现Fibonacci函数，然后通过Cython用Python封装，其中<code>cfib.h</code>为Fibonacci函数C语言实现，如下：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">cfib</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> i;</span><br><span class="line">  <span class="keyword">double</span> a=<span class="number">0.0</span>, b=<span class="number">1.0</span>, tmp;</span><br><span class="line">  <span class="keyword">for</span> (i=<span class="number">0</span>; i&lt;n; ++i) &#123;</span><br><span class="line">    tmp = a; a = a + b; b = tmp;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> a;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%%cython</span><br><span class="line"></span><br><span class="line">cdef extern <span class="keyword">from</span> <span class="string">"/home/ldy/MEGA/python/cython/cfib.h"</span>:</span><br><span class="line">    double cfib(int n)  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fib3</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="string">"""Returns the nth Fibonacci number."""</span></span><br><span class="line">    <span class="keyword">return</span> cfib(n)</span><br></pre></td></tr></table></figure>
<p>比较不同方法的运行时间：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">%timeit result=fib1(<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">%timeit result=fib2(<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">%timeit result=fib3(<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<pre><code>10000 loops, best of 3: 73.6 µs per loop
1000000 loops, best of 3: 1.94 µs per loop
1000000 loops, best of 3: 1.92 µs per loop
</code></pre><h2 id="Cython代码的编译"><a href="#Cython代码的编译" class="headerlink" title="Cython代码的编译"></a>Cython代码的编译</h2><p>Cython代码的编译为Python可调用模块的过程主要分为两步：第一步是cython编译器把Cython代码优化成C或C++代码；第二步是使用C或C++编译器编译产生的C或C++代码得到Python可调用的模块。</p>
<p>我们通过一个<code>setup.py</code>脚本来编译上面写的<code>fib.pyx</code>Cython代码，如下所示，关键就在第三行，<code>cythonize</code>函数的作用是通过cython编译器把Cython代码转换为C代码，<code>setup</code>函数则是把产生的C代码转换成Python可调用模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> distutils.core <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> Cython.Build <span class="keyword">import</span> cythonize</span><br><span class="line">setup(ext_modules=cythonize(<span class="string">'fib.pyx'</span>))</span><br><span class="line"><span class="comment">#setup(ext_modules=cythonize('*.pyx','fib1.pyx'))也可以一次编译多个Cython文件</span></span><br></pre></td></tr></table></figure>
<p>写好<code>setup.py</code>文件后，就可以通过下述命令执行编译：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python setup.py build_ext --inplace</span><br></pre></td></tr></table></figure></p>
<p>执行后产生了<code>fib.c</code>代码以及<code>fib.so</code>文件，以及一些中间结果保存在build文件夹里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.chdir(<span class="string">'/home/ldy/MEGA/python/cython/test'</span>)</span><br><span class="line">os.getcwd()</span><br><span class="line">!ls</span><br></pre></td></tr></table></figure>
<pre><code>build  fib.c  fib.pyx  fib.so  setup.py
</code></pre><p>通过Python调用产出的<code>fib.so</code>模块：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> fib</span><br><span class="line">fib.fib2(<span class="number">90</span>)</span><br></pre></td></tr></table></figure>
<pre><code>2.880067194370816e+18
</code></pre><h2 id="Cython中类型的定义"><a href="#Cython中类型的定义" class="headerlink" title="Cython中类型的定义"></a>Cython中类型的定义</h2><p>为什么Cython和Python比会提高很多性能，主要原因有两点：一是Python是解释型语言，在运行之前Python解释器把Python代码解释成Python字节码运行在Python虚拟机上，Python虚拟机把Python字节码最终翻译成CPU能执行的机器码；而Cython代码是事先直接编译成可被Python调用的机器码，在运行时可直接执行。第二个主要的原因是Python是动态类型，Python解释器在解释时需要判断类型，然后再提取出底层能够运行的数据以及操作；然而C语言等比较底层的语言是静态类型，编译器直接提取数据进行操作产生机器码。</p>
<p>Cython中使用<code>cdef</code>来定义静态类型：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cdef int i</span><br><span class="line">cdef int j</span><br><span class="line">cdef float f</span><br></pre></td></tr></table></figure></p>
<p>也可以一次定义多个：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cdef:</span><br><span class="line">    int i</span><br><span class="line">    int j</span><br><span class="line">    float f</span><br></pre></td></tr></table></figure></p>
<p>Cython中还允许在静态类型和动态类型同时存在及相互赋值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%%cython</span><br><span class="line">cdef int a=<span class="number">1</span>,b=<span class="number">2</span>,c=<span class="number">3</span></span><br><span class="line">list_of_ints=[a,b,c]</span><br><span class="line">list_of_ints.append(<span class="number">4</span>)</span><br><span class="line">a=list_of_ints[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">print</span> a,list_of_ints</span><br></pre></td></tr></table></figure>
<pre><code>2 [1, 2, 3, 4]
</code></pre><p>声明Python类型为静态类型，Cython支持把一些Python内置的如<code>list</code>,<code>tuple</code>,<code>dict</code>等类型声明为静态类型，这样声明使得它们能像正常Python类型一样使用，但是需要约束成只能是他们所申明的类型，不能随意变动。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">%%cython</span><br><span class="line">cdef:</span><br><span class="line">    list names</span><br><span class="line">    dict name_num</span><br><span class="line"></span><br><span class="line">name_num=&#123;<span class="string">'jerry'</span>:<span class="number">1</span>,<span class="string">'Tom'</span>:<span class="number">2</span>,<span class="string">'Bell'</span>:<span class="number">3</span>&#125;</span><br><span class="line">names=list(name_num.keys())</span><br><span class="line"><span class="keyword">print</span> names</span><br><span class="line">other_names=names<span class="comment">#动态类型可以从静态类型的Python对象初始化</span></span><br><span class="line"><span class="keyword">del</span> other_names[<span class="number">0</span>]<span class="comment">#因为引用了同一个list，所以都会删除第一个元素</span></span><br><span class="line"><span class="keyword">print</span> names,other_names</span><br><span class="line">other_names=tuple(other_names)<span class="comment">#names和other_names的区别在于names只能是list类型，</span></span><br><span class="line"><span class="keyword">print</span> other_names           <span class="comment">#other_names可以引用任何类型</span></span><br></pre></td></tr></table></figure>
<pre><code>[&apos;Bell&apos;, &apos;jerry&apos;, &apos;Tom&apos;]
[&apos;jerry&apos;, &apos;Tom&apos;] [&apos;jerry&apos;, &apos;Tom&apos;]
(&apos;jerry&apos;, &apos;Tom&apos;)
</code></pre><h2 id="Cython中numpy的使用"><a href="#Cython中numpy的使用" class="headerlink" title="Cython中numpy的使用"></a>Cython中numpy的使用</h2><p>我们先构造一个函数来测试下使用纯Python时的运算时间来做对比，这个函数的作用是对一副输入图像求梯度（不必过分关注函数的功能，在这只是使用这个函数作为测试）。函数的输入数据是<code>indata</code>一个像素为1400*1600的图片；输出为<code>outdata</code>,为每个像素梯度值，下面是这个函数的纯Python实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">indata = np.random.rand(<span class="number">1400</span>,<span class="number">1600</span>)</span><br><span class="line">outdata = np.zeros(shape=indata.shape, dtype=<span class="string">'float64'</span>)  <span class="comment"># eventually holds our output</span></span><br><span class="line"><span class="keyword">from</span> numpy.lib <span class="keyword">import</span> pad</span><br><span class="line">print(<span class="string">"shape before"</span>, indata.shape)</span><br><span class="line">indata = pad(indata, (<span class="number">1</span>, <span class="number">1</span>), <span class="string">'reflect'</span>, reflect_type=<span class="string">'odd'</span>)  <span class="comment"># allow edge calcs</span></span><br><span class="line">print(<span class="string">"shape after"</span>, indata.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">slope</span><span class="params">(indata, outdata)</span>:</span></span><br><span class="line">    I = outdata.shape[<span class="number">0</span>]</span><br><span class="line">    J = outdata.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(I):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(J):</span><br><span class="line">            <span class="comment"># percent slope using Zevenbergen-Thorne method</span></span><br><span class="line">            <span class="comment"># assume edges added, inarr is offset by one on both axes cmp to outarr</span></span><br><span class="line">            dzdx = (indata[i+<span class="number">1</span>, j] - indata[i+<span class="number">1</span>, j+<span class="number">2</span>]) / <span class="number">2</span>  <span class="comment"># assume cellsize == one unit, otherwise (2 * cellsize)</span></span><br><span class="line">            dzdy = (indata[i, j+<span class="number">1</span>] - indata[i+<span class="number">2</span>, j+<span class="number">1</span>]) / <span class="number">2</span></span><br><span class="line">            slp = math.sqrt((dzdx * dzdx) + (dzdy * dzdy)) * <span class="number">100</span>  <span class="comment"># percent slope (take math.atan to get angle)</span></span><br><span class="line">            outdata[i, j] = slp</span><br></pre></td></tr></table></figure>
<pre><code>(&apos;shape before&apos;, (1400, 1600))
(&apos;shape after&apos;, (1402, 1602))
</code></pre><p>测试运行时间，为5.31 s每个循环</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%timeit slope(indata, outdata)</span><br></pre></td></tr></table></figure>
<pre><code>1 loop, best of 3: 5.31 s per loop
</code></pre><p>重置输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_outdata</span><span class="params">()</span>:</span></span><br><span class="line">    outdata = np.zeros(shape=indata.shape, dtype=<span class="string">'float64'</span>)</span><br><span class="line"></span><br><span class="line">reset_outdata()</span><br></pre></td></tr></table></figure>
<p>使用Cython重写求图像梯度函数,其中函数<code>slope_cython2</code>使用Cython里的numpy类型，并重写了里面的开方函数，其中<code>%%cython -a</code>表示使用cython编译Cython代码，并可以对照显示编译器把Cython代码编译成的C代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">%%cython</span><br><span class="line"><span class="keyword">import</span> cython</span><br><span class="line">cimport numpy <span class="keyword">as</span> np</span><br><span class="line">ctypedef np.float64_t DTYPE_t</span><br><span class="line"><span class="meta">@cython.boundscheck(False)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">slope_cython2</span><span class="params">(np.ndarray[DTYPE_t, ndim=<span class="number">2</span>] indata, np.ndarray[DTYPE_t, ndim=<span class="number">2</span>] outdata)</span>:</span></span><br><span class="line">    cdef int I, J</span><br><span class="line">    cdef int i, j, x</span><br><span class="line">    cdef double k, slp, dzdx, dzdy</span><br><span class="line">    I = outdata.shape[<span class="number">0</span>]</span><br><span class="line">    J = outdata.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(I):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(J):</span><br><span class="line">            dzdx = (indata[i+<span class="number">1</span>, j] - indata[i+<span class="number">1</span>, j+<span class="number">2</span>]) / <span class="number">2</span></span><br><span class="line">            dzdy = (indata[i, j+<span class="number">1</span>] - indata[i+<span class="number">2</span>, j+<span class="number">1</span>]) / <span class="number">2</span></span><br><span class="line">            k = (dzdx * dzdx) + (dzdy * dzdy)</span><br><span class="line">            slp = k**<span class="number">0.5</span> * <span class="number">100</span></span><br><span class="line">            outdata[i, j] = slp</span><br></pre></td></tr></table></figure>
<p>测试运行时间：208ms,快了有25倍左右</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%timeit slope_cython2(indata, outdata)</span><br></pre></td></tr></table></figure>
<pre><code>1 loop, best of 3: 208 ms per loop
</code></pre><h2 id="Cython中多进程"><a href="#Cython中多进程" class="headerlink" title="Cython中多进程"></a>Cython中多进程</h2><p>Cython还支持<a href="http://docs.cython.org/src/userguide/parallelism.html" target="_blank" rel="external">并行运算</a>,后台由OpenMP支持，所以在编译Cython语言时需要加上如下代码第一行所示的标记。在进行并行计算时，需使用<code>nogil</code>关键词来释放Python里的<a href="https://wiki.python.org/moin/GlobalInterpreterLock" target="_blank" rel="external">GIL</a>锁,当代码中只有C而没有Python对象时，这样做是安全的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">%%cython --compile-args=-fopenmp --link-args=-fopenmp --force</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython</span><br><span class="line"><span class="keyword">from</span> cython.parallel <span class="keyword">import</span> prange, parallel</span><br><span class="line"></span><br><span class="line"><span class="meta">@cython.boundscheck(False)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">slope_cython_openmp</span><span class="params">(double [:, :] indata, double [:, :] outdata)</span>:</span></span><br><span class="line">    cdef int I, J</span><br><span class="line">    cdef int i, j, x</span><br><span class="line">    cdef double k, slp, dzdx, dzdy</span><br><span class="line">    I = outdata.shape[<span class="number">0</span>]</span><br><span class="line">    J = outdata.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">with</span> nogil, parallel(num_threads=<span class="number">4</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> prange(I, schedule=<span class="string">'dynamic'</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(J):</span><br><span class="line">                dzdx = (indata[i+<span class="number">1</span>, j] - indata[i+<span class="number">1</span>, j+<span class="number">2</span>]) / <span class="number">2</span></span><br><span class="line">                dzdy = (indata[i, j+<span class="number">1</span>] - indata[i+<span class="number">2</span>, j+<span class="number">1</span>]) / <span class="number">2</span></span><br><span class="line">                k = (dzdx * dzdx) + (dzdy * dzdy)</span><br><span class="line">                slp = k**<span class="number">0.5</span> * <span class="number">100</span></span><br><span class="line">                outdata[i, j] = slp</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">reset_outdata()</span><br><span class="line">%timeit slope_cython_openmp(indata, outdata)</span><br></pre></td></tr></table></figure>
<pre><code>10 loops, best of 3: 78.2 ms per loop
</code></pre><p>测试的时间如上所示，多进程大概快了2.7倍左右。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Classification in Remote Sensing Optical Images by CNNs]]></title>
      <url>http://buptldy.github.io/2016/06/12/2016-06-12-CNN%20Remote%20Sensing%20Optical%20Images/</url>
      <content type="html"><![CDATA[<p><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-6-11/17213122.jpg" alt=""><br></center><br><a id="more"></a></p>
<h2 id="CNN简介"><a href="#CNN简介" class="headerlink" title="CNN简介"></a>CNN简介</h2><p>从06年开始，深度结构学习方法（深度学习或者分层学习方法）作为机器学习领域的新的研究方向出现。由于芯片处理性能的巨大提升，数据爆炸性增长，在过去的短短几年时间，深度学习技术得到快速发展，已经深深的影响了学术领域，其研究涉及的应用领域包括计算机视觉、语音识别、对话语音识别、图像特征编码、语意表达分类、自然语言理解、手写识别、音频处理、信息检索、机器人学。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-22/96930177.jpg" alt=""><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-22/75458021.jpg" alt=""></p>
<p>由于深度学习在众多领域表现比较好的性能，越来越多的学术机构把目光投入深度学习领域。今年来活跃在机器学习领域的研究机构包括众多高校比如斯坦福，伯克利，还有一些企业例如Google，IBM 研究院，微软研究院，FaceBook，百度等等。</p>
<h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p>人工神经网络是一种模仿生物神经网络(动物的中枢神经系统，特别是大脑)的结构和功能的数学模型或计算模型，简单结构如下图所示，包含了输入层，隐含层，和输出层，其中隐含层可能有多层。在神经网络中每个神经元都和它前一层的所有节点相连接，称之为全连接，其中每个连接以一定的权值相连接，<strong>网络训练的过程就是得到权值的过程</strong>。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-22/86340649.jpg" alt=""></p>
<p>不管是机器学习还是深度学习实际上都是在解决分类问题，当数据线性可分时，一个sigmoid函数就可以把数据分开，如下图所示，其中两类数据是线性可分的，我们只需要神经网络的输入和输出层就可以把两类数据分开，其中黄色的连线表示权值为负，蓝色的连线表示权值为正，连线的粗细表示权值的绝对值大小。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-22/47661239.jpg" alt=""></p>
<p>如下图，当原本的数据不可分时，我们就需要对数据进行一些非线性的变化，使得数据可分，而神经网络中的隐含层的作用就是对线性不可分的数据进行非线性变化，下图中包含了4个隐含层节点，数据被正确的分类。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-22/28174683.jpg" alt=""></p>
<h3 id="卷积神经网络-CNN"><a href="#卷积神经网络-CNN" class="headerlink" title="卷积神经网络(CNN)"></a>卷积神经网络(CNN)</h3><p>一维的CNN如下所示，和人工神经网络相比，CNN中的卷积层只与前一层节点的部分节点相连，称为局部连接，且卷积层中的每个神经元的权值相等，这一属性称为权值共享。卷积神经网络为什么有卷积两个字，就是因为这两个属性：局部连接，权值相等，具体原因可参考<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/" target="_blank" rel="external">http://colah.github.io/posts/2014-07-Understanding-Convolutions/</a>。下图中的max层成为池化层(pooling),下图为max pooling ，就是对两个神经元的输出取其中的较大值。池化操作能够降低特征的维度(相比使用所有提取得到的特征)，同时还会改善结果(不容易过拟合)，池化单元也具有一定的平移不变性。下图中的B层为第二层卷积层卷积层，F层为全连接层，也就是上面所说的人工神经网络。</p>
<p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-22/35987179.jpg" alt=""></p>
<p>二维卷积神经网络如下所示，二维数据的输入可以看成是一张图像的每个像素值，<strong>卷积层看做是一个滤波器对图像提取特征</strong>，max pooling层相当于对图像进行更高维的抽象，然后后面连接全连接层(也就是传统的人工神经网络)进行分类。所以总的说来，利用CNN进行图像处理就是前面的卷积层对图像进行特征提取，经过学习提取出利于图像分类的特征，然后对提取出的特征利用人工神经网络进行分类。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-22/1408005.jpg" alt=""></p>
<p><strong>训练</strong>：上面说到了网络训练的过程就是得到权值的过程，我们在开始训练之前网络的权值是随机初始化的，也就是我们的图片滤波器是随机初始化的。比如我们输入一张图片，随机初始化的CNN分类告诉我们有6%的可能是一个<code>网球场</code>，但实际上我们告诉CNN这是一个<code>飞机场</code>，然后其中会有一个<a href="https://zh.wikipedia.org/wiki/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95" target="_blank" rel="external">反向传播</a>的处理过程来稍稍改变滤波器的参数以便它下次碰到相同的图片时会更可能的预测为<code>网球场</code>。然后我们对我们的训练数据重复这一过程，CNN里的滤波器会逐渐的调整到能够提取我们图片里的供我们分类的重要特征。</p>
<p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-7-8/73324267.jpg" alt=""></p>
<h2 id="数据集分析"><a href="#数据集分析" class="headerlink" title="数据集分析"></a>数据集分析</h2><p>UC Merced Land Use数据集包含21类土地类型，每类图像为100张，每张图像的像素为256*256。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-22/46888988.jpg" alt=""><br>数据集特点，数据集比较小，每一类只有100张图片，这个数据集还有其他的一些特点比如类间距离小，如下图所示，不同类的图片之间很相似。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-23/95817534.jpg" alt=""></p>
<p>类内距离大，同类图片之间差别较大，如下图所示：<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-23/31732082.jpg" alt=""></p>
<p>这些特点都是不利于图片的分类的，尤其是数据量太小，如果从头开始用数据集来训练网络肯定会造成严重的过拟合。考虑到这种情况，一个解决方法就是使用训练好的网络进行微调以适应我们自己的数据集，这种方法不仅能解决数据集小的问题，也能大大加快训练的速度。</p>
<h2 id="网络微调"><a href="#网络微调" class="headerlink" title="网络微调"></a>网络微调</h2><p>网络微调就是使用事先已经训练好的网络，对网络进行微小的改造再训练以适用与我们自己的数据库。为什么别人训练好的网络，我们自己拿到改改就能使用呢？就像之前所说的，CNN的卷积层是用来提取图像的特征的，事实上图片的线条一级色彩纹理大致上是一样的，也就是说一个训练好CNN网络的卷积层也可以用来提取其他数据集图像的特征，因为图像的特征基本相似。特别的，能够使用网络微调的一个重要因素是使用的事先训练好的网络使用的数据集要和我们自己的训练集图像之间的‘距离’要比较小。因为我们的数据集是光学遥感图像，所以和我们的光学图像在底层上的特征有非常强的相似性。</p>
<p>下图是Imagenet数据集的部分图片，也是我们要使用的预先训练好的所用网络的数据集。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-23/54435346.jpg" alt=""><br>基于遥感SAR图像每个像素级别的统计特性，这种用光学图像训练好的网络微调的方法是不适用与SAR图像分类的。SAR图像如下所示，直观上看也与光学图像差别很大。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-23/59029562.jpg" alt=""></p>
<p>我们选择Caffe里预先使用Imagnet训练好的CaffeNet网络来经行微调，CaffeNet网络结构如下所示，fc6前为CNN中的卷积层用来提取图像特征，f6、fc7、fc8为全连接层(可以看成是人工神经网络的输入层，隐含层，输出层)，因为CaffeNet网络是用来分类1000类的图像的，所以最后一层有1000个神经元。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-23/63655708.jpg" alt=""></p>
<p>而我们的数据集是分开21类的图像，所以微调网络中的调整主要就体现在这里，修改上述网络以使用我们自己的数据集，如下所示，只要把网络的输出层改为21个神经元即可。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-23/18035393.jpg" alt=""></p>
<p>我们说的要使用要使用预先训练好的网络就是要使用它事先训练好的权值，比较上述两个网络，只有最后一层不同，所以它们的其他层的权值的维数都是相同的，所以我们把CaffeNet训练好的权值直接用在我们自己定义的网络上，最后一层的权值则随机初始化并设置较大的学习速率，然后就可以用我们定义好的网络训练我们自己的数据集。</p>
<p>定义好网络之后就可以开始训练了，把数据集按4:1分为训练集和测试集，在测试集上的预测准确率在92%左右。</p>
<p>还有一种常用的方法是不用CNN的最后一层分类，用CNN提取到的特征用SVM来分类，也能达到不错的效果。在这里我们提取fc7层输出的特征，根据上面定义的网络结构，fc7层共有4096个神经元，所以每张图片的特征维数为4096维，维数比较大，所以我们使用SVM的线性核即可达到分类效果。</p>
<h2 id="结果展示与分析"><a href="#结果展示与分析" class="headerlink" title="结果展示与分析"></a>结果展示与分析</h2><p>fine-turning结果展示：<br>其中对预测结果做了一些可视化展示，左图表示为预测前五类的概率，左右为图片真实的类别。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-23/60490439.jpg" alt=""><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-23/91330851.jpg" alt=""><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-23/20682353.jpg" alt=""></p>
<p>CNN提取特征，SVM分类结果展示：<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-23/64622617.jpg" alt=""><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-23/23497392.jpg" alt=""><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-23/62208562.jpg" alt=""></p>
<p>从两种方法中可以看出，虽然都分类正确了，但用SVM作为分类器的正确分类的概率更高。</p>
<p>fine-turning方法每个类别的准确率：<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-23/23347725.jpg" alt=""><br>从上图中我们可以看出，tenniscount类别的预测准确率最低，我们来看看有哪些tenniscount类是预测错了的：<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-23/83947048.jpg" alt=""><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-23/49629661.jpg" alt=""><br>从上面两个图片中可以看出，其实并不能说是预测错误，因为上面两张图中既包含了tenniscount类和CNN预测的类别，可以说本来就是有两个类。</p>
<h3 id="t-sne特征降维可视化"><a href="#t-sne特征降维可视化" class="headerlink" title="t-sne特征降维可视化"></a>t-sne特征降维可视化</h3><p>对CNN中第七层提取到的4096维特征经行降维可视化，从下图可以看出，分类准备率比较低的类别靠的都比较紧密，难以区分。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-7-8/84288255.jpg" alt=""></p>
<p>CNN+SVM每个类别的准确率：<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-23/97608126.jpg" alt=""></p>
<h2 id="CNN中间层可视化"><a href="#CNN中间层可视化" class="headerlink" title="CNN中间层可视化"></a>CNN中间层可视化</h2><p>神经网络不仅仅是一个黑盒子，我们可以查看一些中间结果和参数。上面我们也说了一个卷积层就相当与一个图像滤波器，在上面的网络的第一层的卷积层中我们定义了96个滤波器，96个滤波器可视化如下图所示，学过图像处理的同学都知道，下图中第一个滤波器是提取斜向下的边缘特征，第二个滤波器是提取斜向上的边缘特征，前面的滤波器大多数是在提取边缘特征，后面的大多是在统计颜色特征。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-23/15829905.jpg" alt=""></p>
<p>我们输入一张图片，并输出其经过第一层卷积层滤波器滤波后的输出：<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-23/12013586.jpg" alt=""><br>从第一层滤波后的结果可以看出，前面两个滤波器就是在显示斜向下和斜向上的边缘。</p>
<p>第五层卷积层滤波器输出如下图所示，高层的滤波器输出比较抽象。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-23/58707834.jpg" alt=""></p>
<h2 id="总结及展望"><a href="#总结及展望" class="headerlink" title="总结及展望"></a>总结及展望</h2><p>当我们数据集不够的时候可以使用微调的方法，探索CNN怎么应用于SAR图像分类，解决图片类标签的分类问题。</p>
<h2 id="代码地址"><a href="#代码地址" class="headerlink" title="代码地址"></a>代码地址</h2><p><a href="https://github.com/BUPTLdy/Land_Use_CNN" target="_blank" rel="external">land_use_CNN</a></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://vision.ucmerced.edu/datasets/landuse.html" target="_blank" rel="external">http://vision.ucmerced.edu/datasets/landuse.html</a><br><a href="http://ufldl.stanford.edu/wiki/index.php/%E6%B1%A0%E5%8C%96" target="_blank" rel="external">http://ufldl.stanford.edu/wiki/index.php/%E6%B1%A0%E5%8C%96</a><br><a href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/" target="_blank" rel="external">http://colah.github.io/posts/2014-07-Conv-Nets-Modular/</a><br><a href="http://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=xor&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4&amp;seed=0.38108&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification" target="_blank" rel="external">Tinker With a Neural Network Right Here in Your Browser</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Force-Directed Graph Visualization Based in Location]]></title>
      <url>http://buptldy.github.io/2016/06/11/2016-06-11-Force-directed/</url>
      <content type="html"><![CDATA[<p><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-6-11/65263838.jpg" alt=""><br></center><br><a id="more"></a></p>
<h2 id="任务介绍"><a href="#任务介绍" class="headerlink" title="任务介绍"></a>任务介绍</h2><p>图是表现社交网络、知识图谱等关系的主要形式，对图的节点进行布局是图可视化的重要内容。然而，现有方法大多在布局时没有考虑节点地理位置对布局的约束。比如在POI点评应用中，我们希望一个“餐厅”节点出现在它实际的地理位置上，或者在热点事件应用中，希望“北京”节点出现在“上海”节点的北方（上方）。在布局中加入地理位置约束，能够使图的可视化结果更好的与位置关联，包含地理信息相关隐喻，在增加其承载信息量的同时，更好的辅助地理空间数据的可视分析。<br>任务1：调研图可视化中节点布局相关方法，特别是力引导方法和二分图布局方法，形成小综述；<br>任务2：将二分图中一类节点加入绝对地理位置或彼此间相对位置不变作为约束条件，改进一种基于力引导布局的二分图可视化方法，给出模型、公式、算法流程描述；<br>任务3：基于给定数据集（两类节点，一类节点包含地理坐标），选择一种可视化工具（如VTK、D3等），对上述改进算法进行实现。</p>
<p>数据集形式如下所示：</p>
<p>文件PlaceTolation.txt内容如下，分别为地名和经纬度<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">地名	纬度,经度</span><br><span class="line">北京	39.90,116.40</span><br><span class="line">北京市	39.90,116.40</span><br><span class="line">北京站	39.90,116.40</span><br><span class="line">北京路	39.90,116.40</span><br><span class="line">天安门	39.90,116.38</span><br><span class="line">崇文	39.88,116.43</span><br><span class="line">崇文区	39.88,116.43</span><br><span class="line">......</span><br></pre></td></tr></table></figure></p>
<p>文件TitlePlace.txt内容如下,分别为序号,新闻标题和从该新闻中抽取出来的地名实体<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1	落马高官忏悔：从未感觉到还有党组织存在	中国</span><br><span class="line">2	佩帅：442阵型没问题对方进球很无解不怪门将	利物浦,切尔西</span><br><span class="line">3	今日数据趣谈：半场20+命中率8成5小加变大加	北京,德安,奎尔,孟菲斯</span><br><span class="line">4	工业领域控煤计划将出台：2020年力争节煤1.6亿吨	北京,河北,山西</span><br><span class="line">5	公交乘客与司机扭打发生车祸致1人重伤(图)	呼和浩特,呼和浩特市,青城,内蒙古,赛罕区,青洲</span><br><span class="line">6	深圳机场行人围观飞机起降被撞倒已致5死24伤	深圳</span><br><span class="line">7	云南临沧发生3.5级地震震源深度14千米	中国,云南省,临沧市,沧源佤族自治县,云南</span><br><span class="line">......</span><br></pre></td></tr></table></figure></p>
<p>需要构建的二分图中两类节点分别为新闻标题和地名，节点间的关系为标题和地名的映射关系（多对多的），其中地名节点具有经纬度属性。</p>
<h2 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h2><p>从数据中可以看出，有很多地名是重复的，比如北京其实和北京市是同一个意思，还有什么天安门，崇文区都是属于北京的，从经纬度上来看，应该把他们都归为一类，不然在地图上也不好显示，都是相聚很短的重合的点，基于以上考虑，我们可以根据经纬度把每个地点替换为其的所属的省或直辖市的名称。</p>
<p>要想判读每个地名所属的省市，那我们就需要每个省市的经纬度范围，在网上找到的中国地图的<a href="http://www.ourd3js.com/demo/rm/R-10.0/china.geojson" target="_blank" rel="external">JSON</a>文件,其中包含了每个省边界的经纬度值，为一系列的点，判断某个地点属于哪一个省实际上就是根据地点的经纬度判断这一点是否在某所有省边界点围成的多边形里，也就是一个<a href="https://en.wikipedia.org/wiki/Point_in_polygon" target="_blank" rel="external">Point in Polygon</a>问题。</p>
<p>Python matplotlib包中的Path提供了相应的函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.path <span class="keyword">as</span> mplPath</span><br><span class="line">bbPath = mplPath.Path(np.array([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>]]))</span><br><span class="line">bbPath.contains_point((<span class="number">0.5</span>, <span class="number">0.5</span>))</span><br></pre></td></tr></table></figure></p>
<h2 id="力导向图的制作"><a href="#力导向图的制作" class="headerlink" title="力导向图的制作"></a>力导向图的制作</h2><p>力导向图中每一个节点都受到力的作用而运动，这种是一种非常绚丽的图表。</p>
<p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-22/7813444.jpg" alt=""></p>
<p>力导向图（Force-Directed Graph），是绘图的一种算法。在二维或三维空间里配置节点，节点之间用线连接，称为连线。各连线的长度几乎相等，且尽可能不相交。节点和连线都被施加了力的作用，力是根据节点和连线的相对位置计算的。根据力的作用，来计算节点和连线的运动轨迹，并不断降低它们的能量，最终达到一种能量很低的安定状态。力导向图能表示节点之间的多对多的关系。</p>
<p>d3.layout.force()包含了力导向算法的实现，其主要参数为：</p>
<p>d3.layout.force - 使用物理模拟排放链接节点的位置。<br>force.alpha - 取得或者设置力布局的冷却参数。<br>force.chargeDistance - 取得或者设置最大电荷距离。<br>force.charge - 取得或者设置电荷强度。<br>force.drag - 给节点绑定拖动行为。<br>force.friction - 取得或者设置摩擦系数。<br>force.gravity - 取得或者设置重力强度。<br>force.linkDistance - 取得或者设置链接距离。<br>force.linkStrength - 取得或者设置链接强度。<br>force.links - 取得或者设置节点间的链接数组。<br>force.nodes - 取得或者设置布局的节点数组。<br>force.on - 监听在计算布局位置时的更新。<br>force.resume - 重新加热冷却参数，并重启模拟。<br>force.size - 取得或者设置布局大小。<br>force.start - 当节点变化时启动或者重启模拟。<br>force.stop - 立即停止模拟。<br>force.theta - 取得或者设置电荷作用的精度。<br>force.tick - 运行布局模拟的一步。</p>
<p>关于d3.layout.force()的使用可参考<a href="http://www.ourd3js.com/wordpress/?p=196" target="_blank" rel="external">力导向图的制作</a></p>
<h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><p>结合我们题目的实际要求，我们有两类节点：一类是地点节点，其位置要求固定；一类是新闻节点，其位置根据力导向算法计算得到，所以节点定义如下。<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> nodes = [</span><br><span class="line">              &#123;name:<span class="string">"青海"</span>,x:青海[<span class="number">0</span>],y:青海[<span class="number">1</span>],fixed:<span class="literal">true</span>,<span class="string">"group"</span>:<span class="number">1</span>&#125;,</span><br><span class="line">              &#123;name:<span class="string">"河南"</span>,x:河南[<span class="number">0</span>],y:河南[<span class="number">1</span>],fixed:<span class="literal">true</span>,<span class="string">"group"</span>:<span class="number">1</span>&#125;,</span><br><span class="line">              &#123;name:<span class="string">"山东"</span>,x:山东[<span class="number">0</span>],y:山东[<span class="number">1</span>],fixed:<span class="literal">true</span>,<span class="string">"group"</span>:<span class="number">1</span>&#125;,</span><br><span class="line">              .</span><br><span class="line">              .</span><br><span class="line">              .</span><br><span class="line"></span><br><span class="line">              &#123;name:<span class="string">"从WCBA争冠到无缘新赛季浙江女篮怎么了"</span>,fixed:<span class="literal">false</span>,<span class="string">"group"</span>:<span class="number">2</span>&#125;,</span><br><span class="line">              &#123;name:<span class="string">"成都的哥:专车司机玩着跑半个月超过我月收入"</span>,fixed:<span class="literal">false</span>,<span class="string">"group"</span>:<span class="number">2</span>&#125;,</span><br><span class="line">              &#123;name:<span class="string">"部分农村教师月薪不到2千暑假当小工补贴家用"</span>,fixed:<span class="literal">false</span>,<span class="string">"group"</span>:<span class="number">2</span>&#125;</span><br><span class="line">                ];</span><br></pre></td></tr></table></figure></p>
<p>其中第一类节点为固定地点节点，第二类节点为新闻节点，使用力导向算法计算节点的位置。所以我们需要提供地点节点的位置，在定义节点之前，加上地点经纬度：<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> 青海 =[<span class="number">96.5122866869</span>,<span class="number">35.12781926</span>];</span><br><span class="line"><span class="keyword">var</span> 河南 =[<span class="number">114.130772484</span>,<span class="number">34.00715756</span>];</span><br><span class="line"><span class="keyword">var</span> 山东 =[<span class="number">118.354817653</span>,<span class="number">36.2612648184</span>];</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br></pre></td></tr></table></figure></p>
<p>接下来是连线之间的定义，某一新闻里包含哪几个地点，则这几个地点就和这个新闻之间连一条线，其中0表示上面定义的第一个节点,185表示第186个节点。<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> edges = [</span><br><span class="line">                &#123;source:<span class="number">0</span>,target:<span class="number">185</span>&#125;,</span><br><span class="line">                &#123;source:<span class="number">0</span>,target:<span class="number">204</span>&#125;,</span><br><span class="line">                &#123;source:<span class="number">0</span>,target:<span class="number">389</span>&#125;,</span><br><span class="line">                &#123;source:<span class="number">0</span>,target:<span class="number">430</span>&#125;,</span><br><span class="line">                &#123;source:<span class="number">0</span>,target:<span class="number">494</span>&#125;,</span><br><span class="line">                &#123;source:<span class="number">1</span>,target:<span class="number">42</span>&#125;,</span><br><span class="line">                .</span><br><span class="line">                .</span><br><span class="line">                .</span><br><span class="line">              ]</span><br></pre></td></tr></table></figure></p>
<p>定义好数据之后，就可以开始布局了</p>
<p>定义一个力导向图的布局如下。</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> force = d3.layout.force()</span><br><span class="line">      .nodes(nodes) <span class="comment">//指定节点数组</span></span><br><span class="line">      .links(edges) <span class="comment">//指定连线数组</span></span><br><span class="line">      .size([width,height]) <span class="comment">//指定作用域范围</span></span><br><span class="line">      .linkDistance(<span class="number">150</span>) <span class="comment">//指定连线长度</span></span><br><span class="line">      .charge([<span class="number">-400</span>]); <span class="comment">//相互之间的作用力</span></span><br></pre></td></tr></table></figure>
<p>然后，使力学作用生效：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">force.start();    <span class="comment">//开始作用</span></span><br></pre></td></tr></table></figure>
<h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p>力学作业生效以后，新闻节点的坐标地址就会产生，根据产生的新闻坐标地址就可以绘制出整个可视化图。</p>
<p>分别绘制三种图形元素：</p>
<ul>
<li>line，线段，表示连线。</li>
<li>circle，圆，表示节点。</li>
<li>text，文字，描述节点。</li>
</ul>
<p>代码如下：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//添加连线</span></span><br><span class="line"> <span class="keyword">var</span> svg_edges = svg.selectAll(<span class="string">"line"</span>)</span><br><span class="line">     .data(edges)</span><br><span class="line">     .enter()</span><br><span class="line">     .append(<span class="string">"line"</span>)</span><br><span class="line">     .style(<span class="string">"stroke"</span>,<span class="string">"#ccc"</span>)</span><br><span class="line">     .style(<span class="string">"stroke-width"</span>,<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"> <span class="keyword">var</span> color = d3.scale.category20();</span><br><span class="line"></span><br><span class="line"> <span class="comment">//添加节点</span></span><br><span class="line"> <span class="keyword">var</span> svg_nodes = svg.selectAll(<span class="string">"circle"</span>)</span><br><span class="line">     .data(nodes)</span><br><span class="line">     .enter()</span><br><span class="line">     .append(<span class="string">"circle"</span>)</span><br><span class="line">     .attr(<span class="string">"r"</span>,<span class="number">20</span>)</span><br><span class="line">     .style(<span class="string">"fill"</span>,<span class="function"><span class="keyword">function</span>(<span class="params">d,i</span>)</span>&#123;</span><br><span class="line">         <span class="keyword">return</span> color(i);</span><br><span class="line">     &#125;)</span><br><span class="line">     .call(force.drag);  <span class="comment">//使得节点能够拖动</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">//添加描述节点的文字</span></span><br><span class="line"> <span class="keyword">var</span> svg_texts = svg.selectAll(<span class="string">"text"</span>)</span><br><span class="line">     .data(nodes)</span><br><span class="line">     .enter()</span><br><span class="line">     .append(<span class="string">"text"</span>)</span><br><span class="line">     .style(<span class="string">"fill"</span>, <span class="string">"black"</span>)</span><br><span class="line">     .attr(<span class="string">"dx"</span>, <span class="number">20</span>)</span><br><span class="line">     .attr(<span class="string">"dy"</span>, <span class="number">8</span>)</span><br><span class="line">     .text(<span class="function"><span class="keyword">function</span>(<span class="params">d</span>)</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> d.name;</span><br><span class="line">     &#125;);</span><br></pre></td></tr></table></figure>
<p>调用 call( force.drag ) 后节点可被拖动。force.drag() 是一个函数，将其作为 call() 的参数，相当于将当前选择的元素传到 force.drag() 函数中。</p>
<h2 id="结果展示"><a href="#结果展示" class="headerlink" title="结果展示"></a>结果展示</h2><p>可视化结果如下所示，在线演示地址:<a href="http://buptldy.github.io/DEMO/news_map.html">http://buptldy.github.io/DEMO/news_map.html</a></p>
<p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-21/3836519.jpg" alt=""></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Basic Sorting Algorithms Implemented In Python]]></title>
      <url>http://buptldy.github.io/2016/05/09/2016-05-09-Python%20sorting/</url>
      <content type="html"><![CDATA[<p><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-9/51291175.jpg" alt=""><br></center><br><a id="more"></a></p>
<h1 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h1><p>冒泡排序比较简单，主要过程如下：</p>
<ul>
<li>比较相邻的元素。如果第一个比第二个大，就交换他们两个。</li>
<li>对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。</li>
<li>针对所有的元素重复以上的步骤，除了最后一个。</li>
<li>持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">BubbleSort</span><span class="params">(array)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(len(array)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(len(array)<span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> array[j]&gt;array[j+<span class="number">1</span>]:</span><br><span class="line">                array[j],array[j+<span class="number">1</span>]=array[j+<span class="number">1</span>],array[j]</span><br><span class="line">    <span class="keyword">return</span> array</span><br></pre></td></tr></table></figure>
<h1 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h1><p>选择排序（Selection sort）是一种简单直观的排序算法。它的工作原理如下。首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SelectionSort</span><span class="params">(array)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(len(array)):</span><br><span class="line">        min_index=i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(i+<span class="number">1</span>,len(array)):</span><br><span class="line">            <span class="keyword">if</span> array[j]&lt;array[min_index]:</span><br><span class="line">                min_index=j</span><br><span class="line">        array[i],array[min_index]=array[min_index],array[i]</span><br><span class="line">    <span class="keyword">return</span> array</span><br></pre></td></tr></table></figure>
<h1 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h1><p>插入排序（英语：Insertion Sort）是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。插入排序在实现上，通常采用in-place排序（即只需用到O(1)的额外空间的排序），因而在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">InsertionSort</span><span class="params">(array)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1</span>,len(array)):</span><br><span class="line">        temp=array[i]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(i,<span class="number">-1</span>,<span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> temp&gt;array[j<span class="number">-1</span>]:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                array[j]=array[j<span class="number">-1</span>]</span><br><span class="line">        array[j]=temp</span><br><span class="line">    <span class="keyword">return</span> array</span><br></pre></td></tr></table></figure>
<h1 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h1><p>归并排序（英语：Merge sort，或mergesort），是创建在归并操作上的一种有效的排序算法，效率为O(n log n)。1945年由约翰·冯·诺伊曼首次提出。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用，且各层分治递归可以同时进行。</p>
<p>有关归并排序中的详细内容可以参考<a href="http://buptldy.github.io/2016/01/06/2016-01-06-%E5%88%86%E6%B2%BB%E7%AD%96%E7%95%A5[Divide%20and%20Conquer]/">分治策略中的归并排序</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MergeSort</span><span class="params">(array)</span>:</span></span><br><span class="line">    n=len(array)</span><br><span class="line">    <span class="keyword">if</span> n&lt;=<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> array</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        n=n/<span class="number">2</span></span><br><span class="line">        left=MergeSort(array[<span class="number">0</span>:n])</span><br><span class="line">        right=MergeSort(array[n:])</span><br><span class="line">        <span class="keyword">return</span> Merge(left,right)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Merge</span><span class="params">(left,right)</span>:</span></span><br><span class="line">    array=[]</span><br><span class="line">    <span class="keyword">while</span> len(left)&gt;<span class="number">0</span> <span class="keyword">and</span> len(right)&gt;<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">if</span> left[<span class="number">0</span>]&lt;right[<span class="number">0</span>]:</span><br><span class="line">            array.append(left[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">del</span> left[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            array.append(right[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">del</span> right[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> len(left)&gt;<span class="number">0</span>:</span><br><span class="line">        array.extend(left)</span><br><span class="line">    <span class="keyword">if</span> len(right)&gt;<span class="number">0</span>:</span><br><span class="line">        array.extend(right)</span><br><span class="line">    <span class="keyword">return</span> array</span><br></pre></td></tr></table></figure>
<h1 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h1><p>快速排序使用分治法（Divide and conquer）策略来把一个序列（list）分为两个子序列（sub-lists）。</p>
<p>步骤为：</p>
<ul>
<li>从数列中挑出一个元素，称为”基准”（pivot），重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作。</li>
<li>递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。</li>
<li>递归的最底部情形，是数列的大小是零或一，也就是永远都已经被排序好了。虽然一直递归下去，但是这个算法总会结束，因为在每次的迭代（iteration）中，它至少会把一个元素摆到它最后的位置去。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">QuickSort</span><span class="params">(array)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(array)&lt;=<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> array</span><br><span class="line">    pivot=array[<span class="number">0</span>]</span><br><span class="line">    left=[x <span class="keyword">for</span> x <span class="keyword">in</span> array[<span class="number">1</span>:]<span class="keyword">if</span> x&lt;pivot ]</span><br><span class="line">    right=[x <span class="keyword">for</span> x <span class="keyword">in</span> array[<span class="number">1</span>:] <span class="keyword">if</span> x&gt;=pivot]</span><br><span class="line">    <span class="keyword">return</span> QuickSort(left)+[pivot]+QuickSort(right)</span><br></pre></td></tr></table></figure>
<h1 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h1><p>在堆的数据结构中，堆中的最大值总是位于根节点。堆中定义以下几种操作：</p>
<ul>
<li>最大堆调整（Max_Heapify）：将堆的末端子节点作调整，使得子节点永远小于父节点</li>
<li>创建最大堆（Build_Max_Heap）：将堆所有数据重新排序</li>
<li>堆排序（HeapSort）：移除位在第一个数据的根节点，并做最大堆调整的递归运算</li>
</ul>
<p>堆排序可以参考这篇博文：[<a href="http://www.cnblogs.com/cj723/archive/2011/04/22/2024269.html]（http://www.cnblogs.com/cj723/archive/2011/04/22/2024269.html）" target="_blank" rel="external">http://www.cnblogs.com/cj723/archive/2011/04/22/2024269.html]（http://www.cnblogs.com/cj723/archive/2011/04/22/2024269.html）</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">heap_sort</span><span class="params">(array)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sift_down</span><span class="params">(start, end)</span>:</span></span><br><span class="line"><span class="string">"""最大堆调整"""</span></span><br><span class="line">root = start</span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    child = <span class="number">2</span> * root + <span class="number">1</span>    <span class="comment">#左子节点</span></span><br><span class="line">    <span class="keyword">if</span> child &gt; end:         <span class="comment">#如果没有子节点退出</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> child + <span class="number">1</span> &lt;= end <span class="keyword">and</span> array[child] &lt; array[child + <span class="number">1</span>]: <span class="comment">#如果左子节点值小于右子节点</span></span><br><span class="line">        child += <span class="number">1</span>                             <span class="comment">#下标由左子节点更换为右子节点</span></span><br><span class="line">    <span class="keyword">if</span> array[root] &lt; array[child]:             <span class="comment">#如果父节点小与子节点，则值相互交换</span></span><br><span class="line">        array[root], array[child] = array[child], array[root]</span><br><span class="line">        root = child                           <span class="comment">#对发生变化的子节点向下递归，重复上述过程</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建最大堆</span></span><br><span class="line"><span class="keyword">for</span> start <span class="keyword">in</span> xrange((len(array) - <span class="number">2</span>) // <span class="number">2</span>, <span class="number">-1</span>, <span class="number">-1</span>):<span class="comment">#从最后一个非叶子节点开始构造最大堆</span></span><br><span class="line">sift_down(start, len(array) - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 堆排序</span></span><br><span class="line"><span class="keyword">for</span> end <span class="keyword">in</span> xrange(len(array) - <span class="number">1</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">array[<span class="number">0</span>], array[end] = array[end], array[<span class="number">0</span>] <span class="comment">#把最大值放在最后</span></span><br><span class="line">sift_down(<span class="number">0</span>, end - <span class="number">1</span>)                      <span class="comment">#除最大值之外的继续构造最大堆</span></span><br><span class="line"><span class="keyword">return</span> array</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Implementing a Singly Linked List in Python]]></title>
      <url>http://buptldy.github.io/2016/05/09/2016-05-09-Python%20linked%20list/</url>
      <content type="html"><![CDATA[<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-9/62221771.jpg" alt=""><br></center><br><a id="more"></a><br><br><br><br>链表中最简单的一种是单向链表，它包含两个域，一个信息域和一个指针域。这个链接指向列表中的下一个节点，而最后一个节点则指向一个空值。一个单向链表的节点被分成两个部分。第一个部分保存或者显示关于节点的信息，第二个部分存储下一个节点的地址。单向链表只可向一个方向遍历。<br><br><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-9/37737204.jpg" alt=""><br></center>

<h1 id="链表节点类的实现"><a href="#链表节点类的实现" class="headerlink" title="链表节点类的实现"></a>链表节点类的实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,initdata)</span>:</span></span><br><span class="line">        self.data = initdata</span><br><span class="line">        self.next = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getData</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.data</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getNext</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.next</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">setData</span><span class="params">(self,newdata)</span>:</span></span><br><span class="line">        self.data = newdata</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">setNext</span><span class="params">(self,newnext)</span>:</span></span><br><span class="line">        self.next = newnext</span><br></pre></td></tr></table></figure>
<p>生成一个节点对象：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>temp = Node(<span class="number">93</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>temp.getData()</span><br><span class="line"><span class="number">93</span></span><br></pre></td></tr></table></figure>
<p>结构如下图所示：</p>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-9/39620482.jpg" alt=""><br></center>

<h1 id="链表类的实现"><a href="#链表类的实现" class="headerlink" title="链表类的实现"></a>链表类的实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UnorderedList</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.head = <span class="keyword">None</span></span><br></pre></td></tr></table></figure>
<p>新建一个链表对象：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>mylist = UnorderedList()</span><br></pre></td></tr></table></figure></p>
<h1 id="往链表前端中加入节点"><a href="#往链表前端中加入节点" class="headerlink" title="往链表前端中加入节点"></a>往链表前端中加入节点</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self,item)</span>:</span></span><br><span class="line">    temp = Node(item)</span><br><span class="line">    temp.setNext(self.head)</span><br><span class="line">    self.head = temp</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>mylist.add(<span class="number">31</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mylist.add(<span class="number">77</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mylist.add(<span class="number">17</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mylist.add(<span class="number">93</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mylist.add(<span class="number">26</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mylist.add(<span class="number">54</span>)</span><br></pre></td></tr></table></figure>
<p>现在链表结构如下图所示：</p>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-9/89034482.jpg" alt=""><br></center>

<h1 id="在链表尾端添加节点"><a href="#在链表尾端添加节点" class="headerlink" title="在链表尾端添加节点"></a>在链表尾端添加节点</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">append</span><span class="params">(self,item)</span>:</span></span><br><span class="line">    temp=Node(item)</span><br><span class="line">    <span class="keyword">if</span> self.head == <span class="keyword">None</span>:</span><br><span class="line">        self.head=item</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        current=self.head</span><br><span class="line">        <span class="keyword">while</span> current.getNext()!=<span class="keyword">None</span>:</span><br><span class="line">            current=current.getNext</span><br><span class="line">        current.setNext(temp)</span><br></pre></td></tr></table></figure>
<h1 id="链表的长度计算"><a href="#链表的长度计算" class="headerlink" title="链表的长度计算"></a>链表的长度计算</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">size</span><span class="params">(self)</span>:</span></span><br><span class="line">    count=<span class="number">0</span></span><br><span class="line">    current=self.head</span><br><span class="line">    <span class="keyword">while</span> current.getNext !=<span class="keyword">None</span>:</span><br><span class="line">        count=count+<span class="number">1</span></span><br><span class="line">        current=current.getNext</span><br></pre></td></tr></table></figure>
<p>计算过程如下图所示：</p>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-9/28270792.jpg" alt=""><br></center>

<h1 id="寻找是否存在某一节点"><a href="#寻找是否存在某一节点" class="headerlink" title="寻找是否存在某一节点"></a>寻找是否存在某一节点</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">serch</span><span class="params">(self,item)</span>:</span></span><br><span class="line">    current=self.head</span><br><span class="line">    <span class="keyword">while</span> current.getNext()!=<span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">if</span> current.getData==item:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            current=current.getNext()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">False</span></span><br></pre></td></tr></table></figure>
<h1 id="删除某一节点"><a href="#删除某一节点" class="headerlink" title="删除某一节点"></a>删除某一节点</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove</span><span class="params">(self,item)</span>:</span></span><br><span class="line">    current=self.head</span><br><span class="line">    pre=<span class="keyword">None</span></span><br><span class="line">    <span class="keyword">while</span> current!=<span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">if</span> current.getData()==item:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> pre:</span><br><span class="line">                self.head=current.getNext()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pre.setNext(current.getNext())</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pre=current</span><br><span class="line">            current=current.getNext()</span><br></pre></td></tr></table></figure>
<h1 id="链表反转"><a href="#链表反转" class="headerlink" title="链表反转"></a>链表反转</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rev</span><span class="params">(self)</span>:</span></span><br><span class="line">    pre=<span class="keyword">None</span></span><br><span class="line">    current=self.head</span><br><span class="line">    <span class="keyword">while</span> current!=<span class="keyword">None</span>:</span><br><span class="line">        next=current.getNext()</span><br><span class="line">        current.setNext=pre</span><br><span class="line">        pre=current</span><br><span class="line">        curren=next</span><br><span class="line">    <span class="keyword">return</span> pre</span><br></pre></td></tr></table></figure>
<h1 id="链表成对调换"><a href="#链表成对调换" class="headerlink" title="链表成对调换"></a>链表成对调换</h1><p>例如：<code>1-&gt;2-&gt;3-&gt;4</code>转换成<code>2-&gt;1-&gt;4-&gt;3</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pairswap</span><span class="params">(self)</span>:</span></span><br><span class="line">    curren=self.head</span><br><span class="line">    <span class="keyword">while</span> curren!=<span class="keyword">None</span> <span class="keyword">and</span> curren.getNext().getNext()!=<span class="keyword">None</span>:</span><br><span class="line">        temp=curren.getData()</span><br><span class="line">        curren.setData(curren.getNext().getData())</span><br><span class="line">        curren.getNext().setData(temp)</span><br><span class="line">        curren=curren.getNext().getNext()</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Python Binary Search Tree implementation]]></title>
      <url>http://buptldy.github.io/2016/05/09/2016-05-09-Python%20BST/</url>
      <content type="html"><![CDATA[<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-9/90589523.jpg" alt=""><br></center><br><a id="more"></a><br><br><br>二叉查找树（英语：Binary Search Tree），也称二叉搜索树、有序二叉树（英语：ordered binary tree），排序二叉树（英语：sorted binary tree），是指一棵空树或者具有下列性质的二叉树：<br><br>- 任意节点的左子树不空，则左子树上所有结点的值均小于它的根结点的值；<br>- 任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值；<br>- 任意节点的左、右子树也分别为二叉查找树；<br>- 没有键值相等的节点。<br>如下所示为一棵二叉查找树：<br><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-8/57493358.jpg" alt=""><br></center>

<h1 id="定义节点类"><a href="#定义节点类" class="headerlink" title="定义节点类"></a>定义节点类</h1><p>二叉树的每个节点有三个属性:</p>
<ul>
<li>左节点</li>
<li>右节点</li>
<li>节点值</li>
</ul>
<p>所以用Python定义一个节点类为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data,left=None,right=None)</span>:</span></span><br><span class="line">        self.left = left</span><br><span class="line">        self.right = right</span><br><span class="line">        self.data = data</span><br></pre></td></tr></table></figure></p>
<p>现在来创建一个根节点为8的树：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root=Node(<span class="number">8</span>)</span><br></pre></td></tr></table></figure></p>
<p>如下图所示：</p>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-8/11936419.jpg" alt=""><br></center>

<h1 id="插入节点"><a href="#插入节点" class="headerlink" title="插入节点"></a>插入节点</h1><p>比较要插入数据和根节点的大小，递归的调用插入方法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.data:<span class="comment">#如果存在根节点</span></span><br><span class="line">            <span class="keyword">if</span> data &lt; self.data:</span><br><span class="line">                <span class="keyword">if</span> self.left <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                    self.left = Node(data)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.left.insert(data)</span><br><span class="line">            <span class="keyword">elif</span> data &gt; self.data:</span><br><span class="line">                <span class="keyword">if</span> self.right <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                    self.right = Node(data)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.right.insert(data)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.data = data</span><br></pre></td></tr></table></figure></p>
<p>现在来插入三个节点：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root.insert(<span class="number">3</span>)</span><br><span class="line">root.insert(<span class="number">10</span>)</span><br><span class="line">root.insert(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>现在的二叉树如下所示：</p>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-8/48415300.jpg" alt=""><br></center>

<p>继续增加一些节点，让二叉树看起来更完整：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root.insert(<span class="number">6</span>)</span><br><span class="line">root.insert(<span class="number">4</span>)</span><br><span class="line">root.insert(<span class="number">7</span>)</span><br><span class="line">root.insert(<span class="number">14</span>)</span><br><span class="line">root.insert(<span class="number">13</span>)</span><br></pre></td></tr></table></figure></p>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-9/52855138.jpg" alt=""><br></center>

<h1 id="二叉查找树的查找"><a href="#二叉查找树的查找" class="headerlink" title="二叉查找树的查找"></a>二叉查找树的查找</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lookup</span><span class="params">(self, data, parent=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> data &lt; self.data:</span><br><span class="line">            <span class="keyword">if</span> self.left <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line">            <span class="keyword">return</span> self.left.lookup(data, self)</span><br><span class="line">        <span class="keyword">elif</span> data &gt; self.data:</span><br><span class="line">            <span class="keyword">if</span> self.right <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line">            <span class="keyword">return</span> self.right.lookup(data, self)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self, parent</span><br></pre></td></tr></table></figure>
<p>查找是否存在节点6，并返回这个节点和其父节点：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">node, parent = root.lookup(<span class="number">6</span>)</span><br></pre></td></tr></table></figure></p>
<p>其中查找的过程如下所示：</p>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-9/13534624.jpg" alt=""><br></center>

<h1 id="删除节点"><a href="#删除节点" class="headerlink" title="删除节点"></a>删除节点</h1><p>在删除节点时，首先得统计节点孩子的个数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">children_count</span><span class="params">(self)</span>:</span></span><br><span class="line">        cnt = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> self.left:</span><br><span class="line">            cnt += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> self.right:</span><br><span class="line">            cnt += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> cnt</span><br></pre></td></tr></table></figure>
<p>删除节点，分三种情况：</p>
<ul>
<li>要删除的节点没有孩子节点</li>
<li>要删除的节点有一个孩子节点</li>
<li>要删除的节点有两个孩子节点</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">delete</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        node, parent = self.lookup(data)</span><br><span class="line">        <span class="keyword">if</span> node <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            children_count = node.children_count()</span><br><span class="line">                <span class="keyword">if</span> children_count == <span class="number">0</span>:</span><br><span class="line">                    <span class="comment"># if node has no children, just remove it</span></span><br><span class="line">                    <span class="keyword">if</span> parent:</span><br><span class="line">                        <span class="keyword">if</span> parent.left <span class="keyword">is</span> node:</span><br><span class="line">                            parent.left = <span class="keyword">None</span></span><br><span class="line">                        <span class="keyword">else</span>:</span><br><span class="line">                            parent.right = <span class="keyword">None</span></span><br><span class="line">                        <span class="keyword">del</span> node</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        self.data = <span class="keyword">None</span></span><br><span class="line">                <span class="keyword">elif</span> children_count == <span class="number">1</span>:</span><br><span class="line">                      <span class="comment"># if node has 1 child</span></span><br><span class="line">                      <span class="comment"># replace node with its child</span></span><br><span class="line">                    <span class="keyword">if</span> node.left:</span><br><span class="line">                        n = node.left</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        n = node.right</span><br><span class="line">                    <span class="keyword">if</span> parent:</span><br><span class="line">                        <span class="keyword">if</span> parent.left <span class="keyword">is</span> node:</span><br><span class="line">                            parent.left = n</span><br><span class="line">                        <span class="keyword">else</span>:</span><br><span class="line">                            parent.right = n</span><br><span class="line">                        <span class="keyword">del</span> node</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        self.left = n.left</span><br><span class="line">                        self.right = n.right</span><br><span class="line">                        self.data = n.data</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># if node has 2 children</span></span><br><span class="line">                    <span class="comment"># find its successor</span></span><br><span class="line">                    parent = node</span><br><span class="line">                    successor = node.right</span><br><span class="line">                    <span class="keyword">while</span> successor.left:</span><br><span class="line">                        parent = successor</span><br><span class="line">                        successor = successor.left</span><br><span class="line">                    <span class="comment"># replace node data by its successor data</span></span><br><span class="line">                    node.data = successor.data</span><br><span class="line">                    <span class="comment"># fix successor's parent's child</span></span><br><span class="line">                    <span class="keyword">if</span> parent.left == successor:</span><br><span class="line">                        parent.left = successor.right</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        parent.right = successor.right</span><br></pre></td></tr></table></figure>
<h1 id="打印二叉树"><a href="#打印二叉树" class="headerlink" title="打印二叉树"></a>打印二叉树</h1><p>按照中序打印二叉树，前序和后序只需要修改打印的顺序就行。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">print_tree</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span><br><span class="line">        Print tree content inorder</span><br><span class="line">        """</span></span><br><span class="line">        <span class="keyword">if</span> self.left:</span><br><span class="line">            self.left.print_tree()</span><br><span class="line">        <span class="keyword">print</span> self.data,</span><br><span class="line">        <span class="keyword">if</span> self.right:</span><br><span class="line">            self.right.print_tree()</span><br></pre></td></tr></table></figure></p>
<p>按层次打印一个树：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">print_each_level</span><span class="params">(self)</span>:</span></span><br><span class="line">      <span class="comment"># Start off with root node</span></span><br><span class="line">      thislevel = [self]</span><br><span class="line"></span><br><span class="line">      <span class="comment"># While there is another level</span></span><br><span class="line">      <span class="keyword">while</span> thislevel:</span><br><span class="line">        nextlevel = list()</span><br><span class="line">        <span class="comment">#Print all the nodes in the current level, and   store the next level in a list</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> thislevel:</span><br><span class="line">          <span class="keyword">print</span> node.data</span><br><span class="line">          <span class="keyword">if</span> node.left: nextlevel.append(node.left)</span><br><span class="line">          <span class="keyword">if</span> node.right: nextlevel.append(node.right)</span><br><span class="line">          <span class="keyword">print</span></span><br><span class="line">          thislevel = nextlevel</span><br></pre></td></tr></table></figure></p>
<h1 id="比较两棵树"><a href="#比较两棵树" class="headerlink" title="比较两棵树"></a>比较两棵树</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compare_trees</span><span class="params">(self, node)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> node <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">if</span> self.data != node.data:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        res = <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">if</span> self.left <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">if</span> node.left:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            res = self.left.compare_trees(node.left)</span><br><span class="line">        <span class="keyword">if</span> res <span class="keyword">is</span> <span class="keyword">False</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">if</span> self.right <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">if</span> node.right:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            res = self.right.compare_trees(node.right)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h1 id="二叉树的重建"><a href="#二叉树的重建" class="headerlink" title="二叉树的重建"></a>二叉树的重建</h1><p>根据前序遍历和中序遍历来重建树，重建的原理可以参看这篇博文<a href="http://blog.csdn.net/hinyunsin/article/details/6315502" target="_blank" rel="external">根据二叉树的前序和中序求后序</a>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rebuilt</span><span class="params">(preorder,inorder)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> preorder==<span class="string">''</span> <span class="keyword">or</span> inorder==<span class="string">''</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    root=preorder[<span class="number">0</span>]</span><br><span class="line">    index=inorder.index(root)</span><br><span class="line">    <span class="keyword">return</span> Node(root,</span><br><span class="line">                rebuilt(preorder[<span class="number">1</span>:<span class="number">1</span>+index],inorder[<span class="number">0</span>:index]),</span><br><span class="line">                rebuilt(preorder[index+<span class="number">1</span>:],inorder[index+<span class="number">1</span>:]))</span><br></pre></td></tr></table></figure></p>
<p>根据中序和后序来重建树：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rebuilt1</span><span class="params">(inorder,postorder)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> postorder==<span class="string">''</span> <span class="keyword">or</span> inorder==<span class="string">''</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    root=postorder[<span class="number">-1</span>]</span><br><span class="line">    index=inorder.index(root)</span><br><span class="line">    <span class="keyword">return</span> Node(root,</span><br><span class="line">                rebuilt1(inorder[<span class="number">0</span>:index],postorder[<span class="number">0</span>:index]),</span><br><span class="line">                rebuilt1(inorder[index+<span class="number">1</span>:],postorder[index:<span class="number">-1</span>]))</span><br></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://zh.wikipedia.org/wiki/%E4%BA%8C%E5%85%83%E6%90%9C%E5%B0%8B%E6%A8%B9" target="_blank" rel="external">二叉搜索树</a><br><a href="http://www.laurentluce.com/posts/binary-search-tree-library-in-python/" target="_blank" rel="external">Binary Search Tree library in Python</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Learning with Caffe in Python]]></title>
      <url>http://buptldy.github.io/2016/05/05/2016-05-05-Caffe%20Python/</url>
      <content type="html"><![CDATA[<p><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-5/41421657.jpg" alt=""><br></center><br><a id="more"></a><br>在这个例子中，我们开始尝试通过Python调用<code>Solver</code>接口来训练一个网络。</p>
<h3 id="环境设置"><a href="#环境设置" class="headerlink" title="环境设置"></a>环境设置</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> *</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">caffe_root = <span class="string">'/home/ldy/workspace/caffe/'</span>  <span class="comment"># this file should be run from &#123;caffe_root&#125;/examples (otherwise change this line)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.insert(<span class="number">0</span>, caffe_root + <span class="string">'python'</span>)</span><br><span class="line"><span class="keyword">import</span> caffe</span><br></pre></td></tr></table></figure>
<ul>
<li>下载训练用的数据，并导入lmdb</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># run scripts from caffe root</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.chdir(caffe_root)</span><br><span class="line"><span class="comment"># Download data</span></span><br><span class="line">!data/mnist/get_mnist.sh</span><br><span class="line"><span class="comment"># Prepare data</span></span><br><span class="line">!examples/mnist/create_mnist.sh</span><br><span class="line"><span class="comment"># back to examples</span></span><br><span class="line">os.chdir(<span class="string">'examples'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Downloading...
Creating lmdb...
I0505 20:49:32.535013 18388 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0505 20:49:32.535306 18388 convert_mnist_data.cpp:88] A total of 60000 items.
I0505 20:49:32.535323 18388 convert_mnist_data.cpp:89] Rows: 28 Cols: 28
I0505 20:49:32.547651 18388 db_lmdb.cpp:101] Doubling LMDB map size to 2MB ...
I0505 20:49:32.556696 18388 db_lmdb.cpp:101] Doubling LMDB map size to 4MB ...
I0505 20:49:32.578054 18388 db_lmdb.cpp:101] Doubling LMDB map size to 8MB ...
I0505 20:49:32.627709 18388 db_lmdb.cpp:101] Doubling LMDB map size to 16MB ...
I0505 20:49:32.718138 18388 db_lmdb.cpp:101] Doubling LMDB map size to 32MB ...
I0505 20:49:32.960189 18388 db_lmdb.cpp:101] Doubling LMDB map size to 64MB ...
I0505 20:49:33.271764 18388 convert_mnist_data.cpp:108] Processed 60000 files.
I0505 20:49:33.403015 18390 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0505 20:49:33.403692 18390 convert_mnist_data.cpp:88] A total of 10000 items.
I0505 20:49:33.403733 18390 convert_mnist_data.cpp:89] Rows: 28 Cols: 28
I0505 20:49:33.423638 18390 db_lmdb.cpp:101] Doubling LMDB map size to 2MB ...
I0505 20:49:33.439213 18390 db_lmdb.cpp:101] Doubling LMDB map size to 4MB ...
I0505 20:49:33.470553 18390 db_lmdb.cpp:101] Doubling LMDB map size to 8MB ...
I0505 20:49:33.525192 18390 db_lmdb.cpp:101] Doubling LMDB map size to 16MB ...
I0505 20:49:33.546480 18390 convert_mnist_data.cpp:108] Processed 10000 files.
Done.
</code></pre><h3 id="搭建网络"><a href="#搭建网络" class="headerlink" title="搭建网络"></a>搭建网络</h3><p>搭建网络结构，并保存为lenet_auto_train.prototxt（训练网络），lenet_auto_test.prototxt（测试网络）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> caffe <span class="keyword">import</span> layers <span class="keyword">as</span> L, params <span class="keyword">as</span> P</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lenet</span><span class="params">(lmdb, batch_size)</span>:</span></span><br><span class="line">    <span class="comment"># our version of LeNet: a series of linear and simple nonlinear transformations</span></span><br><span class="line">    n = caffe.NetSpec()</span><br><span class="line"></span><br><span class="line">    n.data, n.label = L.Data(batch_size=batch_size, backend=P.Data.LMDB, source=lmdb,</span><br><span class="line">                             transform_param=dict(scale=<span class="number">1.</span>/<span class="number">255</span>), ntop=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    n.conv1 = L.Convolution(n.data, kernel_size=<span class="number">5</span>, num_output=<span class="number">20</span>, weight_filler=dict(type=<span class="string">'xavier'</span>))</span><br><span class="line">    n.pool1 = L.Pooling(n.conv1, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, pool=P.Pooling.MAX)</span><br><span class="line">    n.conv2 = L.Convolution(n.pool1, kernel_size=<span class="number">5</span>, num_output=<span class="number">50</span>, weight_filler=dict(type=<span class="string">'xavier'</span>))</span><br><span class="line">    n.pool2 = L.Pooling(n.conv2, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, pool=P.Pooling.MAX)</span><br><span class="line">    n.fc1 =   L.InnerProduct(n.pool2, num_output=<span class="number">500</span>, weight_filler=dict(type=<span class="string">'xavier'</span>))</span><br><span class="line">    n.relu1 = L.ReLU(n.fc1, in_place=<span class="keyword">True</span>)</span><br><span class="line">    n.score = L.InnerProduct(n.relu1, num_output=<span class="number">10</span>, weight_filler=dict(type=<span class="string">'xavier'</span>))</span><br><span class="line">    n.loss =  L.SoftmaxWithLoss(n.score, n.label)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> n.to_proto()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'mnist/lenet_auto_train.prototxt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(str(lenet(<span class="string">'mnist/mnist_train_lmdb'</span>, <span class="number">64</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'mnist/lenet_auto_test.prototxt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(str(lenet(<span class="string">'mnist/mnist_test_lmdb'</span>, <span class="number">100</span>)))</span><br></pre></td></tr></table></figure>
<p>查看训练网络结构：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!cat mnist/lenet_auto_train.prototxt</span><br></pre></td></tr></table></figure>
<pre><code>layer {
  name: &quot;data&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  transform_param {
    scale: 0.00392156862745
  }
  data_param {
    source: &quot;mnist/mnist_train_lmdb&quot;
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: &quot;conv1&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  convolution_param {
    num_output: 20
    kernel_size: 5
    weight_filler {
      type: &quot;xavier&quot;
    }
  }
}
layer {
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  convolution_param {
    num_output: 50
    kernel_size: 5
    weight_filler {
      type: &quot;xavier&quot;
    }
  }
}
layer {
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv2&quot;
  top: &quot;pool2&quot;
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: &quot;fc1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool2&quot;
  top: &quot;fc1&quot;
  inner_product_param {
    num_output: 500
    weight_filler {
      type: &quot;xavier&quot;
    }
  }
}
layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;fc1&quot;
  top: &quot;fc1&quot;
}
layer {
  name: &quot;score&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;fc1&quot;
  top: &quot;score&quot;
  inner_product_param {
    num_output: 10
    weight_filler {
      type: &quot;xavier&quot;
    }
  }
}
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;score&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
</code></pre><p>查看学习参数，参数文件已经保存在本地磁盘：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!cat mnist/lenet_auto_solver.prototxt</span><br></pre></td></tr></table></figure>
<pre><code># The train/test net protocol buffer definition
train_net: &quot;mnist/lenet_auto_train.prototxt&quot;
test_net: &quot;mnist/lenet_auto_test.prototxt&quot;
# test_iter specifies how many forward passes the test should carry out.
# In the case of MNIST, we have test batch size 100 and 100 test iterations,
# covering the full 10,000 testing images.
test_iter: 100
# Carry out testing every 500 training iterations.
test_interval: 500
# The base learning rate, momentum and the weight decay of the network.
base_lr: 0.01
momentum: 0.9
weight_decay: 0.0005
# The learning rate policy
lr_policy: &quot;inv&quot;
gamma: 0.0001
power: 0.75
# Display every 100 iterations
display: 100
# The maximum number of iterations
max_iter: 10000
# snapshot intermediate results
snapshot: 5000
snapshot_prefix: &quot;mnist/lenet&quot;
</code></pre><h3 id="加载并检查solver"><a href="#加载并检查solver" class="headerlink" title="加载并检查solver"></a>加载并检查solver</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">caffe.set_device(<span class="number">0</span>)</span><br><span class="line">caffe.set_mode_gpu()</span><br><span class="line"></span><br><span class="line"><span class="comment">### load the solver and create train and test nets</span></span><br><span class="line">solver = <span class="keyword">None</span>  <span class="comment"># ignore this workaround for lmdb data (can't instantiate two solvers on the same data)</span></span><br><span class="line">solver = caffe.SGDSolver(<span class="string">'mnist/lenet_auto_solver.prototxt'</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>检查网络参数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># each output is (batch size, feature dim, spatial dim)</span></span><br><span class="line">[(k, v.data.shape) <span class="keyword">for</span> k, v <span class="keyword">in</span> solver.net.blobs.items()]</span><br></pre></td></tr></table></figure>
<pre><code>[(&apos;data&apos;, (64, 1, 28, 28)),
 (&apos;label&apos;, (64,)),
 (&apos;conv1&apos;, (64, 20, 24, 24)),
 (&apos;pool1&apos;, (64, 20, 12, 12)),
 (&apos;conv2&apos;, (64, 50, 8, 8)),
 (&apos;pool2&apos;, (64, 50, 4, 4)),
 (&apos;fc1&apos;, (64, 500)),
 (&apos;score&apos;, (64, 10)),
 (&apos;loss&apos;, ())]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># just print the weight sizes (we'll omit the biases)</span></span><br><span class="line">[(k, v[<span class="number">0</span>].data.shape) <span class="keyword">for</span> k, v <span class="keyword">in</span> solver.net.params.items()]</span><br></pre></td></tr></table></figure>
<pre><code>[(&apos;conv1&apos;, (20, 1, 5, 5)),
 (&apos;conv2&apos;, (50, 20, 5, 5)),
 (&apos;fc1&apos;, (500, 800)),
 (&apos;score&apos;, (10, 500))]
</code></pre><ul>
<li>在开始前，我们先检查下训练网络和测试网络是否包含我们的数据</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">solver.net.forward()  <span class="comment"># train net</span></span><br><span class="line">solver.test_nets[<span class="number">0</span>].forward()  <span class="comment"># test net (there can be more than one)</span></span><br></pre></td></tr></table></figure>
<pre><code>{&apos;loss&apos;: array(2.3089799880981445, dtype=float32)}
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># we use a little trick to tile the first eight images</span></span><br><span class="line">imshow(solver.net.blobs[<span class="string">'data'</span>].data[:<span class="number">8</span>, <span class="number">0</span>].transpose(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>).reshape(<span class="number">28</span>, <span class="number">8</span>*<span class="number">28</span>), cmap=<span class="string">'gray'</span>); axis(<span class="string">'off'</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'train labels:'</span>, solver.net.blobs[<span class="string">'label'</span>].data[:<span class="number">8</span>]</span><br></pre></td></tr></table></figure>
<pre><code>train labels: [ 5.  0.  4.  1.  9.  2.  1.  3.]
</code></pre><p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-5/69386136.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">imshow(solver.test_nets[<span class="number">0</span>].blobs[<span class="string">'data'</span>].data[:<span class="number">8</span>, <span class="number">0</span>].transpose(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>).reshape(<span class="number">28</span>, <span class="number">8</span>*<span class="number">28</span>), cmap=<span class="string">'gray'</span>); axis(<span class="string">'off'</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'test labels:'</span>, solver.test_nets[<span class="number">0</span>].blobs[<span class="string">'label'</span>].data[:<span class="number">8</span>]</span><br></pre></td></tr></table></figure>
<pre><code>test labels: [ 7.  2.  1.  0.  4.  1.  4.  9.]
</code></pre><p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-5/12928484.jpg" alt=""></p>
<h3 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h3><ul>
<li>先训练一个batch看会有什么结果</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">solver.step(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>运行一次之后，看看我们的第一层卷积层的滤波器是否有变化，20个滤波器如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">imshow(solver.net.params[<span class="string">'conv1'</span>][<span class="number">0</span>].diff[:, <span class="number">0</span>].reshape(<span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">       .transpose(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).reshape(<span class="number">4</span>*<span class="number">5</span>, <span class="number">5</span>*<span class="number">5</span>), cmap=<span class="string">'gray'</span>); axis(<span class="string">'off'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(-0.5, 24.5, 19.5, -0.5)
</code></pre><p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-5/2284082.jpg" alt=""></p>
<p>上面说明权重已经更新，我们可以在迭代训练的时候，记录一些参数，决定什么时候停止迭代</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">niter = <span class="number">200</span></span><br><span class="line">test_interval = <span class="number">25</span></span><br><span class="line"><span class="comment"># losses will also be stored in the log</span></span><br><span class="line">train_loss = zeros(niter)</span><br><span class="line">test_acc = zeros(int(np.ceil(niter / test_interval)))</span><br><span class="line">output = zeros((niter, <span class="number">8</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># the main solver loop</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(niter):</span><br><span class="line">    solver.step(<span class="number">1</span>)  <span class="comment"># SGD by Caffe</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># store the train loss</span></span><br><span class="line">    train_loss[it] = solver.net.blobs[<span class="string">'loss'</span>].data</span><br><span class="line"></span><br><span class="line">    <span class="comment"># store the output on the first test batch</span></span><br><span class="line">    <span class="comment"># (start the forward pass at conv1 to avoid loading new data)</span></span><br><span class="line">    solver.test_nets[<span class="number">0</span>].forward(start=<span class="string">'conv1'</span>)</span><br><span class="line">    output[it] = solver.test_nets[<span class="number">0</span>].blobs[<span class="string">'score'</span>].data[:<span class="number">8</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># run a full test every so often</span></span><br><span class="line">    <span class="comment"># (Caffe can also do this for us and write to a log, but we show here</span></span><br><span class="line">    <span class="comment">#  how to do it directly in Python, where more complicated things are easier.)</span></span><br><span class="line">    <span class="keyword">if</span> it % test_interval == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'Iteration'</span>, it, <span class="string">'testing...'</span></span><br><span class="line">        correct = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> test_it <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">            solver.test_nets[<span class="number">0</span>].forward()</span><br><span class="line">            correct += sum(solver.test_nets[<span class="number">0</span>].blobs[<span class="string">'score'</span>].data.argmax(<span class="number">1</span>)</span><br><span class="line">                           == solver.test_nets[<span class="number">0</span>].blobs[<span class="string">'label'</span>].data)</span><br><span class="line">        test_acc[it // test_interval] = correct / <span class="number">1e4</span></span><br></pre></td></tr></table></figure>
<pre><code>Iteration 0 testing...
Iteration 25 testing...
Iteration 50 testing...
Iteration 75 testing...
Iteration 100 testing...
Iteration 125 testing...
Iteration 150 testing...
Iteration 175 testing...
CPU times: user 1min 15s, sys: 15.3 s, total: 1min 31s
Wall time: 1min 18s
</code></pre><ul>
<li>画出train loss和test accuracy</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">_, ax1 = subplots()</span><br><span class="line">ax2 = ax1.twinx()</span><br><span class="line">ax1.plot(arange(niter), train_loss)</span><br><span class="line">ax2.plot(test_interval * arange(len(test_acc)), test_acc, <span class="string">'r'</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">'iteration'</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">'train loss'</span>)</span><br><span class="line">ax2.set_ylabel(<span class="string">'test accuracy'</span>)</span><br><span class="line">ax2.set_title(<span class="string">'Test Accuracy: &#123;:.2f&#125;'</span>.format(test_acc[<span class="number">-1</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.text.Text at 0x7feabeae91d0&gt;
</code></pre><p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-5/18901685.jpg" alt=""></p>
<ul>
<li>因为我们保存第一次测试batch的结果，所以可以看看每次迭代结果的变化，下面画出每个图像随迭代次数每个标签的可能性。(只显示了一个数字，其他的数字类似)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">8</span>):</span><br><span class="line">    figure(figsize=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    imshow(solver.test_nets[<span class="number">0</span>].blobs[<span class="string">'data'</span>].data[i, <span class="number">0</span>], cmap=<span class="string">'gray'</span>)</span><br><span class="line">    figure(figsize=(<span class="number">10</span>, <span class="number">2</span>))</span><br><span class="line">    imshow(output[:<span class="number">50</span>, i].T, interpolation=<span class="string">'nearest'</span>, cmap=<span class="string">'gray'</span>)</span><br><span class="line">    xlabel(<span class="string">'iteration'</span>)</span><br><span class="line">    ylabel(<span class="string">'label'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-5/47308921.jpg" alt=""></p>
<p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-5/21846569.jpg" alt=""></p>
<h3 id="尝试改变网络结构和优化函数"><a href="#尝试改变网络结构和优化函数" class="headerlink" title="尝试改变网络结构和优化函数"></a>尝试改变网络结构和优化函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line">train_net_path = <span class="string">'mnist/custom_auto_train.prototxt'</span></span><br><span class="line">test_net_path = <span class="string">'mnist/custom_auto_test.prototxt'</span></span><br><span class="line">solver_config_path = <span class="string">'mnist/custom_auto_solver.prototxt'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### define net</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">custom_net</span><span class="params">(lmdb, batch_size)</span>:</span></span><br><span class="line">    <span class="comment"># define your own net!</span></span><br><span class="line">    n = caffe.NetSpec()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># keep this data layer for all networks</span></span><br><span class="line">    n.data, n.label = L.Data(batch_size=batch_size, backend=P.Data.LMDB, source=lmdb,</span><br><span class="line">                             transform_param=dict(scale=<span class="number">1.</span>/<span class="number">255</span>), ntop=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># EDIT HERE to try different networks</span></span><br><span class="line">    <span class="comment"># this single layer defines a simple linear classifier</span></span><br><span class="line">    <span class="comment"># (in particular this defines a multiway logistic regression)</span></span><br><span class="line">    n.score =   L.InnerProduct(n.data, num_output=<span class="number">10</span>, weight_filler=dict(type=<span class="string">'xavier'</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># EDIT HERE this is the LeNet variant we have already tried</span></span><br><span class="line">    <span class="comment"># n.conv1 = L.Convolution(n.data, kernel_size=5, num_output=20, weight_filler=dict(type='xavier'))</span></span><br><span class="line">    <span class="comment"># n.pool1 = L.Pooling(n.conv1, kernel_size=2, stride=2, pool=P.Pooling.MAX)</span></span><br><span class="line">    <span class="comment"># n.conv2 = L.Convolution(n.pool1, kernel_size=5, num_output=50, weight_filler=dict(type='xavier'))</span></span><br><span class="line">    <span class="comment"># n.pool2 = L.Pooling(n.conv2, kernel_size=2, stride=2, pool=P.Pooling.MAX)</span></span><br><span class="line">    <span class="comment"># n.fc1 =   L.InnerProduct(n.pool2, num_output=500, weight_filler=dict(type='xavier'))</span></span><br><span class="line">    <span class="comment"># EDIT HERE consider L.ELU or L.Sigmoid for the nonlinearity</span></span><br><span class="line">    <span class="comment"># n.relu1 = L.ReLU(n.fc1, in_place=True)</span></span><br><span class="line">    <span class="comment"># n.score =   L.InnerProduct(n.fc1, num_output=10, weight_filler=dict(type='xavier'))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># keep this loss layer for all networks</span></span><br><span class="line">    n.loss =  L.SoftmaxWithLoss(n.score, n.label)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> n.to_proto()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(train_net_path, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(str(custom_net(<span class="string">'mnist/mnist_train_lmdb'</span>, <span class="number">64</span>)))    </span><br><span class="line"><span class="keyword">with</span> open(test_net_path, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(str(custom_net(<span class="string">'mnist/mnist_test_lmdb'</span>, <span class="number">100</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">### define solver</span></span><br><span class="line"><span class="keyword">from</span> caffe.proto <span class="keyword">import</span> caffe_pb2</span><br><span class="line">s = caffe_pb2.SolverParameter()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set a seed for reproducible experiments:</span></span><br><span class="line"><span class="comment"># this controls for randomization in training.</span></span><br><span class="line">s.random_seed = <span class="number">0xCAFFE</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Specify locations of the train and (maybe) test networks.</span></span><br><span class="line">s.train_net = train_net_path</span><br><span class="line">s.test_net.append(test_net_path)</span><br><span class="line">s.test_interval = <span class="number">500</span>  <span class="comment"># Test after every 500 training iterations.</span></span><br><span class="line">s.test_iter.append(<span class="number">100</span>) <span class="comment"># Test on 100 batches each time we test.</span></span><br><span class="line"></span><br><span class="line">s.max_iter = <span class="number">10000</span>     <span class="comment"># no. of times to update the net (training iterations)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># EDIT HERE to try different solvers</span></span><br><span class="line"><span class="comment"># solver types include "SGD", "Adam", and "Nesterov" among others.</span></span><br><span class="line">s.type = <span class="string">"SGD"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the initial learning rate for SGD.</span></span><br><span class="line">s.base_lr = <span class="number">0.01</span>  <span class="comment"># EDIT HERE to try different learning rates</span></span><br><span class="line"><span class="comment"># Set momentum to accelerate learning by</span></span><br><span class="line"><span class="comment"># taking weighted average of current and previous updates.</span></span><br><span class="line">s.momentum = <span class="number">0.9</span></span><br><span class="line"><span class="comment"># Set weight decay to regularize and prevent overfitting</span></span><br><span class="line">s.weight_decay = <span class="number">5e-4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set `lr_policy` to define how the learning rate changes during training.</span></span><br><span class="line"><span class="comment"># This is the same policy as our default LeNet.</span></span><br><span class="line">s.lr_policy = <span class="string">'inv'</span></span><br><span class="line">s.gamma = <span class="number">0.0001</span></span><br><span class="line">s.power = <span class="number">0.75</span></span><br><span class="line"><span class="comment"># EDIT HERE to try the fixed rate (and compare with adaptive solvers)</span></span><br><span class="line"><span class="comment"># `fixed` is the simplest policy that keeps the learning rate constant.</span></span><br><span class="line"><span class="comment"># s.lr_policy = 'fixed'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Display the current training loss and accuracy every 1000 iterations.</span></span><br><span class="line">s.display = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Snapshots are files used to store networks we've trained.</span></span><br><span class="line"><span class="comment"># We'll snapshot every 5K iterations -- twice during training.</span></span><br><span class="line">s.snapshot = <span class="number">5000</span></span><br><span class="line">s.snapshot_prefix = <span class="string">'mnist/custom_net'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train on the GPU</span></span><br><span class="line">s.solver_mode = caffe_pb2.SolverParameter.GPU</span><br><span class="line"></span><br><span class="line"><span class="comment"># Write the solver to a temporary file and return its filename.</span></span><br><span class="line"><span class="keyword">with</span> open(solver_config_path, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(str(s))</span><br><span class="line"></span><br><span class="line"><span class="comment">### load the solver and create train and test nets</span></span><br><span class="line">solver = <span class="keyword">None</span>  <span class="comment"># ignore this workaround for lmdb data (can't instantiate two solvers on the same data)</span></span><br><span class="line">solver = caffe.get_solver(solver_config_path)</span><br><span class="line"></span><br><span class="line"><span class="comment">### solve</span></span><br><span class="line">niter = <span class="number">250</span>  <span class="comment"># EDIT HERE increase to train for longer</span></span><br><span class="line">test_interval = niter / <span class="number">10</span></span><br><span class="line"><span class="comment"># losses will also be stored in the log</span></span><br><span class="line">train_loss = zeros(niter)</span><br><span class="line">test_acc = zeros(int(np.ceil(niter / test_interval)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># the main solver loop</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(niter):</span><br><span class="line">    solver.step(<span class="number">1</span>)  <span class="comment"># SGD by Caffe</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># store the train loss</span></span><br><span class="line">    train_loss[it] = solver.net.blobs[<span class="string">'loss'</span>].data</span><br><span class="line"></span><br><span class="line">    <span class="comment"># run a full test every so often</span></span><br><span class="line">    <span class="comment"># (Caffe can also do this for us and write to a log, but we show here</span></span><br><span class="line">    <span class="comment">#  how to do it directly in Python, where more complicated things are easier.)</span></span><br><span class="line">    <span class="keyword">if</span> it % test_interval == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'Iteration'</span>, it, <span class="string">'testing...'</span></span><br><span class="line">        correct = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> test_it <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">            solver.test_nets[<span class="number">0</span>].forward()</span><br><span class="line">            correct += sum(solver.test_nets[<span class="number">0</span>].blobs[<span class="string">'score'</span>].data.argmax(<span class="number">1</span>)</span><br><span class="line">                           == solver.test_nets[<span class="number">0</span>].blobs[<span class="string">'label'</span>].data)</span><br><span class="line">        test_acc[it // test_interval] = correct / <span class="number">1e4</span></span><br><span class="line"></span><br><span class="line">_, ax1 = subplots()</span><br><span class="line">ax2 = ax1.twinx()</span><br><span class="line">ax1.plot(arange(niter), train_loss)</span><br><span class="line">ax2.plot(test_interval * arange(len(test_acc)), test_acc, <span class="string">'r'</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">'iteration'</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">'train loss'</span>)</span><br><span class="line">ax2.set_ylabel(<span class="string">'test accuracy'</span>)</span><br><span class="line">ax2.set_title(<span class="string">'Custom Test Accuracy: &#123;:.2f&#125;'</span>.format(test_acc[<span class="number">-1</span>]))</span><br></pre></td></tr></table></figure>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/01-learning-lenet.ipynb" target="_blank" rel="external">Solving in Python with LeNet</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Classification with Caffenet]]></title>
      <url>http://buptldy.github.io/2016/05/03/2016-05-03-Classification%20with%20Caffenet/</url>
      <content type="html"><![CDATA[<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-3/46730593.jpg" alt=""><br></center>

<a id="more"></a>
<p>Caffe直接使用训练好的CaffeNet模型来进行分类，Caffe的安装有很多教程，<a href="http://ixez.info/?p=105" target="_blank" rel="external">千秋轻松装Caffe教程（含CUDA 7.0和CuDNN）
</a>这个教程说的很详细，其中比较繁琐的就是CUDA的安装了，可以参考这里：<a href="http://buptldy.github.io/2016/04/09/2016-04-09-Deepin%20CUDA%E5%AE%89%E8%A3%85%E5%8F%8AKeras%E4%BD%BF%E7%94%A8GPU%E6%A8%A1%E5%BC%8F%E8%BF%90%E8%A1%8C/">Deepin CUDA安装及Keras使用GPU模式运行</a>。其中遇到的一个比较大的坑就是cuDNN的安装，首先得确定你的GPU是否支持cuDNN，cuDNN要求GPU的计算能力在3.0以上，这里<a href="http://developer.nvidia.com/cuda-gpus" target="_blank" rel="external"> http://developer.nvidia.com/cuda-gpus</a>可以查询GPU的计算能力，也能查询你的GPU是否支持CUDA，如果你的GPU不支持cuDNN但是支持CUDA，在编译配置文件注释掉<code>USE_CUDNN :=1</code>和<code>CPU_ONLY :=1</code>就可以使用CUDA了。如果你的GPU支持GUDA和cuDNN，得注意你下的Caffe所支持cuDNN的版本，这里可以查看<a href="http://caffe.berkeleyvision.org/installation.html" target="_blank" rel="external">http://caffe.berkeleyvision.org/installation.html</a>。</p>
<p>在这里我们比较下CPU和GPU模式下，网络的运行速度，并了解模型特征的提取。</p>
<h2 id="设置环境"><a href="#设置环境" class="headerlink" title="设置环境"></a>设置环境</h2><p>导入Python,numpy,matplotlib</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="comment"># set display defaults</span></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">10</span>, <span class="number">10</span>)        <span class="comment"># large images</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span>  <span class="comment"># don't interpolate: show square pixels</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span>  <span class="comment"># use grayscale output rather than a (potentially misleading) color heatmap</span></span><br></pre></td></tr></table></figure>
<p>导入caffe，其中注意caffe的路径设置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">caffe_root=<span class="string">'/home/ldy/workspace/caffe/'</span> <span class="comment">#设置你caffe的安装目录</span></span><br><span class="line">sys.path.insert(<span class="number">0</span>,caffe_root+<span class="string">'python'</span>)</span><br><span class="line"><span class="keyword">import</span> caffe                            <span class="comment">#导入caffe</span></span><br></pre></td></tr></table></figure>
<p>第一次运行需要联网下载模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">if</span> os.path.isfile(caffe_root + <span class="string">'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel'</span>):</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'CaffeNet found.'</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Downloading pre-trained CaffeNet model...'</span></span><br><span class="line">    !/home/ldy/workspace/caffe/scripts/download_model_binary.py /home/ldy/workspace/caffe/models/bvlc_reference_caffenet</span><br></pre></td></tr></table></figure>
<pre><code>CaffeNet found.
</code></pre><h2 id="设置网络并对输入进行处理"><a href="#设置网络并对输入进行处理" class="headerlink" title="设置网络并对输入进行处理"></a>设置网络并对输入进行处理</h2><p>设置CPU模式并从本地加载网络</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">caffe.set_mode_cpu()</span><br><span class="line"></span><br><span class="line">model_def = caffe_root + <span class="string">'models/bvlc_reference_caffenet/deploy.prototxt'</span></span><br><span class="line">model_weights = caffe_root + <span class="string">'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel'</span></span><br><span class="line"></span><br><span class="line">net = caffe.Net(model_def,      <span class="comment"># defines the structure of the model</span></span><br><span class="line">                model_weights,  <span class="comment"># contains the trained weights</span></span><br><span class="line">                caffe.TEST)     <span class="comment"># use test mode (e.g., don't perform dropout)</span></span><br></pre></td></tr></table></figure>
<p>设置输入处理</p>
<p>CaffeNet默认的输入图像格式是BGR模式，像素值是[0,255]然后减去ImageNet的像素平均值，而且图像通道的维数是在第一维。</p>
<p>matplotlib导入图像的格式是RGB,像素值的范围是[0,1]，通道维数在第三维，所以我们需要进行转换。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load the mean ImageNet image (as distributed with Caffe) for subtraction</span></span><br><span class="line">mu = np.load(caffe_root + <span class="string">'python/caffe/imagenet/ilsvrc_2012_mean.npy'</span>)</span><br><span class="line">mu = mu.mean(<span class="number">1</span>).mean(<span class="number">1</span>)  <span class="comment"># average over pixels to obtain the mean (BGR) pixel values</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'mean-subtracted values:'</span>, zip(<span class="string">'BGR'</span>, mu)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create transformer for the input called 'data'</span></span><br><span class="line">transformer = caffe.io.Transformer(&#123;<span class="string">'data'</span>: net.blobs[<span class="string">'data'</span>].data.shape&#125;)</span><br><span class="line"></span><br><span class="line">transformer.set_transpose(<span class="string">'data'</span>, (<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>))  <span class="comment"># move image channels to outermost dimension</span></span><br><span class="line">transformer.set_mean(<span class="string">'data'</span>, mu)            <span class="comment"># subtract the dataset-mean value in each channel</span></span><br><span class="line">transformer.set_raw_scale(<span class="string">'data'</span>, <span class="number">255</span>)      <span class="comment"># rescale from [0, 1] to [0, 255]</span></span><br><span class="line">transformer.set_channel_swap(<span class="string">'data'</span>, (<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>))  <span class="comment"># swap channels from RGB to BGR</span></span><br></pre></td></tr></table></figure></p>
<pre><code>mean-subtracted values: [(&apos;B&apos;, 104.0069879317889), (&apos;G&apos;, 116.66876761696767), (&apos;R&apos;, 122.6789143406786)]
</code></pre><h2 id="CPU模式分类"><a href="#CPU模式分类" class="headerlink" title="CPU模式分类"></a>CPU模式分类</h2><p>设置输入的大小</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set the size of the input (we can skip this if we're happy</span></span><br><span class="line"><span class="comment">#  with the default; we can also change it later, e.g., for different batch sizes)</span></span><br><span class="line">net.blobs[<span class="string">'data'</span>].reshape(<span class="number">50</span>,        <span class="comment"># batch size</span></span><br><span class="line">                          <span class="number">3</span>,         <span class="comment"># 3-channel (BGR) images</span></span><br><span class="line">                          <span class="number">227</span>, <span class="number">227</span>)  <span class="comment"># image size is 227x227</span></span><br></pre></td></tr></table></figure>
<p>加载图片并转换</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">image = caffe.io.load_image(caffe_root + <span class="string">'examples/images/cat.jpg'</span>)</span><br><span class="line">transformed_image = transformer.preprocess(<span class="string">'data'</span>, image)</span><br><span class="line">plt.imshow(image)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.image.AxesImage at 0x7f7ba44f0a50&gt;
</code></pre><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-3/52956146.jpg" alt=""><br></center>

<p>进行分类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># copy the image data into the memory allocated for the net</span></span><br><span class="line">net.blobs[<span class="string">'data'</span>].data[...] = transformed_image</span><br><span class="line"></span><br><span class="line"><span class="comment">### perform classification</span></span><br><span class="line">output = net.forward()</span><br><span class="line"></span><br><span class="line">output_prob = output[<span class="string">'prob'</span>][<span class="number">0</span>]  <span class="comment"># the output probability vector for the first image in the batch</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'predicted class is:'</span>, output_prob.argmax()</span><br></pre></td></tr></table></figure>
<pre><code>predicted class is: 281
</code></pre><p>从上面的输出，我们得到输入的图片得到的类别可能是第281类，但是并不知道它对应的标签，下面我们来加载ImageNet的标签(首次需要联网)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load ImageNet labels</span></span><br><span class="line">labels_file = caffe_root + <span class="string">'data/ilsvrc12/synset_words.txt'</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(labels_file):</span><br><span class="line">    !/home/ldy/workspace/caffe/data/ilsvrc12/get_ilsvrc_aux.sh</span><br><span class="line"></span><br><span class="line">labels = np.loadtxt(labels_file, str, delimiter=<span class="string">'\t'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'output label:'</span>, labels[output_prob.argmax()]</span><br></pre></td></tr></table></figure>
<pre><code>Downloading...
--2016-05-03 10:54:43--  http://dl.caffe.berkeleyvision.org/caffe_ilsvrc12.tar.gz
正在解析主机 dl.caffe.berkeleyvision.org (dl.caffe.berkeleyvision.org)... 169.229.222.251
正在连接 dl.caffe.berkeleyvision.org (dl.caffe.berkeleyvision.org)|169.229.222.251|:80... 已连接。
已发出 HTTP 请求，正在等待回应... 200 OK
长度：17858008 (17M) [application/octet-stream]
正在保存至: “caffe_ilsvrc12.tar.gz”

caffe_ilsvrc12.tar. 100%[===================&gt;]  17.03M  2.54MB/s    in 8.9s    

2016-05-03 10:54:53 (1.91 MB/s) - 已保存 “caffe_ilsvrc12.tar.gz” [17858008/17858008])

Unzipping...
Done.
output label: n02123045 tabby, tabby cat
</code></pre><p>现在我们得到了输出为｀tabby cat｀，如果我们想得到其他的可能类别，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sort top five predictions from softmax output</span></span><br><span class="line">top_inds = output_prob.argsort()[::<span class="number">-1</span>][:<span class="number">5</span>]  <span class="comment"># reverse sort and take five largest items</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'probabilities and labels:'</span></span><br><span class="line">zip(output_prob[top_inds], labels[top_inds])</span><br></pre></td></tr></table></figure>
<pre><code>probabilities and labels:
[(0.31243625, &apos;n02123045 tabby, tabby cat&apos;),
 (0.23797135, &apos;n02123159 tiger cat&apos;),
 (0.12387258, &apos;n02124075 Egyptian cat&apos;),
 (0.10075716, &apos;n02119022 red fox, Vulpes vulpes&apos;),
 (0.070957333, &apos;n02127052 lynx, catamount&apos;)]
</code></pre><h2 id="切换到GPU模式"><a href="#切换到GPU模式" class="headerlink" title="切换到GPU模式"></a>切换到GPU模式</h2><p>查看CPU模式花费的时间</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%timeit net.forward()</span><br></pre></td></tr></table></figure>
<pre><code>1 loop, best of 3: 8.87 s per loop
</code></pre><p>切换到GPU模式，查看花费时间</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#caffe.set_device(0)  # if we have multiple GPUs, pick the first one</span></span><br><span class="line">caffe.set_mode_gpu()</span><br><span class="line">net.forward()  <span class="comment"># run once before timing to set up memory</span></span><br><span class="line">%timeit net.forward()</span><br></pre></td></tr></table></figure>
<pre><code>1 loop, best of 3: 2.27 s per loop
</code></pre><p>##查看中间输入</p>
<p>神经网络不仅仅是一个黑盒子，我们可以查看一些中间结果和参数。</p>
<p>查看激活函数输出的数据维数，格式为(batch_size, channel_dim, height, width)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># for each layer, show the output shape</span></span><br><span class="line"><span class="keyword">for</span> layer_name, blob <span class="keyword">in</span> net.blobs.iteritems():</span><br><span class="line">    <span class="keyword">print</span> layer_name + <span class="string">'\t'</span> + str(blob.data.shape)</span><br></pre></td></tr></table></figure>
<pre><code>data    (50, 3, 227, 227)
conv1    (50, 96, 55, 55)
pool1    (50, 96, 27, 27)
norm1    (50, 96, 27, 27)
conv2    (50, 256, 27, 27)
pool2    (50, 256, 13, 13)
norm2    (50, 256, 13, 13)
conv3    (50, 384, 13, 13)
conv4    (50, 384, 13, 13)
conv5    (50, 256, 13, 13)
pool5    (50, 256, 6, 6)
fc6    (50, 4096)
fc7    (50, 4096)
fc8    (50, 1000)
prob    (50, 1000)
</code></pre><p>查看权值参数的维数，权值格式为(output_channels, input_channels, filter_height, filter_width)，偏置的格式为(output_channels,)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> layer_name, param <span class="keyword">in</span> net.params.iteritems():</span><br><span class="line">    <span class="keyword">print</span> layer_name + <span class="string">'\t'</span> + str(param[<span class="number">0</span>].data.shape), str(param[<span class="number">1</span>].data.shape)</span><br></pre></td></tr></table></figure>
<pre><code>conv1    (96, 3, 11, 11) (96,)
conv2    (256, 48, 5, 5) (256,)
conv3    (384, 256, 3, 3) (384,)
conv4    (384, 192, 3, 3) (384,)
conv5    (256, 192, 3, 3) (256,)
fc6    (4096, 9216) (4096,)
fc7    (4096, 4096) (4096,)
fc8    (1000, 4096) (1000,)
</code></pre><h2 id="输出可视化"><a href="#输出可视化" class="headerlink" title="输出可视化"></a>输出可视化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vis_square</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="string">"""Take an array of shape (n, height, width) or (n, height, width, 3)</span><br><span class="line">       and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)"""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># normalize data for display</span></span><br><span class="line">    data = (data - data.min()) / (data.max() - data.min())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># force the number of filters to be square</span></span><br><span class="line">    n = int(np.ceil(np.sqrt(data.shape[<span class="number">0</span>])))</span><br><span class="line">    padding = (((<span class="number">0</span>, n ** <span class="number">2</span> - data.shape[<span class="number">0</span>]),</span><br><span class="line">               (<span class="number">0</span>, <span class="number">1</span>), (<span class="number">0</span>, <span class="number">1</span>))                 <span class="comment"># add some space between filters</span></span><br><span class="line">               + ((<span class="number">0</span>, <span class="number">0</span>),) * (data.ndim - <span class="number">3</span>))  <span class="comment"># don't pad the last dimension (if there is one)</span></span><br><span class="line">    data = np.pad(data, padding, mode=<span class="string">'constant'</span>, constant_values=<span class="number">1</span>)  <span class="comment"># pad with ones (white)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># tile the filters into an image</span></span><br><span class="line">    data = data.reshape((n, n) + data.shape[<span class="number">1</span>:]).transpose((<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>) + tuple(range(<span class="number">4</span>, data.ndim + <span class="number">1</span>)))</span><br><span class="line">    data = data.reshape((n * data.shape[<span class="number">1</span>], n * data.shape[<span class="number">3</span>]) + data.shape[<span class="number">4</span>:])</span><br><span class="line"></span><br><span class="line">    plt.imshow(data); plt.axis(<span class="string">'off'</span>)</span><br></pre></td></tr></table></figure>
<p>第一层卷积滤波器<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the parameters are a list of [weights, biases]</span></span><br><span class="line">filters = net.params[<span class="string">'conv1'</span>][<span class="number">0</span>].data</span><br><span class="line">vis_square(filters.transpose(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure></p>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-3/10647296.jpg" alt=""><br></center>

<p>第一层卷积层的输出<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">feat = net.blobs[<span class="string">'conv1'</span>].data[<span class="number">0</span>, :<span class="number">36</span>]</span><br><span class="line">vis_square(feat)</span><br></pre></td></tr></table></figure></p>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-3/25648526.jpg" alt=""><br></center>

<p>第五层pooling之后的输出<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">feat = net.blobs[<span class="string">'pool5'</span>].data[<span class="number">0</span>]</span><br><span class="line">vis_square(feat)</span><br></pre></td></tr></table></figure></p>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-3/47782240.jpg" alt=""><br></center>

<p>第一个全连接层的输出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">feat = net.blobs[<span class="string">'fc6'</span>].data[<span class="number">0</span>]</span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(feat.flat)</span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">_ = plt.hist(feat.flat[feat.flat &gt; <span class="number">0</span>], bins=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-3/71842859.jpg" alt=""><br></center>

<p>最后的类别概率输出<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">feat = net.blobs[<span class="string">'prob'</span>].data[<span class="number">0</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">3</span>))</span><br><span class="line">plt.plot(feat.flat)</span><br></pre></td></tr></table></figure></p>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x7f7ba0177d10&gt;]
</code></pre><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-3/77194220.jpg" alt=""><br></center>

<h2 id="对自己的图片分类"><a href="#对自己的图片分类" class="headerlink" title="对自己的图片分类"></a>对自己的图片分类</h2><p>设置好图片的链接就好了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># download an image</span></span><br><span class="line"><span class="comment">#my_image_url = "..."  # paste your URL here</span></span><br><span class="line"><span class="comment"># for example:</span></span><br><span class="line">my_image_url = <span class="string">"https://upload.wikimedia.org/wikipedia/commons/b/be/Orang_Utan%2C_Semenggok_Forest_Reserve%2C_Sarawak%2C_Borneo%2C_Malaysia.JPG"</span></span><br><span class="line">!wget -O image.jpg $my_image_url</span><br><span class="line"></span><br><span class="line"><span class="comment"># transform it and copy it into the net</span></span><br><span class="line">image = caffe.io.load_image(<span class="string">'image.jpg'</span>)</span><br><span class="line">net.blobs[<span class="string">'data'</span>].data[...] = transformer.preprocess(<span class="string">'data'</span>, image)</span><br><span class="line"></span><br><span class="line"><span class="comment"># perform classification</span></span><br><span class="line">net.forward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># obtain the output probabilities</span></span><br><span class="line">output_prob = net.blobs[<span class="string">'prob'</span>].data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># sort top five predictions from softmax output</span></span><br><span class="line">top_inds = output_prob.argsort()[::<span class="number">-1</span>][:<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">plt.imshow(image)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'probabilities and labels:'</span></span><br><span class="line">zip(output_prob[top_inds], labels[top_inds])</span><br></pre></td></tr></table></figure>
<pre><code>--2016-05-03 11:23:33--  https://upload.wikimedia.org/wikipedia/commons/b/be/Orang_Utan%2C_Semenggok_Forest_Reserve%2C_Sarawak%2C_Borneo%2C_Malaysia.JPG
正在解析主机 upload.wikimedia.org (upload.wikimedia.org)... 2620:0:863:ed1a::2:b, 2620:0:863:ed1a::2:b, 198.35.26.112, ...
正在连接 upload.wikimedia.org (upload.wikimedia.org)|2620:0:863:ed1a::2:b|:443... 已连接。
已发出 HTTP 请求，正在等待回应... 200 OK
长度：1443340 (1.4M) [image/jpeg]
正在保存至: “image.jpg”

image.jpg           100%[===================&gt;]   1.38M  1.41MB/s    in 1.0s    

2016-05-03 11:23:35 (1.41 MB/s) - 已保存 “image.jpg” [1443340/1443340])

probabilities and labels:

[(0.9680779, &apos;n02480495 orangutan, orang, orangutang, Pongo pygmaeus&apos;),
 (0.030589299, &apos;n02492660 howler monkey, howler&apos;),
 (0.00085892546, &apos;n02493509 titi, titi monkey&apos;),
 (0.00015429084, &apos;n02493793 spider monkey, Ateles geoffroyi&apos;),
 (7.2596376e-05, &apos;n02488291 langur&apos;)]
</code></pre><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-3/25241651.jpg" alt=""><br></center>

<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/00-classification.ipynb" target="_blank" rel="external">Classification: Instant Recognition with Caffe</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Vim Cheat SHeet]]></title>
      <url>http://buptldy.github.io/2016/04/23/2016-04-23-Vim/</url>
      <content type="html"><![CDATA[<p><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-4-23/88022819.jpg" alt=""><br></center><br><a id="more"></a></p>
<h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><ol>
<li><p>光标在屏幕文本中的移动既可以用箭头键，也可以使用 hjkl 字母键。<br>h (左移)    j (下行)       k (上行)        l (右移)</p>
</li>
<li><p>欲进入 Vim 编辑器(从命令行提示符)，请输入：vim 文件名 &lt;回车&gt;</p>
</li>
<li><p>欲退出 Vim 编辑器，请输入 <esc>   :q!   &lt;回车&gt; 放弃所有改动。或者输入 <esc>   :wq   &lt;回车&gt; 保存改动。</esc></esc></p>
</li>
<li><p>在正常模式下删除光标所在位置的字符，请按： x</p>
</li>
<li><p>欲插入或添加文本，请输入：</p>
<p>i   输入欲插入文本   <esc>        在光标前插入文本<br>A   输入欲添加文本   <esc>             在一行后添加文本</esc></esc></p>
<p>特别提示：按下 <esc> 键会带您回到正常模式或者撤消一个不想输入或部分完整的命令。</esc></p>
</li>
</ol>
<h2 id="删除类命令"><a href="#删除类命令" class="headerlink" title="删除类命令"></a>删除类命令</h2><ol>
<li>欲从当前光标删除至下一个单词，请输入：dw</li>
<li>欲从当前光标删除至当前行末尾，请输入：d$</li>
<li><p>欲删除整行，请输入：dd</p>
</li>
<li><p>欲重复一个动作，请在它前面加上一个数字：2w</p>
</li>
<li><p>在正常模式下修改命令的格式是：</p>
<pre><code>operator   [number]   motion
</code></pre><p>其中：<br>operator - 操作符，代表要做的事情，比如 d 代表删除<br>[number] - 可以附加的数字，代表动作重复的次数<br>motion   - 动作，代表在所操作的文本上的移动，例如 w 代表单词(word)，<br>$ 代表行末等等。</p>
</li>
<li><p>欲移动光标到行首，请按数字0键：0</p>
</li>
<li><p>欲撤消以前的操作，请输入：u (小写的u)<br>欲撤消在一行中所做的改动，请输入：U (大写的U)<br>欲撤消以前的撤消命令，恢复以前的操作结果，请输入：CTRL-R</p>
</li>
</ol>
<h2 id="置入类命令"><a href="#置入类命令" class="headerlink" title="置入类命令"></a>置入类命令</h2><ol>
<li><p>要重新置入已经删除的文本内容，请按小写字母 p 键。该操作可以将已删除<br>的文本内容置于光标之后。如果最后一次删除的是一个整行，那么该行将置<br>于当前光标所在行的下一行。</p>
</li>
<li><p>要替换光标所在位置的字符，请输入小写的 r 和要替换掉原位置字符的新字<br>符即可。</p>
</li>
<li><p>更改类命令允许您改变从当前光标所在位置直到动作指示的位置中间的文本。<br>比如输入 ce 可以替换当前光标到单词的末尾的内容；输入 c$ 可以替换当<br>前光标到行末的内容。</p>
</li>
<li><p>更改类命令的格式是：</p>
<pre><code>c   [number]   motion
</code></pre></li>
</ol>
<h2 id="定位及文件状态"><a href="#定位及文件状态" class="headerlink" title="定位及文件状态"></a>定位及文件状态</h2><ol>
<li><p>CTRL-G 用于显示当前光标所在位置和文件状态信息。<br>G 用于将光标跳转至文件最后一行。<br>先敲入一个行号然后输入大写 G 则是将光标移动至该行号代表的行。<br>gg 用于将光标跳转至文件第一行。</p>
</li>
<li><p>输入 / 然后紧随一个字符串是在当前所编辑的文档中正向查找该字符串。<br>输入 ? 然后紧随一个字符串则是在当前所编辑的文档中反向查找该字符串。<br>完成一次查找之后按 n 键是重复上一次的命令，可在同一方向上查<br>找下一个匹配字符串所在；或者按大写 N 向相反方向查找下一匹配字符串所在。<br>CTRL-O 带您跳转回较旧的位置，CTRL-I 则带您到较新的位置。</p>
</li>
<li><p>如果光标当前位置是括号(、)、[、]、{、}，按 % 会将光标移动到配对的括号上。</p>
</li>
<li><p>在一行内替换头一个字符串 old 为新的字符串 new，请输入  :s/old/new<br>在一行内替换所有的字符串 old 为新的字符串 new，请输入  :s/old/new/g<br>在两行内替换所有的字符串 old 为新的字符串 new，请输入  :#,#s/old/new/g<br>在文件内替换所有的字符串 old 为新的字符串 new，请输入  :%s/old/new/g<br>进行全文替换时询问用户确认每个替换需添加 c 标志        :%s/old/new/gc</p>
</li>
</ol>
<h2 id="在-VIM-内执行外部命令的方法"><a href="#在-VIM-内执行外部命令的方法" class="headerlink" title="在 VIM 内执行外部命令的方法"></a>在 VIM 内执行外部命令的方法</h2><ol>
<li><p>:!command 用于执行一个外部命令 command。</p>
<p>请看一些实际例子：<br> :!dir           :!ls           -  用于显示当前目录的内容。<br> :!del FILENAME   :!rm FILENAME   -  用于删除名为 FILENAME 的文件。</p>
</li>
<li><p>:w FILENAME  可将当前 VIM 中正在编辑的文件保存到名为 FILENAME 的文件中。</p>
</li>
<li><p>v motion :w FILENAME 可将当前编辑文件中可视模式下选中的内容保存到文件FILENAME 中。</p>
</li>
<li><p>:r FILENAME 可提取磁盘文件 FILENAME 并将其插入到当前文件的光标位置后面。</p>
</li>
<li><p>:r !dir 可以读取 dir 命令的输出并将其放置到当前文件的光标位置后面。</p>
</li>
</ol>
<h2 id="打开类命令"><a href="#打开类命令" class="headerlink" title="打开类命令"></a>打开类命令</h2><ol>
<li><p>输入小写的 o 可以在光标下方打开新的一行并进入插入模式。<br>输入大写的 O 可以在光标上方打开新的一行。</p>
</li>
<li><p>输入小写的 a 可以在光标所在位置之后插入文本。<br>输入大写的 A 可以在光标所在行的行末之后插入文本。</p>
</li>
<li><p>e 命令可以使光标移动到单词末尾。</p>
</li>
<li><p>操作符 y 复制文本，p 粘贴先前复制的文本。</p>
</li>
<li><p>输入大写的 R 将进入替换模式，直至按 <esc> 键回到正常模式。</esc></p>
</li>
<li><p>输入 :set xxx 可以设置 xxx 选项。一些有用的选项如下：<br>‘ic’ ‘ignorecase’    查找时忽略字母大小写<br>‘is’ ‘incsearch’    查找短语时显示部分匹配<br>‘hls’ ‘hlsearch’    高亮显示所有的匹配短语<br>选项名可以用完整版本，也可以用缩略版本。</p>
</li>
<li><p>在选项前加上 no 可以关闭选项：  :set noic</p>
</li>
</ol>
<h2 id="获取帮助信息"><a href="#获取帮助信息" class="headerlink" title="获取帮助信息"></a>获取帮助信息</h2><ol>
<li><p>输入 :help 或者按 <f1> 键或 <help> 键可以打开帮助窗口。</help></f1></p>
</li>
<li><p>输入 :help cmd 可以找到关于 cmd 命令的帮助。</p>
</li>
<li><p>输入 CTRL-W CTRL-W  可以使您在窗口之间跳转。</p>
</li>
<li><p>输入 :q 以关闭帮助窗口</p>
</li>
<li><p>您可以创建一个 vimrc 启动脚本文件用来保存您偏好的设置。</p>
</li>
<li><p>当输入 : 命令时，按 CTRL-D 可以查看可能的补全结果。按 <tab> 可以使用一个补全。</tab></p>
</li>
</ol>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Deepin CUDA Install and Run Keras on GPU]]></title>
      <url>http://buptldy.github.io/2016/04/09/2016-04-09-Deepin%20CUDA%20Keras%20GPU/</url>
      <content type="html"><![CDATA[<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-4-9/8883646.jpg" alt=""><br></center>

<a id="more"></a>
<h1 id="Deepin简介"><a href="#Deepin简介" class="headerlink" title="Deepin简介"></a>Deepin简介</h1><p><a href="https://www.deepin.org/" target="_blank" rel="external">Deepin</a>是由武汉深之度科技有限公司开发的Linux发行版,Deepin 为所有人提供稳定、高效的操作系统，强调安全、易用、美观。其口号为“免除新手痛苦，节约老手时间”。</p>
<h1 id="cuda安装"><a href="#cuda安装" class="headerlink" title="cuda安装"></a>cuda安装</h1><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>按照系统的版本下载对应的cuda版本，下载地址：<a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="external">https://developer.nvidia.com/cuda-downloads</a></p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>注意执行安装文件的时候一定要加上’–­­override’，不然会出现错误：’”Toolkit: Installation Failed. Using unsupported Compiler.”‘<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod 755 cuda_7.5.18_linux.run</span><br><span class="line">sudo ./cuda_7.5.18_linux.run --­­override</span><br></pre></td></tr></table></figure></p>
<p><strong>如果你电脑里已经装好比cuda内置的NVIDIA驱动更新的版本，那么在安装的时候就不要选择安装NVIDIA驱动。</strong></p>
<p>安装过程的设置如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">-------------------------------------------------------------</span><br><span class="line">Do you accept the previously read EULA? (accept/decline/quit): accept</span><br><span class="line">You are attempting to install on an unsupported configuration. Do you wish to continue? ((y)es/(n)o) [ default is no ]: y</span><br><span class="line">Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 352.39? ((y)es/(n)o/(q)uit): n</span><br><span class="line">Install the CUDA 7.5 Toolkit? ((y)es/(n)o/(q)uit): y</span><br><span class="line">Enter Toolkit Location [ default is /usr/local/cuda-7.5 ]:</span><br><span class="line">Do you want to install a symbolic link at /usr/local/cuda? ((y)es/(n)o/(q)uit): y</span><br><span class="line">Install the CUDA 7.5 Samples? ((y)es/(n)o/(q)uit): y</span><br><span class="line">Enter CUDA Samples Location [ default is /home/kinghorn ]: /usr/local/cuda-7.5</span><br><span class="line">Installing the CUDA Toolkit in /usr/local/cuda-7.5 ...</span><br><span class="line">Finished copying samples.</span><br><span class="line"></span><br><span class="line">===========</span><br><span class="line">= Summary =</span><br><span class="line">===========</span><br><span class="line"></span><br><span class="line">Driver:   Not Selected</span><br><span class="line">Toolkit:  Installed in /usr/local/cuda-7.5</span><br><span class="line">Samples:  Installed in /usr/local/cuda-7.5</span><br></pre></td></tr></table></figure></p>
<h2 id="环境设置"><a href="#环境设置" class="headerlink" title="环境设置"></a>环境设置</h2><p>打开~/.bashrc<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gedit ~/.bashrc</span><br></pre></td></tr></table></figure></p>
<p>添加下面两条语句：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">export PATH=$PATH:/usr/local/cuda/bin</span><br><span class="line"></span><br><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64</span><br></pre></td></tr></table></figure></p>
<h2 id="强制cuda使用gcc-5"><a href="#强制cuda使用gcc-5" class="headerlink" title="强制cuda使用gcc 5"></a>强制cuda使用gcc 5</h2><p>因为cuda默认不使用gcc&gt;4.8，通过注释掉报错行来强制使用gcc 5。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /usr/local/cuda/include/host_config.h</span><br><span class="line"></span><br><span class="line">//注释掉115行</span><br><span class="line">//#error -- unsupported GNU version! gcc versions later than 4.9 are not supported!</span><br></pre></td></tr></table></figure></p>
<h2 id="运行cuda内置的例子"><a href="#运行cuda内置的例子" class="headerlink" title="运行cuda内置的例子"></a>运行cuda内置的例子</h2><p>为了测试是否安装成功</p>
<p>进入内置例程<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/cuda/samples/1_Utilities/deviceQuery</span><br></pre></td></tr></table></figure></p>
<p>编译<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make</span><br></pre></td></tr></table></figure></p>
<p>运行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./deviceQuery</span><br></pre></td></tr></table></figure></p>
<p>得到结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">CUDA Device Query (Runtime API) version (CUDART static linking)</span><br><span class="line"></span><br><span class="line">Detected 1 CUDA Capable device(s)</span><br><span class="line"></span><br><span class="line">Device 0: &quot;GeForce GT 520M&quot;</span><br><span class="line"> CUDA Driver Version / Runtime Version          8.0 / 7.5</span><br><span class="line"> CUDA Capability Major/Minor version number:    2.1</span><br><span class="line"> Total amount of global memory:                 1024 MBytes (1073414144 bytes)</span><br><span class="line"> ( 1) Multiprocessors, ( 48) CUDA Cores/MP:     48 CUDA Cores</span><br><span class="line"> GPU Max Clock rate:                            1480 MHz (1.48 GHz)</span><br><span class="line"> Memory Clock rate:                             800 Mhz</span><br><span class="line"> Memory Bus Width:                              64-bit</span><br><span class="line"> L2 Cache Size:                                 65536 bytes</span><br><span class="line"> Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65535), 3D=(2048, 2048, 2048)</span><br><span class="line"> Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers</span><br><span class="line"> Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers</span><br><span class="line"> Total amount of constant memory:               65536 bytes</span><br><span class="line"> Total amount of shared memory per block:       49152 bytes</span><br><span class="line"> Total number of registers available per block: 32768</span><br><span class="line"> Warp size:                                     32</span><br><span class="line"> Maximum number of threads per multiprocessor:  1536</span><br><span class="line"> Maximum number of threads per block:           1024</span><br><span class="line"> Max dimension size of a thread block (x,y,z): (1024, 1024, 64)</span><br><span class="line"> Max dimension size of a grid size    (x,y,z): (65535, 65535, 65535)</span><br><span class="line"> Maximum memory pitch:                          2147483647 bytes</span><br><span class="line"> Texture alignment:                             512 bytes</span><br><span class="line"> Concurrent copy and kernel execution:          Yes with 1 copy engine(s)</span><br><span class="line"> Run time limit on kernels:                     No</span><br><span class="line"> Integrated GPU sharing Host Memory:            No</span><br><span class="line"> Support host page-locked memory mapping:       Yes</span><br><span class="line"> Alignment requirement for Surfaces:            Yes</span><br><span class="line"> Device has ECC support:                        Disabled</span><br><span class="line"> Device supports Unified Addressing (UVA):      Yes</span><br><span class="line"> Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0</span><br><span class="line"> Compute Mode:</span><br><span class="line">    &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;</span><br><span class="line"></span><br><span class="line">deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, CUDA Runtime Version = 7.5, NumDevs = 1, Device0 = GeForce GT 520M</span><br><span class="line">Result = PASS</span><br></pre></td></tr></table></figure></p>
<p>如果编译出错，检查是否有强制设置gcc 5来编译；如果输出结果为fail，说明没有检查到显卡，解决方案是升级你的NVIDIA驱动，确保你电脑的NVIDIA驱动版本要不低于cuda的内置版本。</p>
<h1 id="设置Keras运行于GPU模式"><a href="#设置Keras运行于GPU模式" class="headerlink" title="设置Keras运行于GPU模式"></a>设置Keras运行于GPU模式</h1><h2 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h2><p> 使用如下命令行运行<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">THEANO_FLAGS=device=gpu,floatX=float32 python my_keras_script.py</span><br></pre></td></tr></table></figure></p>
<h2 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h2><p>设置$HOME/.theanorc文件</p>
<p>添加如下所示文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">floatX = float32</span><br><span class="line">device = gpu</span><br><span class="line"></span><br><span class="line">[lib]</span><br><span class="line">cnmem = 0.9</span><br><span class="line"></span><br><span class="line">[cuda]</span><br><span class="line">root = /usr/local/cuda</span><br></pre></td></tr></table></figure></p>
<h2 id="方法三"><a href="#方法三" class="headerlink" title="方法三"></a>方法三</h2><p>在你的代码前面，加上如下所示代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> theano</span><br><span class="line">theano.config.device = <span class="string">'gpu'</span></span><br><span class="line">theano.config.floatX = <span class="string">'float32'</span></span><br></pre></td></tr></table></figure></p>
<p>我们来运行Keras里的一个用于电影评论情感分析的例子<a href="https://github.com/fchollet/keras/blob/master/examples/imdb_cnn.py" target="_blank" rel="external">imdb_cnn.py</a>,第一次运行时需要联网，要下载数据库。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''This example demonstrates the use of Convolution1D for text classification.</span><br><span class="line">Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python imdb_cnn.py</span><br><span class="line">Get to 0.835 test accuracy after 2 epochs. 100s/epoch on K520 GPU.</span><br><span class="line">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random.seed(<span class="number">1337</span>)  <span class="comment"># for reproducibility</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> sequence</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense, Dropout, Activation, Flatten</span><br><span class="line"><span class="keyword">from</span> keras.layers.embeddings <span class="keyword">import</span> Embedding</span><br><span class="line"><span class="keyword">from</span> keras.layers.convolutional <span class="keyword">import</span> Convolution1D, MaxPooling1D</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># set parameters:</span></span><br><span class="line">max_features = <span class="number">5000</span></span><br><span class="line">maxlen = <span class="number">100</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">embedding_dims = <span class="number">100</span></span><br><span class="line">nb_filter = <span class="number">250</span></span><br><span class="line">filter_length = <span class="number">3</span></span><br><span class="line">hidden_dims = <span class="number">250</span></span><br><span class="line">nb_epoch = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'Loading data...'</span>)</span><br><span class="line">(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features,</span><br><span class="line">                                                      test_split=<span class="number">0.2</span>)</span><br><span class="line">print(len(X_train), <span class="string">'train sequences'</span>)</span><br><span class="line">print(len(X_test), <span class="string">'test sequences'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Pad sequences (samples x time)'</span>)</span><br><span class="line">X_train = sequence.pad_sequences(X_train, maxlen=maxlen)</span><br><span class="line">X_test = sequence.pad_sequences(X_test, maxlen=maxlen)</span><br><span class="line">print(<span class="string">'X_train shape:'</span>, X_train.shape)</span><br><span class="line">print(<span class="string">'X_test shape:'</span>, X_test.shape)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Build model...'</span>)</span><br><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line"><span class="comment"># we start off with an efficient embedding layer which maps</span></span><br><span class="line"><span class="comment"># our vocab indices into embedding_dims dimensions</span></span><br><span class="line">model.add(Embedding(max_features, embedding_dims, input_length=maxlen))</span><br><span class="line">model.add(Dropout(<span class="number">0.25</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># we add a Convolution1D, which will learn nb_filter</span></span><br><span class="line"><span class="comment"># word group filters of size filter_length:</span></span><br><span class="line">model.add(Convolution1D(nb_filter=nb_filter,</span><br><span class="line">                        filter_length=filter_length,</span><br><span class="line">                        border_mode=<span class="string">'valid'</span>,</span><br><span class="line">                        activation=<span class="string">'relu'</span>,</span><br><span class="line">                        subsample_length=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># we use standard max pooling (halving the output of the previous layer):</span></span><br><span class="line">model.add(MaxPooling1D(pool_length=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># We flatten the output of the conv layer,</span></span><br><span class="line"><span class="comment"># so that we can add a vanilla dense layer:</span></span><br><span class="line">model.add(Flatten())</span><br><span class="line"></span><br><span class="line"><span class="comment"># We add a vanilla hidden layer:</span></span><br><span class="line">model.add(Dense(hidden_dims))</span><br><span class="line">model.add(Dropout(<span class="number">0.25</span>))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># We project onto a single unit output layer, and squash it with a sigmoid:</span></span><br><span class="line">model.add(Dense(<span class="number">1</span>))</span><br><span class="line">model.add(Activation(<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              optimizer=<span class="string">'rmsprop'</span>)</span><br><span class="line">model.fit(X_train, y_train, batch_size=batch_size,</span><br><span class="line">          nb_epoch=nb_epoch, show_accuracy=<span class="keyword">True</span>,</span><br><span class="line">          validation_data=(X_test, y_test))</span><br></pre></td></tr></table></figure></p>
<p>运行这个例子，在K520 GPU上是100s一次循环，我电脑显卡型号为GeForce GT 520M，大概需要175s一次循环，不过比在cpu上运行快多啦，在我这四年前旧电脑cpu上运行差不多要一个小时。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.pugetsystems.com/labs/articles/NVIDIA-CUDA-with-Ubuntu-16-04-beta-on-a-laptop-if-you-just-cannot-wait-775/" target="_blank" rel="external">    NVIDIA CUDA with Ubuntu 16.04 beta on a laptop</a></p>
<p><a href="http://keras.io/faq/#how-can-i-run-keras-on-gpu" target="_blank" rel="external">Keras FAQ</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Keras Introduction]]></title>
      <url>http://buptldy.github.io/2016/04/07/2016-04-07-Keras%20Introduction/</url>
      <content type="html"><![CDATA[<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-4-7/16129390.jpg" alt=""><br></center>

<h1 id="Keras-简介"><a href="#Keras-简介" class="headerlink" title="Keras 简介"></a>Keras 简介</h1><p><a href="http://keras.io/" target="_blank" rel="external">Keras</a>是一个用Python编写的基于 TensorFlow 和 Theano高度模块化的神经网络库。其最大的优点在于样例丰富，现有主流模型封装完美。复杂点的模型可以像搭积木一样搞出来，适合快速地搭建模型。</p>
<a id="more"></a>
<p>安装：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pip install keras</span><br></pre></td></tr></table></figure></p>
<h1 id="Keras里的基本模块"><a href="#Keras里的基本模块" class="headerlink" title="Keras里的基本模块"></a>Keras里的基本模块</h1><h2 id="optimizers"><a href="#optimizers" class="headerlink" title="optimizers"></a>optimizers</h2><p>Keras包含了很多优化方法。比如最常用的随机梯度下降法(SGD)，还有Adagrad、Adadelta、RMSprop、Adam等。下面通过具体的代码介绍一下优化器的使用方法。<br>在编译一个Keras模型时，优化器是2个参数之一（另外一个是损失函数）。看如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()  </span><br><span class="line">model.add(Dense(<span class="number">64</span>, init=<span class="string">'uniform'</span>, input_dim=<span class="number">10</span>))  </span><br><span class="line">model.add(Activation(<span class="string">'tanh'</span>))  </span><br><span class="line">model.add(Activation(<span class="string">'softmax'</span>))  </span><br><span class="line"></span><br><span class="line">sgd = SGD(lr=<span class="number">0.1</span>, decay=<span class="number">1e-6</span>, momentum=<span class="number">0.9</span>, nesterov=<span class="keyword">True</span>)  </span><br><span class="line">model.compile(loss=<span class="string">'mean_squared_error'</span>, optimizer=sgd)</span><br></pre></td></tr></table></figure>
<p>这个例子中是在调用compile之前实例化了一个优化器。我们也可以通过传递名字的方式调用默认的优化器。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># passoptimizer by name: default parameters will be used  </span></span><br><span class="line">model.compile(loss=<span class="string">'mean_squared_error'</span>, optimizer=<span class="string">'sgd'</span>)</span><br></pre></td></tr></table></figure>
<p>SGD（随机梯度下降优化器，性价比最好的算法）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.optimizers.SGD(lr=<span class="number">0.01</span>, momentum=<span class="number">0.</span>, decay=<span class="number">0.</span>, nesterov=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>参数：</p>
<ul>
<li>lr :float&gt;=0，学习速率</li>
<li>momentum :float&gt;=0 参数更新的动量</li>
<li>decay : float&gt;=0 每次更新后学习速率的衰减量</li>
<li>nesterov :Boolean 是否使用Nesterov动量项</li>
</ul>
<h2 id="objectives"><a href="#objectives" class="headerlink" title="objectives"></a>objectives</h2><p>目标函数模块，keras提供了mean_squared_error，mean_absolute_error，squared_hinge，hinge，binary_crossentropy，categorical_crossentropy这几种目标函数。</p>
<p>这里binary_crossentropy 和categorical_crossentropy也就是常说的logloss.</p>
<h2 id="Activations"><a href="#Activations" class="headerlink" title="Activations"></a>Activations</h2><p>激活函数模块，keras提供了linear、sigmoid、hard_sigmoid、tanh、softplus、relu、softplus，另外softmax也放在Activations模块里。此外，像LeakyReLU和PReLU这种比较新的激活函数，keras在keras.layers.advanced_activations模块里提供。</p>
<h2 id="initializations"><a href="#initializations" class="headerlink" title="initializations"></a>initializations</h2><p>权值初始化，在Keras中对权值矩阵初始化的方式很简单，就是在add某一层时，同时注明初始化该层的概率分布是什么就可以了。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># init是关键字，’uniform’表示用均匀分布去初始化  </span></span><br><span class="line">model.add(Dense(<span class="number">64</span>, init=<span class="string">'uniform'</span>))</span><br></pre></td></tr></table></figure>
<p>keras提供了uniform、lecun_uniform、normal、orthogonal、zero、glorot_normal、he_normal这几种。</p>
<h2 id="regularizers"><a href="#regularizers" class="headerlink" title="regularizers"></a>regularizers</h2><p>深度学习容易出现过拟合，通过使用<a href="http://blog.csdn.net/u012162613/article/details/44261657" target="_blank" rel="external">正则化方法</a>，防止过拟合，提高泛化能力。</p>
<p>使用示例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.regularizers <span class="keyword">import</span> l2, activity_l2  </span><br><span class="line">model.add(Dense(<span class="number">64</span>, input_dim=<span class="number">64</span>, W_regularizer=l2(<span class="number">0.01</span>), activity_regularizer=activity_l2(<span class="number">0.01</span>)))</span><br></pre></td></tr></table></figure>
<h2 id="constraints"><a href="#constraints" class="headerlink" title="constraints"></a>constraints</h2><p>除了正则化外，Keras还有一个约束限制功能。函数可以设置在训练网络到最优时对网络参数的约束。这个约束就是限制参数值的取值范围。比如最大值是多少，不允许为负值等。</p>
<p>2个关键的参数：</p>
<ul>
<li>W_constraint：约束主要的权值矩阵</li>
<li>b_constraint：约束偏置值</li>
</ul>
<p>使用示例代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.constraints <span class="keyword">import</span> maxnorm</span><br><span class="line">model.add(Dense(<span class="number">64</span>, W_constraint =maxnorm(<span class="number">2</span>)))</span><br><span class="line"><span class="comment">#限制权值的各个参数不能大于2</span></span><br></pre></td></tr></table></figure></p>
<p>可用的约束限制</p>
<ul>
<li>maxnorm(m=2): 最大值约束</li>
<li>nonneg(): 不允许负值</li>
<li>unitnorm(): 归一化</li>
</ul>
<h1 id="实例：解决XOR问题"><a href="#实例：解决XOR问题" class="headerlink" title="实例：解决XOR问题"></a>实例：解决XOR问题</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Activation, Dense</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> SGD</span><br><span class="line"></span><br><span class="line">X = np.zeros((<span class="number">4</span>, <span class="number">2</span>), dtype=<span class="string">'uint8'</span>)<span class="comment">#训练数据</span></span><br><span class="line">y = np.zeros(<span class="number">4</span>, dtype=<span class="string">'uint8'</span>)<span class="comment">#训练标签</span></span><br><span class="line"></span><br><span class="line">X[<span class="number">0</span>] = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">y[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">X[<span class="number">1</span>] = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">y[<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">X[<span class="number">2</span>] = [<span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">y[<span class="number">2</span>] = <span class="number">1</span></span><br><span class="line">X[<span class="number">3</span>] = [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">y[<span class="number">3</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">model = Sequential()<span class="comment">#实例化模型</span></span><br><span class="line">model.add(Dense(<span class="number">2</span>, input_dim=<span class="number">2</span>))<span class="comment">#输入层，输入数据维数为2</span></span><br><span class="line">model.add(Activation(<span class="string">'sigmoid'</span>))<span class="comment">#设置激活函数</span></span><br><span class="line">model.add(Dense(<span class="number">1</span>))</span><br><span class="line">model.add(Activation(<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">sgd = SGD(lr=<span class="number">0.1</span>, decay=<span class="number">1e-6</span>, momentum=<span class="number">0.9</span>, nesterov=<span class="keyword">True</span>)</span><br><span class="line">model.compile(loss=<span class="string">'mean_squared_error'</span>, optimizer=sgd)</span><br><span class="line"></span><br><span class="line">history = model.fit(X, y, nb_epoch=<span class="number">10000</span>, batch_size=<span class="number">4</span>, show_accuracy=<span class="keyword">True</span>, verbose=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> model.predict(X)<span class="comment">#预测</span></span><br></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://keras.io/" target="_blank" rel="external">Keras Documentation</a></p>
<p><a href="http://www.lai18.com/user/301164.html" target="_blank" rel="external">Keras 学习随笔</a></p>
<p><a href="http://blog.csdn.net/u012162613/article/details/45397033" target="_blank" rel="external">深度学习框架Keras简介</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Connect to MySQL in Python]]></title>
      <url>http://buptldy.github.io/2016/04/04/2016-04-04-Python%20MySQL/</url>
      <content type="html"><![CDATA[<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-4-4/94530193.jpg" alt=""><br></center>

<h1 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h1><p>  安装MySQL：</p>
<pre><code>sudo apt-get install mysql-server
</code></pre><a id="more"></a>
<p>  安装MySQLdb模块：</p>
<pre><code>sudo apt-get install python-mysqldb
</code></pre><p>  测试是否安装成功：</p>
<pre><code>import MySQLdb
</code></pre><h1 id="MySQL建立数据库"><a href="#MySQL建立数据库" class="headerlink" title="MySQL建立数据库"></a>MySQL建立数据库</h1><p>  进入MySQL：</p>
<pre><code>mysql -u root -p
</code></pre><p>  进入MySQL并打开补全：</p>
<pre><code>mysql -u USER -p --local-infile=1  --auto-rehash
</code></pre><p>  建立一个数据库：</p>
<pre><code>create database testdb character set utf8;
</code></pre><p>  调用已经建立的数据库：</p>
<pre><code>use testdb;
</code></pre><p>  建立一个数据表：</p>
<pre><code>create table users
(id int(2) not null primary key auto_increment,
username varchar(40),
password text,email text)
default charset=utf8;
</code></pre><p>  显示表格：</p>
<pre><code>show tables;
</code></pre><p>  显示表格结构：</p>
<pre><code>desc users;
</code></pre><p>  表格中插入数据：</p>
<pre><code>insert into
users(username,password,email)
values(&quot;qiwsir&quot;,&quot;123123&quot;,&quot;qiwsir@gmail.com&quot;);
</code></pre><p>  查询表格内容：</p>
<pre><code>select * from users;
</code></pre><h1 id="Python操作数据库"><a href="#Python操作数据库" class="headerlink" title="Python操作数据库"></a>Python操作数据库</h1><p>  连接数据库：</p>
<pre><code>conn =
MySQLdb.connect
(host=&quot;localhost&quot;,user=&quot;root&quot;,
passwd=&quot;123123&quot;,db=&quot;qiwsirtest&quot;,charset=&quot;utf8&quot;)
</code></pre><p>  用游标（指针）cursor的方式操作数据库：</p>
<pre><code>cur = conn.cursor()
</code></pre><p>  在表中插入一条记录：</p>
<pre><code>cur.execute(&quot;insert into
users (username,password,email)
values (%s,%s,%s)&quot;,(&quot;python&quot;,&quot;123456&quot;,&quot;python@gmail.com&quot;))
</code></pre><p>  使插入的记录生效，提交：</p>
<pre><code>conn.commit()
</code></pre><p>  同时插入多条记录：</p>
<pre><code>cur.executemany(&quot;insert into
users (username,password,email)
values (%s,%s,%s)&quot;,
((&quot;google&quot;,&quot;111222&quot;,&quot;g@gmail.com&quot;),
(&quot;facebook&quot;,&quot;222333&quot;,&quot;f@face.book&quot;),
(&quot;github&quot;,&quot;333444&quot;,&quot;git@hub.com&quot;),
(&quot;docker&quot;,&quot;444555&quot;,&quot;doc@ker.com&quot;)))
</code></pre><p>  要记得提交生效</p>
<p>  查询数据库：</p>
<pre><code>cur.execute(&quot;select * from users&quot;)
</code></pre><p>  上述操作只是得到结果的指针，想要显示查询结果，可以用到以下方法：</p>
<ul>
<li>fetchall(self):接收全部的返回结果行.</li>
<li>fwetchmany(size=None):接收size条返回结果行.如果size的值大于返回的结果行的数量,则会返回cursor.arraysize条数据.</li>
<li>fetchone():返回一条结果行.</li>
<li>scroll(value,mode=’relative’):移动指针到某一行.如果mode=’relative’,则表示从当前所在行移动value条,如果mode=’absolute’,则表示从结果集的第一行移动value条.</li>
</ul>
<p>  python的MySQLdb指针提供了一个参数，可以实现将读取到的数据变成字典形式：</p>
<pre><code>cur = conn.cursor(cursorclass=MySQLdb.cursors.DictCursor)
</code></pre><p>  更新数据库：</p>
<pre><code>cur.execute(&quot;update users set username=%s where id=2&quot;,(&quot;mypython&quot;))
</code></pre><p>   如果再下述连接数据库的语句中，如果没有指定具体的数据库，则连接到MySQL：</p>
<pre><code>conn = MySQLdb.connect
(host=&quot;localhost&quot;,
user=&quot;root&quot;,passwd=&quot;123123&quot;,
db=&quot;qiwsirtest&quot;,charset=&quot;utf8&quot;)
</code></pre><p>  然后可以通过用conn.select_db()选择要操作的数据库：</p>
<pre><code>conn.select_db(&quot;testdb&quot;)
</code></pre><p>  不选数据库，而是要新建一个数据库，如下所示：</p>
<pre><code>cur = conn.cursor()
cur.execute(&quot;create database newtest&quot;)
</code></pre><p>  建立数据库之后，就可以选择这个数据库，然后在这个数据库中建立一个数据表：</p>
<pre><code>cur.execute(&quot;create table newusers
(id int(2) primary key auto_increment,
username varchar(20), age int(2), email text)&quot;)
</code></pre><p>  当进行完有关数据操作之后，最后要做的就是关闭游标（指针）和连接。用如下命令实现：</p>
<pre><code>cur.close()
conn.close()
</code></pre><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>  <a href="http://python.xiaoleilu.com/300/302.html" target="_blank" rel="external">通过Python连接数据库</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Human Detection using HOG-Linear SVM in Python]]></title>
      <url>http://buptldy.github.io/2016/04/01/2016-04-01-Human%20Detection/</url>
      <content type="html"><![CDATA[<p><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-6-12/14156501.jpg" alt=""><br></center><br><a id="more"></a></p>
<h2 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h2><p>训练数据来自<a href="http://pascal.inrialpes.fr/data/human/" target="_blank" rel="external">INRIA Person Dataset</a>,其中正样本为64*128的人体图像，负样本为64*128的非人体图像，如下图所示。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-27/56925025.jpg" alt=""></p>
<h2 id="HOG特征"><a href="#HOG特征" class="headerlink" title="HOG特征"></a>HOG特征</h2><p>HOG特征详细介绍：<a href="http://buptldy.github.io/2016/03/31/2016-03-31-HOG%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/">HOG论文笔记</a>提取HOG特征的方法使用了skimage库中的hog函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_features</span><span class="params">()</span>:</span></span><br><span class="line">    des_type = <span class="string">'HOG'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># If feature directories don't exist, create them</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(pos_feat_ph):</span><br><span class="line">        os.makedirs(pos_feat_ph)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># If feature directories don't exist, create them</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(neg_feat_ph):</span><br><span class="line">        os.makedirs(neg_feat_ph)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"Calculating the descriptors for the positive samples and saving them"</span></span><br><span class="line">    <span class="keyword">for</span> im_path <span class="keyword">in</span> glob.glob(os.path.join(pos_im_path, <span class="string">"*"</span>)):</span><br><span class="line">        <span class="comment">#print im_path</span></span><br><span class="line"></span><br><span class="line">        im = imread(im_path, as_grey=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">if</span> des_type == <span class="string">"HOG"</span>:</span><br><span class="line">            fd = hog(im, orientations, pixels_per_cell, cells_per_block, visualize, normalize)</span><br><span class="line">        fd_name = os.path.split(im_path)[<span class="number">1</span>].split(<span class="string">"."</span>)[<span class="number">0</span>] + <span class="string">".feat"</span></span><br><span class="line">        fd_path = os.path.join(pos_feat_ph, fd_name)</span><br><span class="line">        joblib.dump(fd, fd_path)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"Positive features saved in &#123;&#125;"</span>.format(pos_feat_ph)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"Calculating the descriptors for the negative samples and saving them"</span></span><br><span class="line">    <span class="keyword">for</span> im_path <span class="keyword">in</span> glob.glob(os.path.join(neg_im_path, <span class="string">"*"</span>)):</span><br><span class="line">        im = imread(im_path, as_grey=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">if</span> des_type == <span class="string">"HOG"</span>:</span><br><span class="line">            fd = hog(im,  orientations, pixels_per_cell, cells_per_block, visualize, normalize)</span><br><span class="line">        fd_name = os.path.split(im_path)[<span class="number">1</span>].split(<span class="string">"."</span>)[<span class="number">0</span>] + <span class="string">".feat"</span></span><br><span class="line">        fd_path = os.path.join(neg_feat_ph, fd_name)</span><br><span class="line"></span><br><span class="line">        joblib.dump(fd, fd_path)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"Negative features saved in &#123;&#125;"</span>.format(neg_feat_ph)</span><br></pre></td></tr></table></figure>
<h2 id="训练SVM"><a href="#训练SVM" class="headerlink" title="训练SVM"></a>训练SVM</h2><p>因为每张图片提取出来的HOG特征有6480维，所以我们使用线性SVM就足够可分。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_svm</span><span class="params">()</span>:</span></span><br><span class="line">    pos_feat_path = <span class="string">'../data/features/pos'</span></span><br><span class="line">    neg_feat_path = <span class="string">'../data/features/neg'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Classifiers supported</span></span><br><span class="line">    clf_type = <span class="string">'LIN_SVM'</span></span><br><span class="line"></span><br><span class="line">    fds = []</span><br><span class="line">    labels = []</span><br><span class="line">    <span class="comment"># Load the positive features</span></span><br><span class="line">    <span class="keyword">for</span> feat_path <span class="keyword">in</span> glob.glob(os.path.join(pos_feat_path,<span class="string">"*.feat"</span>)):</span><br><span class="line">        fd = joblib.load(feat_path)</span><br><span class="line">        fds.append(fd)</span><br><span class="line">        labels.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Load the negative features</span></span><br><span class="line">    <span class="keyword">for</span> feat_path <span class="keyword">in</span> glob.glob(os.path.join(neg_feat_path,<span class="string">"*.feat"</span>)):</span><br><span class="line">        fd = joblib.load(feat_path)</span><br><span class="line">        fds.append(fd)</span><br><span class="line">        labels.append(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">print</span> np.array(fds).shape,len(labels)</span><br><span class="line">    <span class="keyword">if</span> clf_type <span class="keyword">is</span> <span class="string">"LIN_SVM"</span>:</span><br><span class="line">        clf = LinearSVC()</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"Training a Linear SVM Classifier"</span></span><br><span class="line">        clf.fit(fds, labels)</span><br><span class="line">        <span class="comment"># If feature directories don't exist, create them</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(os.path.split(model_path)[<span class="number">0</span>]):</span><br><span class="line">            os.makedirs(os.path.split(model_path)[<span class="number">0</span>])</span><br><span class="line">        joblib.dump(clf, model_path)</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"Classifier saved to &#123;&#125;"</span>.format(model_path)</span><br></pre></td></tr></table></figure></p>
<h2 id="进行人体检测"><a href="#进行人体检测" class="headerlink" title="进行人体检测"></a>进行人体检测</h2><p>因为对进行人体检测的输入图片大小是未知的，所以需要对图片进行尺度缩放，使用的方法如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skimage.transform <span class="keyword">import</span> pyramid_gaussian</span><br><span class="line">pyramid_gaussian(im, downscale=downscale)</span><br></pre></td></tr></table></figure>
<p>在缩放的尺度上对图片进行滑动窗口检测，可能会在不同尺度上都检测到了目标，这样会造成标记的混乱，可以使用非极大值抑制的方法对重复标记的的目标经行剔除。可以从imutils包中导入非极大值抑制函数。</p>
<p>imutils包安装<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pip install imutils</span><br></pre></td></tr></table></figure></p>
<p>使用非极大值抑制函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> imutils.object_detection <span class="keyword">import</span> non_max_suppression</span><br></pre></td></tr></table></figure></p>
<p>完整检测代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sliding_window</span><span class="params">(image, window_size, step_size)</span>:</span></span><br><span class="line">    <span class="string">'''</span><br><span class="line">    This function returns a patch of the input image `image` of size equal</span><br><span class="line">    to `window_size`. The first image returned top-left co-ordinates (0, 0)</span><br><span class="line">    and are increment in both x and y directions by the `step_size` supplied.</span><br><span class="line">    So, the input parameters are -</span><br><span class="line">    * `image` - Input Image</span><br><span class="line">    * `window_size` - Size of Sliding Window</span><br><span class="line">    * `step_size` - Incremented Size of Window</span><br><span class="line"></span><br><span class="line">    The function returns a tuple -</span><br><span class="line">    (x, y, im_window)</span><br><span class="line">    where</span><br><span class="line">    * x is the top-left x co-ordinate</span><br><span class="line">    * y is the top-left y co-ordinate</span><br><span class="line">    * im_window is the sliding window image</span><br><span class="line">    '''</span></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> xrange(<span class="number">0</span>, image.shape[<span class="number">0</span>], step_size[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> xrange(<span class="number">0</span>, image.shape[<span class="number">1</span>], step_size[<span class="number">0</span>]):</span><br><span class="line">            <span class="keyword">yield</span> (x, y, image[y:y + window_size[<span class="number">1</span>], x:x + window_size[<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detector</span><span class="params">(filename)</span>:</span></span><br><span class="line">    im=cv2.imread(filename)</span><br><span class="line">    im = imutils.resize(im, width=min(<span class="number">400</span>, im.shape[<span class="number">1</span>]))</span><br><span class="line">    min_wdw_sz = (<span class="number">64</span>, <span class="number">128</span>)</span><br><span class="line">    step_size = (<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">    downscale = <span class="number">1.25</span></span><br><span class="line">    <span class="comment"># 导入SVM模型</span></span><br><span class="line">    clf = joblib.load(model_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># List to store the detections</span></span><br><span class="line">    detections = []</span><br><span class="line">    <span class="comment"># The current scale of the image</span></span><br><span class="line">    scale = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 在图像金字塔模型中对每个滑动窗口经行预测</span></span><br><span class="line">    <span class="keyword">for</span> im_scaled <span class="keyword">in</span> pyramid_gaussian(im, downscale=downscale):</span><br><span class="line">        <span class="comment"># This list contains detections at the current scale</span></span><br><span class="line">        cd = []</span><br><span class="line">        <span class="comment"># If the width or height of the scaled image is less than</span></span><br><span class="line">        <span class="comment"># the width or height of the window, then end the iterations.</span></span><br><span class="line">        <span class="keyword">if</span> im_scaled.shape[<span class="number">0</span>] &lt; min_wdw_sz[<span class="number">1</span>] <span class="keyword">or</span> im_scaled.shape[<span class="number">1</span>] &lt; min_wdw_sz[<span class="number">0</span>]:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">for</span> (x, y, im_window) <span class="keyword">in</span> sliding_window(im_scaled, min_wdw_sz, step_size):</span><br><span class="line">            <span class="keyword">if</span> im_window.shape[<span class="number">0</span>] != min_wdw_sz[<span class="number">1</span>] <span class="keyword">or</span> im_window.shape[<span class="number">1</span>] != min_wdw_sz[<span class="number">0</span>]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># 计算每个窗口的Hog特征</span></span><br><span class="line">            im_window=color.rgb2gray(im_window)</span><br><span class="line">            fd = hog(im_window, orientations, pixels_per_cell, cells_per_block, visualize, normalize)</span><br><span class="line"></span><br><span class="line">            fd=fd.reshape(<span class="number">1</span>,<span class="number">-1</span>)</span><br><span class="line">            pred = clf.predict(fd)</span><br><span class="line">            <span class="keyword">if</span> pred == <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> clf.decision_function(fd)&gt;<span class="number">0.5</span>:</span><br><span class="line">                    detections.append((x, y, clf.decision_function(fd),<span class="comment">#样本点到超平面的距离</span></span><br><span class="line">                        int(min_wdw_sz[<span class="number">0</span>]*(downscale**scale)),</span><br><span class="line">                        int(min_wdw_sz[<span class="number">1</span>]*(downscale**scale))))</span><br><span class="line">                    cd.append(detections[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">        scale+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    clone = im.copy()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 画出矩形框</span></span><br><span class="line">    <span class="keyword">for</span> (x_tl, y_tl, _, w, h) <span class="keyword">in</span> detections:</span><br><span class="line">        cv2.rectangle(im, (x_tl, y_tl), (x_tl+w, y_tl+h), (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), thickness=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    rects = np.array([[x, y, x + w, y + h] <span class="keyword">for</span> (x, y,_, w, h) <span class="keyword">in</span> detections])</span><br><span class="line">    pick = non_max_suppression(rects, probs=<span class="keyword">None</span>, overlapThresh=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (xA, yA, xB, yB) <span class="keyword">in</span> pick:</span><br><span class="line">        cv2.rectangle(clone, (xA, yA), (xB, yB), (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    plt.axis(<span class="string">"off"</span>)</span><br><span class="line">    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))</span><br><span class="line">    plt.title(<span class="string">"Raw Detections before NMS"</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    plt.axis(<span class="string">"off"</span>)</span><br><span class="line">    plt.imshow(cv2.cvtColor(clone, cv2.COLOR_BGR2RGB))</span><br><span class="line">    plt.title(<span class="string">"Final Detections after applying NMS"</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="效果演示"><a href="#效果演示" class="headerlink" title="效果演示"></a>效果演示</h2><p>非极大值抑制处理前：</p>
<p><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-27/45995282.jpg" alt=""><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-27/13402395.jpg" alt=""><br></center><br>非极大值抑制处理后：</p>
<p><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-27/43202553.jpg" alt=""><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-5-27/41627345.jpg" alt=""><br></center><br>github地址：<a href="https://github.com/BUPTLdy/object-detector" target="_blank" rel="external">object-detector</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Compute the HOG descriptor by skimage ]]></title>
      <url>http://buptldy.github.io/2016/03/31/2016-03-31-Skimage%20hog/</url>
      <content type="html"><![CDATA[<h2 id="HOG简介"><a href="#HOG简介" class="headerlink" title="HOG简介"></a>HOG简介</h2><p>方向梯度直方图（英语：Histogram of oriented gradient，简称HOG）是应用在计算机视觉和图像处理领域，用于目标检测的特征描述器。这项技术是用来计算局部图像梯度的方向信息的统计值。这种方法跟边缘方向直方图（edge orientation histograms）、尺度不变特征变换（scale-invariant feature transform descriptors）以及形状上下文方法（ shape contexts）有很多相似之处，但与它们的不同点是：HOG描述器是在一个网格密集的大小统一的细胞单元（dense grid of uniformly spaced cells）上计算，而且为了提高性能，还采用了重叠的局部对比度归一化（overlapping local contrast normalization）技术。</p>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-3-31/40452258.jpg" alt=""><br></center>


<a id="more"></a>
<h2 id="函数形式"><a href="#函数形式" class="headerlink" title="函数形式"></a>函数形式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">skimage.feature.hog(image, orientations=<span class="number">9</span>, pixels_per_cell=(<span class="number">8</span>, <span class="number">8</span>), cells_per_block=(<span class="number">3</span>, <span class="number">3</span>), visualise=<span class="keyword">False</span>, transform_sqrt=<span class="keyword">False</span>, feature_vector=<span class="keyword">True</span>, normalise=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<h2 id="hog函数实现的主要步骤"><a href="#hog函数实现的主要步骤" class="headerlink" title="hog函数实现的主要步骤"></a>hog函数实现的主要步骤</h2><ul>
<li>图像归一化（可选）</li>
<li>计算x和y方向的梯度，包括大小和方向</li>
<li>计算梯度柱状图</li>
<li>对块状区域进行归一化处理</li>
<li>得到一个一维的特征向量</li>
</ul>
<p>具体有关hog特征计算流程可参考：<a href="http://buptldy.github.io/2016/03/31/2016-03-31-HOG%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/">《Histograms of Oriented Gradients for Human Detection》论文笔记</a></p>
<h2 id="hog函数参数解释"><a href="#hog函数参数解释" class="headerlink" title="hog函数参数解释"></a>hog函数参数解释</h2><h3 id="传入参数"><a href="#传入参数" class="headerlink" title="传入参数"></a>传入参数</h3><p>image : (M, N) ndarray</p>
<p>传入要进行hog特征计算的灰度图</p>
<p>orientations : int</p>
<p>设置方向梯度直方图的箱子个数</p>
<p>pixels_per_cell : 2 tuple (int, int)</p>
<p>设置每个单元的像素</p>
<p>cells_per_block : 2 tuple (int,int)</p>
<p>设置每个区块的单元数</p>
<p>visualise : bool, optional</p>
<p>设置是否返回可视化的hog特征</p>
<p>transform_sqrt : bool, optional</p>
<p>Apply power law compression to normalise the image before processing. DO NOT use this if the image contains negative values. Also see notes section below.<br>feature_vector : bool, optional</p>
<p>Return the data as a feature vector by calling .ravel() on the result just before returning.<br>normalise : bool, deprecated</p>
<p>The parameter is deprecated. Use transform_sqrt for power law compression. normalise has been deprecated.</p>
<h3 id="返回参数"><a href="#返回参数" class="headerlink" title="返回参数"></a>返回参数</h3><p>newarr : ndarray</p>
<p>返回得到的一维hog特征</p>
<p>hog_image : ndarray (if visualise=True)</p>
<p>hog特征的可视化图像</p>
<h2 id="hog函数举例"><a href="#hog函数举例" class="headerlink" title="hog函数举例"></a>hog函数举例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> skimage.feature <span class="keyword">import</span> hog</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> data, color, exposure</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">image = color.rgb2gray(data.astronaut())</span><br><span class="line"></span><br><span class="line">fd, hog_image = hog(image, orientations=<span class="number">8</span>, pixels_per_cell=(<span class="number">16</span>, <span class="number">16</span>),</span><br><span class="line">                    cells_per_block=(<span class="number">1</span>, <span class="number">1</span>), visualise=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">fig, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">8</span>, <span class="number">4</span>), sharex=<span class="keyword">True</span>, sharey=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">ax1.axis(<span class="string">'off'</span>)</span><br><span class="line">ax1.imshow(image, cmap=plt.cm.gray)</span><br><span class="line">ax1.set_title(<span class="string">'Input image'</span>)</span><br><span class="line">ax1.set_adjustable(<span class="string">'box-forced'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Rescale histogram for better display</span></span><br><span class="line">hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(<span class="number">0</span>, <span class="number">0.02</span>))</span><br><span class="line"></span><br><span class="line">ax2.axis(<span class="string">'off'</span>)</span><br><span class="line">ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)</span><br><span class="line">ax2.set_title(<span class="string">'Histogram of Oriented Gradients'</span>)</span><br><span class="line">ax1.set_adjustable(<span class="string">'box-forced'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-3-31/40452258.jpg" alt=""><br></center>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[《Histograms of Oriented Gradients for Human Detection》Note]]></title>
      <url>http://buptldy.github.io/2016/03/31/2016-03-31-HOG%20Note/</url>
      <content type="html"><![CDATA[<h1 id="HOG算法概述"><a href="#HOG算法概述" class="headerlink" title="HOG算法概述"></a>HOG算法概述</h1><p>局部目标的外表和形状可以被局部梯度或边缘方向的分布很好的描述,即使我们不知道对应的梯度和边缘的位置。在实际操作中,将图像分为小的细胞单元 (cells) ,每个细胞单元计算一个梯度方向 ( 或边缘方向 )直方图。为了对光照和阴影有更好的不变性,需要对直方图进行对比度归一化,可以通过将细胞单元组成更大的块 (blocks) 并归一化块内的所有细胞单元来实现。我们将归一化的块描述符叫做 HOG 描述子。将检测窗口中的所有块的 HOG 描述子组合起来就形成了最终的特征向量,然后使用 SVM 分类器进行人体检测,见下图。<br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-3-31/19766259.jpg" alt=""><br><a id="more"></a><br>HOG特征有个优点,它们提取的边缘和梯度特征能很好的抓住局部形状的特点,并且由于是       在局部进行提取,所以对几何和光学变化都有很好的不变性:变换或旋转对于足够小的区域影响很小。对于人体检测,在粗糙的空域采样 (coarse spatial sampling) 、精细的方向采样 (fine orientationsampling)和较强的局部光学归一化 (stronglocal photometric normalization) 这些条件下,只要行人大体上能够保持直立的姿势,就容许有一些细微的肢体动作,这些细微的动作可以被忽略而不影响检测效果。</p>
<h1 id="算法细节"><a href="#算法细节" class="headerlink" title="算法细节"></a>算法细节</h1><h2 id="伽马颜色归一化"><a href="#伽马颜色归一化" class="headerlink" title="伽马颜色归一化"></a>伽马颜色归一化</h2><p>用不同的幂值 (gamma 参数 ) 评价了几种颜色空间,有灰度空间、 RGB 、 LAB ,结果表明,这些规范化对结果影响很小,可能是由于随后的描述子归一化能达到相似的效果。</p>
<h2 id="梯度计算"><a href="#梯度计算" class="headerlink" title="梯度计算"></a>梯度计算</h2><p>不同的梯度计算方法对检测器性能有很大影响,但事实证明最简单的梯度算子结果是最好的。采用了最简单的一维离散微分模板算子。测试表明，使用 Sobel 算子等其它算子或是引入高斯平滑反而会造成性能降低。对于带颜色的图像,分别计算每个颜色通道的梯度,以范数最大者作为该点的梯度向量。</p>
<h2 id="空间-方向-bin-统计"><a href="#空间-方向-bin-统计" class="headerlink" title="空间 / 方向 bin 统计"></a>空间 / 方向 bin 统计</h2><p>如下图的一个包含行人的图像，红色框标记一个 8 × 8 单元，这些 8 × 8 的单元将被用来计算 HOG 描述符。</p>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-3-31/94180844.jpg" alt=""><br></center><br>在每个单元中，我们在每个像素上计算梯度矢量，将得到 64 个梯度矢量，梯度矢量相角在 0◦ → 180◦ 之间分布，我们对相角进行分箱 (bin)，每箱 20◦，一共 9 箱 (Dalal 和 Triggs 得到的最佳参数)。具有某一相角的梯度矢量的幅度按照权重分配给直方图。这涉及到权重投票表决机制， Dalal 和 Triggs 发现，采用梯度幅度进行分配表现最佳。例如，一个具有 85 度相角的梯度矢量将其幅度的 1/4 分配给中心为 70◦ 的箱，将剩余的 3/4 幅度分配给中心为 90◦ 的箱。这样就得到了下面的方向梯度直方图。<br><br><center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-3-31/13507779.jpg" alt=""><br></center>

<p>上面分配幅度的方法可以减少恰好位于两箱边界的梯度矢量的影响，否则，如果一个强梯度矢量恰好在边界上，其相角的一个很小的绕动都将对直方图造成非常大的影响。同时，在计算出梯度后进行高斯平滑，也可以缓解这种影响。另一方面，特征的复杂程度对分类器的影响很大。通过直方图的构造，我们将特征 64 个二元矢量量化为特征 9 个值，很好地压缩了特征的同时保留了单元的信息。设想对图像加上一些失真，方向梯度直方图的变化也不会很大，这是 HOG 特征的优点。</p>
<h2 id="归一化处理"><a href="#归一化处理" class="headerlink" title="归一化处理"></a>归一化处理</h2><p>前面提到，对图像所有像素进行加减后梯度矢量不变，接下来引入梯度矢量的标准化，使得其在像素值进行乘法运算后仍然保持不变。如果对单元内的像素值都乘以某一常数，梯度矢量的幅度明显会发生变化，幅度会增加常数因子,相角保持不变，这会造成整个直方图的每个箱的幅度增加常数因子。为了解决这个问题，需要引入梯度矢量标准化，一种简单的标准化方法是将梯度矢量除以其幅度，梯度矢量的幅度将保持 1，但是其相角不会发生变化。引入梯度矢量标准化以后，直方图各箱幅度在图像像素值整体乘以某个因子 (变化对比度) 时不会发生变化。除了对每个单元的直方图进行标准化外，另外一种方法是将固定数量的空域邻接的单元封装成区块，然后在区块上进行标准化。 Dalal 和 Triggs 使用 2 × 2 区块 (50% 重叠)，即 16 × 16 像素。将一个区块内的四个单元的直方图信息整合为 36 个值的特征 (9 × 4), 然后对这个 36 元矢量进行标准化。 Dalal 和 Triggs 考察了四种不同的区块标准化算法，设 v 为未标准化的区块梯度矢量， $||v||_{k}(k = 1, 2)$ 是 v 的 k-范数 (norm),e 是一个很小的常数 (具体值并不重要)，四种标准化算法如下：</p>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-3-31/98414203.jpg" alt=""><br></center>

<p>L2-Hys 是在 L2-norm 后进行截断，然后重新进行标准化。 Dalal 和 Triggs 发现 L2-Hys,L2-norm,L1-sqrt 性能相似，L1-norm 性能稍有下降，但都相对于未标准化的梯度矢量有明显的性能提升。区块重叠的影响是使得每个单元会在最终得到的 HOG 描述符中其作用的次数大于 1 次 (角单元出现 1 次，边单元出现 2 次，其它单元出现 4 次)，但每次出现都在不同的区块进行标准化。定义一个区块位移的步长为 8 像素，则可以实现 50% 的重叠。如果检测器窗口为 64x128像素，则会被分为 7 × 15 区块，每个区块包括 2 × 2 个单元，每个单元包括 8 × 8 像素，每个区块进行 9 箱直方图统计 (36 值)，最后的总特征矢量将有 7 × 15 × 4 × 9 = 3780 个特征值元素。</p>
<h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><p>分类器获取了 HOG 特征描述符之后，需要将其递交给监督学习分类器。 Dalal 和 Triggs 使用 SV M light 软件包配合HOG 描述符进行人体检测。 SVM 分类器寻找一个最佳超平面用作决策函数以实现二元分类，其特点是能够同时最小化经验误差与最大化几何边缘区。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://oncemore2020.github.io/blog/hog-using-opencv/" target="_blank" rel="external">HOG/linSVM检测器</a><br><a href="http://blog.csdn.net/masibuaa/article/details/14056807" target="_blank" rel="external">用于人体检测的方向梯度直方图</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[10 Minutes to pandas]]></title>
      <url>http://buptldy.github.io/2016/03/25/2016-03-25-10%20Minutes%20to%20Pandas/</url>
      <content type="html"><![CDATA[<h1 id="10分钟简单介绍pandas"><a href="#10分钟简单介绍pandas" class="headerlink" title="10分钟简单介绍pandas"></a>10分钟简单介绍pandas</h1><p>首先，导入模块如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<h1 id="pandas数据结构：Series"><a href="#pandas数据结构：Series" class="headerlink" title="pandas数据结构：Series"></a>pandas数据结构：Series</h1><a id="more"></a>
<p>Series可以简单地被认为是一维的数组。 Series 和一维数组最主要的区别在于 Series类型具有索引( index ),可以和另一个编程中常见的数据结构哈希( Hash )联系起来。</p>
<p>创建Series类型数据结构，如果没有传入索引，pandas默认的索引为从0开始的整数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s = pd.Series([<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,np.nan,<span class="number">6</span>,<span class="number">8</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s</span><br></pre></td></tr></table></figure>
<pre><code>0     1
1     3
2     5
3   NaN
4     6
5     8
dtype: float64
</code></pre><h1 id="pandas数据结构：DataFrame"><a href="#pandas数据结构：DataFrame" class="headerlink" title="pandas数据结构：DataFrame"></a>pandas数据结构：DataFrame</h1><p>DataFrame 是将数个 Series 按列合并而成的二维数据结构,每一列单独取出来是一个 Series ,这和 SQL 数据库中取出的数据是很类似的。所以,按<br>列对一个 DataFrame 进行处理更为方便,用户在编程时注意培养按列构建数据的思维。 DataFrame 的优势在于可以方便地处理不同类型的列,因此,就不要考虑如何对一个全是浮点数的 DataFrame 求逆之类的问题了,处理这种问题还是把数据存成 NumPy 的 matrix 类型比较便利一些。</p>
<p>通过传入 numpy array数据创建 DataFrame：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dates = pd.date_range(<span class="string">'20130101'</span>, periods=<span class="number">6</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dates</span><br></pre></td></tr></table></figure>
<pre><code>DatetimeIndex([&apos;2013-01-01&apos;, &apos;2013-01-02&apos;, &apos;2013-01-03&apos;, &apos;2013-01-04&apos;,
               &apos;2013-01-05&apos;, &apos;2013-01-06&apos;],
              dtype=&apos;datetime64[ns]&apos;, freq=&apos;D&apos;)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(np.random.randn(<span class="number">6</span>,<span class="number">4</span>), index=dates, columns=list(<span class="string">'ABCD'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">   <thead>     <tr style="text-align: right;">       <th></th>       <th>A</th>       <th>B</th>       <th>C</th>       <th>D</th>     </tr>   </thead>   <tbody>     <tr>       <th>2013-01-01</th>       <td>0.212880</td>       <td>0.351725</td>       <td>-1.350579</td>       <td>-0.107403</td>     </tr>     <tr>       <th>2013-01-02</th>       <td>-0.857903</td>       <td>-1.783324</td>       <td>1.162888</td>       <td>-0.488226</td>     </tr>     <tr>       <th>2013-01-03</th>       <td>-0.245746</td>       <td>-0.226585</td>       <td>1.749624</td>       <td>1.140817</td>     </tr>     <tr>       <th>2013-01-04</th>       <td>0.032400</td>       <td>-0.264382</td>       <td>0.125095</td>       <td>-1.322739</td>     </tr>     <tr>       <th>2013-01-05</th>       <td>-2.260707</td>       <td>0.064878</td>       <td>0.231025</td>       <td>0.682991</td>     </tr>     <tr>       <th>2013-01-06</th>       <td>0.603739</td>       <td>1.490709</td>       <td>0.249649</td>       <td>1.822501</td>     </tr>   </tbody> </table>  

<p>传入字典对象创建DataFrame：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df2 = pd.DataFrame(&#123; <span class="string">'A'</span> : <span class="number">1.</span>,</span><br><span class="line">....:                <span class="string">'B'</span> : pd.Timestamp(<span class="string">'20130102'</span>),</span><br><span class="line">....:                <span class="string">'C'</span> : pd.Series(<span class="number">1</span>,index=list(range(<span class="number">4</span>)),dtype=<span class="string">'float32'</span>),</span><br><span class="line">....:                <span class="string">'D'</span> : np.array([<span class="number">3</span>] * <span class="number">4</span>,dtype=<span class="string">'int32'</span>),</span><br><span class="line">....:                <span class="string">'E'</span> : pd.Categorical([<span class="string">"test"</span>,<span class="string">"train"</span>,<span class="string">"test"</span>,<span class="string">"train"</span>]),</span><br><span class="line">....:                <span class="string">'F'</span> : <span class="string">'foo'</span> &#125;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df2</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">   <thead>     <tr style="text-align: right;">       <th></th>       <th>A</th>       <th>B</th>       <th>C</th>       <th>D</th>       <th>E</th>       <th>F</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>1</td>       <td>2013-01-02</td>       <td>1</td>       <td>3</td>       <td>test</td>       <td>foo</td>     </tr>     <tr>       <th>1</th>       <td>1</td>       <td>2013-01-02</td>       <td>1</td>       <td>3</td>       <td>train</td>       <td>foo</td>     </tr>     <tr>       <th>2</th>       <td>1</td>       <td>2013-01-02</td>       <td>1</td>       <td>3</td>       <td>test</td>       <td>foo</td>     </tr>     <tr>       <th>3</th>       <td>1</td>       <td>2013-01-02</td>       <td>1</td>       <td>3</td>       <td>train</td>       <td>foo</td>     </tr>   </tbody> </table>    

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df2.F</span><br></pre></td></tr></table></figure>
<pre><code>0    foo
1    foo
2    foo
3    foo
Name: F, dtype: object
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df2.A</span><br></pre></td></tr></table></figure>
<pre><code>0    1
1    1
2    1
3    1
Name: A, dtype: float64
</code></pre><p>查看数据顶部或底部的几行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.head(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">   <thead>     <tr style="text-align: right;">       <th></th>       <th>A</th>       <th>B</th>       <th>C</th>       <th>D</th>     </tr>   </thead>   <tbody>     <tr>       <th>2013-01-01</th>       <td>0.212880</td>       <td>0.351725</td>       <td>-1.350579</td>       <td>-0.107403</td>     </tr>     <tr>       <th>2013-01-02</th>       <td>-0.857903</td>       <td>-1.783324</td>       <td>1.162888</td>       <td>-0.488226</td>     </tr>   </tbody> </table>  



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.tail(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">   <thead>     <tr style="text-align: right;">       <th></th>       <th>A</th>       <th>B</th>       <th>C</th>       <th>D</th>     </tr>   </thead>   <tbody>     <tr>       <th>2013-01-04</th>       <td>0.032400</td>       <td>-0.264382</td>       <td>0.125095</td>       <td>-1.322739</td>     </tr>     <tr>       <th>2013-01-05</th>       <td>-2.260707</td>       <td>0.064878</td>       <td>0.231025</td>       <td>0.682991</td>     </tr>     <tr>       <th>2013-01-06</th>       <td>0.603739</td>       <td>1.490709</td>       <td>0.249649</td>       <td>1.822501</td>     </tr>   </tbody> </table>   

<p>显示行列索引和里面的值;</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.index</span><br></pre></td></tr></table></figure>
<pre><code>DatetimeIndex([&apos;2013-01-01&apos;, &apos;2013-01-02&apos;, &apos;2013-01-03&apos;, &apos;2013-01-04&apos;,
               &apos;2013-01-05&apos;, &apos;2013-01-06&apos;],
              dtype=&apos;datetime64[ns]&apos;, freq=&apos;D&apos;)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.columns</span><br></pre></td></tr></table></figure>
<pre><code>Index([u&apos;A&apos;, u&apos;B&apos;, u&apos;C&apos;, u&apos;D&apos;], dtype=&apos;object&apos;)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.values</span><br></pre></td></tr></table></figure>
<pre><code>array([[ 0.21287973,  0.35172526, -1.35057903, -0.10740265],
       [-0.85790301, -1.78332415,  1.16288782, -0.48822551],
       [-0.24574644, -0.22658458,  1.74962416,  1.14081656],
       [ 0.03240016, -0.26438175,  0.12509531, -1.32273918],
       [-2.26070679,  0.06487812,  0.23102475,  0.68299111],
       [ 0.60373902,  1.4907093 ,  0.24964875,  1.82250141]])
</code></pre><p>显示数据的简单统计：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.describe()</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">   <thead>     <tr style="text-align: right;">       <th></th>       <th>A</th>       <th>B</th>       <th>C</th>       <th>D</th>     </tr>   </thead>   <tbody>     <tr>       <th>count</th>       <td>6.000000</td>       <td>6.000000</td>       <td>6.000000</td>       <td>6.000000</td>     </tr>     <tr>       <th>mean</th>       <td>-0.419223</td>       <td>-0.061163</td>       <td>0.361284</td>       <td>0.287990</td>     </tr>     <tr>       <th>std</th>       <td>1.026018</td>       <td>1.061053</td>       <td>1.056953</td>       <td>1.148160</td>     </tr>     <tr>       <th>min</th>       <td>-2.260707</td>       <td>-1.783324</td>       <td>-1.350579</td>       <td>-1.322739</td>     </tr>     <tr>       <th>25%</th>       <td>-0.704864</td>       <td>-0.254932</td>       <td>0.151578</td>       <td>-0.393020</td>     </tr>     <tr>       <th>50%</th>       <td>-0.106673</td>       <td>-0.080853</td>       <td>0.240337</td>       <td>0.287794</td>     </tr>     <tr>       <th>75%</th>       <td>0.167760</td>       <td>0.280013</td>       <td>0.934578</td>       <td>1.026360</td>     </tr>     <tr>       <th>max</th>       <td>0.603739</td>       <td>1.490709</td>       <td>1.749624</td>       <td>1.822501</td>     </tr>   </tbody> </table>   

<p>数据转置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.T</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">   <thead>     <tr style="text-align: right;">       <th></th>       <th>2013-01-01 00:00:00</th>       <th>2013-01-02 00:00:00</th>       <th>2013-01-03 00:00:00</th>       <th>2013-01-04 00:00:00</th>       <th>2013-01-05 00:00:00</th>       <th>2013-01-06 00:00:00</th>     </tr>   </thead>   <tbody>     <tr>       <th>A</th>       <td>0.212880</td>       <td>-0.857903</td>       <td>-0.245746</td>       <td>0.032400</td>       <td>-2.260707</td>       <td>0.603739</td>     </tr>     <tr>       <th>B</th>       <td>0.351725</td>       <td>-1.783324</td>       <td>-0.226585</td>       <td>-0.264382</td>       <td>0.064878</td>       <td>1.490709</td>     </tr>     <tr>       <th>C</th>       <td>-1.350579</td>       <td>1.162888</td>       <td>1.749624</td>       <td>0.125095</td>       <td>0.231025</td>       <td>0.249649</td>     </tr>     <tr>       <th>D</th>       <td>-0.107403</td>       <td>-0.488226</td>       <td>1.140817</td>       <td>-1.322739</td>       <td>0.682991</td>       <td>1.822501</td>     </tr>   </tbody> </table>



<p>按某个索引排序：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.sort_index(axis=<span class="number">1</span>,ascending=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">   <thead>     <tr style="text-align: right;">       <th></th>       <th>D</th>       <th>C</th>       <th>B</th>       <th>A</th>     </tr>   </thead>   <tbody>     <tr>       <th>2013-01-01</th>       <td>-0.107403</td>       <td>-1.350579</td>       <td>0.351725</td>       <td>0.212880</td>     </tr>     <tr>       <th>2013-01-02</th>       <td>-0.488226</td>       <td>1.162888</td>       <td>-1.783324</td>       <td>-0.857903</td>     </tr>     <tr>       <th>2013-01-03</th>       <td>1.140817</td>       <td>1.749624</td>       <td>-0.226585</td>       <td>-0.245746</td>     </tr>     <tr>       <th>2013-01-04</th>       <td>-1.322739</td>       <td>0.125095</td>       <td>-0.264382</td>       <td>0.032400</td>     </tr>     <tr>       <th>2013-01-05</th>       <td>0.682991</td>       <td>0.231025</td>       <td>0.064878</td>       <td>-2.260707</td>     </tr>     <tr>       <th>2013-01-06</th>       <td>1.822501</td>       <td>0.249649</td>       <td>1.490709</td>       <td>0.603739</td>     </tr>   </tbody> </table>  


<p>按数据的值排序：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.sort_values(by=<span class="string">'B'</span>)</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">   <thead>     <tr style="text-align: right;">       <th></th>       <th>A</th>       <th>B</th>       <th>C</th>       <th>D</th>     </tr>   </thead>   <tbody>     <tr>       <th>2013-01-02</th>       <td>-0.857903</td>       <td>-1.783324</td>       <td>1.162888</td>       <td>-0.488226</td>     </tr>     <tr>       <th>2013-01-04</th>       <td>0.032400</td>       <td>-0.264382</td>       <td>0.125095</td>       <td>-1.322739</td>     </tr>     <tr>       <th>2013-01-03</th>       <td>-0.245746</td>       <td>-0.226585</td>       <td>1.749624</td>       <td>1.140817</td>     </tr>     <tr>       <th>2013-01-05</th>       <td>-2.260707</td>       <td>0.064878</td>       <td>0.231025</td>       <td>0.682991</td>     </tr>     <tr>       <th>2013-01-01</th>       <td>0.212880</td>       <td>0.351725</td>       <td>-1.350579</td>       <td>-0.107403</td>     </tr>     <tr>       <th>2013-01-06</th>       <td>0.603739</td>       <td>1.490709</td>       <td>0.249649</td>       <td>1.822501</td>     </tr>   </tbody> </table>



<p>选出某一类：(同df.A)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'A'</span>]</span><br></pre></td></tr></table></figure>
<pre><code>2013-01-01    0.212880
2013-01-02   -0.857903
2013-01-03   -0.245746
2013-01-04    0.032400
2013-01-05   -2.260707
2013-01-06    0.603739
Freq: D, Name: A, dtype: float64
</code></pre><p>通过[]切分出几行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="number">0</span>:<span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">   <thead>     <tr style="text-align: right;">       <th></th>       <th>A</th>       <th>B</th>       <th>C</th>       <th>D</th>     </tr>   </thead>   <tbody>     <tr>       <th>2013-01-01</th>       <td>0.212880</td>       <td>0.351725</td>       <td>-1.350579</td>       <td>-0.107403</td>     </tr>     <tr>       <th>2013-01-02</th>       <td>-0.857903</td>       <td>-1.783324</td>       <td>1.162888</td>       <td>-0.488226</td>     </tr>     <tr>       <th>2013-01-03</th>       <td>-0.245746</td>       <td>-0.226585</td>       <td>1.749624</td>       <td>1.140817</td>     </tr>   </tbody> </table>




<pre><code>df[&apos;20130102&apos;:&apos;20130104&apos;]
</code></pre><table border="1" class="dataframe">   <thead>     <tr style="text-align: right;">       <th></th>       <th>A</th>       <th>B</th>       <th>C</th>       <th>D</th>     </tr>   </thead>   <tbody>     <tr>       <th>2013-01-02</th>       <td>-0.857903</td>       <td>-1.783324</td>       <td>1.162888</td>       <td>-0.488226</td>     </tr>     <tr>       <th>2013-01-03</th>       <td>-0.245746</td>       <td>-0.226585</td>       <td>1.749624</td>       <td>1.140817</td>     </tr>     <tr>       <th>2013-01-04</th>       <td>0.032400</td>       <td>-0.264382</td>       <td>0.125095</td>       <td>-1.322739</td>     </tr>   </tbody> </table>




<p>通过标签选择：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.loc[dates[<span class="number">0</span>],[<span class="string">'A'</span>,<span class="string">'B'</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A    <span class="number">0.212880</span></span><br><span class="line">B    <span class="number">0.351725</span></span><br><span class="line">Name: <span class="number">2013</span><span class="number">-01</span><span class="number">-01</span> <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>, dtype: float64</span><br></pre></td></tr></table></figure>
<p>通过位置选取：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.iloc[<span class="number">1</span>:<span class="number">3</span>,<span class="number">0</span>:<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">   <thead>     <tr style="text-align: right;">       <th></th>       <th>A</th>       <th>B</th>     </tr>   </thead>   <tbody>     <tr>       <th>2013-01-02</th>       <td>-0.857903</td>       <td>-1.783324</td>     </tr>     <tr>       <th>2013-01-03</th>       <td>-0.245746</td>       <td>-0.226585</td>     </tr>   </tbody> </table>  


<p>reindex方法，能够增加行和列：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df1 = df.reindex(index=dates[<span class="number">0</span>:<span class="number">4</span>], columns=list(df.columns) + [<span class="string">'E'</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df1.loc[dates[<span class="number">0</span>]:dates[<span class="number">1</span>],<span class="string">'E'</span>] = <span class="number">1</span></span><br><span class="line">df1</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">   <thead>     <tr style="text-align: right;">       <th></th>       <th>A</th>       <th>B</th>       <th>C</th>       <th>D</th>       <th>E</th>     </tr>   </thead>   <tbody>     <tr>       <th>2013-01-01</th>       <td>0.212880</td>       <td>0.351725</td>       <td>-1.350579</td>       <td>-0.107403</td>       <td>1</td>     </tr>     <tr>       <th>2013-01-02</th>       <td>-0.857903</td>       <td>-1.783324</td>       <td>1.162888</td>       <td>-0.488226</td>       <td>1</td>     </tr>     <tr>       <th>2013-01-03</th>       <td>-0.245746</td>       <td>-0.226585</td>       <td>1.749624</td>       <td>1.140817</td>       <td>NaN</td>     </tr>     <tr>       <th>2013-01-04</th>       <td>0.032400</td>       <td>-0.264382</td>       <td>0.125095</td>       <td>-1.322739</td>       <td>NaN</td>     </tr>   </tbody> </table>



<p>丢失数据的处理:</p>
<p>去掉有丢失数据的所有行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df1.dropna(how=<span class="string">'any'</span>)</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">   <thead>     <tr style="text-align: right;">       <th></th>       <th>A</th>       <th>B</th>       <th>C</th>       <th>D</th>       <th>E</th>     </tr>   </thead>   <tbody>     <tr>       <th>2013-01-01</th>       <td>0.212880</td>       <td>0.351725</td>       <td>-1.350579</td>       <td>-0.107403</td>       <td>1</td>     </tr>     <tr>       <th>2013-01-02</th>       <td>-0.857903</td>       <td>-1.783324</td>       <td>1.162888</td>       <td>-0.488226</td>       <td>1</td>     </tr>   </tbody> </table>  


<p>填充丢失数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df1.fillna(value=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">   <thead>     <tr style="text-align: right;">       <th></th>       <th>A</th>       <th>B</th>       <th>C</th>       <th>D</th>       <th>E</th>     </tr>   </thead>   <tbody>     <tr>       <th>2013-01-01</th>       <td>0.212880</td>       <td>0.351725</td>       <td>-1.350579</td>       <td>-0.107403</td>       <td>1</td>     </tr>     <tr>       <th>2013-01-02</th>       <td>-0.857903</td>       <td>-1.783324</td>       <td>1.162888</td>       <td>-0.488226</td>       <td>1</td>     </tr>     <tr>       <th>2013-01-03</th>       <td>-0.245746</td>       <td>-0.226585</td>       <td>1.749624</td>       <td>1.140817</td>       <td>5</td>     </tr>     <tr>       <th>2013-01-04</th>       <td>0.032400</td>       <td>-0.264382</td>       <td>0.125095</td>       <td>-1.322739</td>       <td>5</td>     </tr>   </tbody> </table>  


<p>判断是否有丢失数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.isnull(df1)</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">   <thead>     <tr style="text-align: right;">       <th></th>       <th>A</th>       <th>B</th>       <th>C</th>       <th>D</th>       <th>E</th>     </tr>   </thead>   <tbody>     <tr>       <th>2013-01-01</th>       <td>False</td>       <td>False</td>       <td>False</td>       <td>False</td>       <td>False</td>     </tr>     <tr>       <th>2013-01-02</th>       <td>False</td>       <td>False</td>       <td>False</td>       <td>False</td>       <td>False</td>     </tr>     <tr>       <th>2013-01-03</th>       <td>False</td>       <td>False</td>       <td>False</td>       <td>False</td>       <td>True</td>     </tr>     <tr>       <th>2013-01-04</th>       <td>False</td>       <td>False</td>       <td>False</td>       <td>False</td>       <td>True</td>     </tr>   </tbody> </table>

<h1 id="读取文件"><a href="#读取文件" class="headerlink" title="读取文件"></a>读取文件</h1><p>写csv文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.to_csv(<span class="string">'foo.csv'</span>)</span><br></pre></td></tr></table></figure>
<p>读csv文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.read_csv(<span class="string">'foo.csv'</span>)</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">   <thead>     <tr style="text-align: right;">       <th></th>       <th>Unnamed: 0</th>       <th>A</th>       <th>B</th>       <th>C</th>       <th>D</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>2013-01-01</td>       <td>0.212880</td>       <td>0.351725</td>       <td>-1.350579</td>       <td>-0.107403</td>     </tr>     <tr>       <th>1</th>       <td>2013-01-02</td>       <td>-0.857903</td>       <td>-1.783324</td>       <td>1.162888</td>       <td>-0.488226</td>     </tr>     <tr>       <th>2</th>       <td>2013-01-03</td>       <td>-0.245746</td>       <td>-0.226585</td>       <td>1.749624</td>       <td>1.140817</td>     </tr>     <tr>       <th>3</th>       <td>2013-01-04</td>       <td>0.032400</td>       <td>-0.264382</td>       <td>0.125095</td>       <td>-1.322739</td>     </tr>     <tr>       <th>4</th>       <td>2013-01-05</td>       <td>-2.260707</td>       <td>0.064878</td>       <td>0.231025</td>       <td>0.682991</td>     </tr>     <tr>       <th>5</th>       <td>2013-01-06</td>       <td>0.603739</td>       <td>1.490709</td>       <td>0.249649</td>       <td>1.822501</td>     </tr>   </tbody> </table>  



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="http://pandas.pydata.org/pandas-docs/stable/10min.html" target="_blank" rel="external">10 Minutes to pandas¶
</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Git Cheat Sheet]]></title>
      <url>http://buptldy.github.io/2016/03/02/2016-03-02-Git%20Cheat%20Sheet/</url>
      <content type="html"><![CDATA[<ol>
<li><p>创建版本库</p>
<p> 初始化一个Git仓库，使用git init命令。</p>
<p> 添加文件到Git仓库，分两步：</p>
<p> 第一步，使用命令git add <file>，注意，可反复多次使用，添加多个文件；</file></p>
<a id="more"></a>
<p> 第二步，使用命令git commit，完成。</p>
<p> 要随时掌握工作区的状态，使用git status命令。</p>
<p> 如果git status告诉你有文件被修改过，用git diff可以查看修改内容。</p>
</li>
<li><p>版本回退</p>
<p> HEAD指向的版本就是当前版本，因此，Git允许我们在版本的历史之间穿梭，使用命令git reset –hard commit_id。Git必须知道当前版本是哪个版本，在Git中，用HEAD表示当前版本，上一个版本就是HEAD^，上上一个版本就是HEAD^^，当然往上100个版本写100个^比较容易数不过来，所以写成HEAD~100。</p>
<p> 穿梭前，用git log可以查看提交历史，以便确定要回退到哪个版本。</p>
<p> 要重返未来，用git reflog查看命令历史，以便确定要回到未来的哪个版本。</p>
</li>
<li><p>工作区和暂存区</p>
<p> 工作区（Working Directory）：就是你在电脑里能看到的目录。</p>
<p> 版本库（Repository）：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。</p>
<p> Git的版本库里存了很多东西，其中最重要的就是称为stage（或者叫index）的暂存区，还有Git为我们自动创建的第一个分支master，以及指向master的一个指针叫HEAD。</p>
</li>
</ol>
<center><br><img src="http://7xritj.com1.z0.glb.clouddn.com/16-3-10/74272820.jpg" alt=""><br></center>

<pre><code>前面讲了我们把文件往Git版本库里添加的时候，是分两步执行的：

第一步是用git add把文件添加进去，实际上就是把文件修改添加到暂存区；

第二步是用git commit提交更改，实际上就是把暂存区的所有内容提交到当前分支。

因为我们创建Git版本库时，Git自动为我们创建了唯一一个master分支，所以，现在，git commit就是往master分支上提交更改。

你可以简单理解为，需要提交的文件修改通通放到暂存区，然后，一次性提交暂存区的所有修改。

Git是如何跟踪修改的:每次修改，如果不add到暂存区，那就不会加入到commit中。

场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout -- file。git checkout其实是用版本库里的版本替换工作区的版本，无论工作区是修改还是删除，都可以“一键还原”。

场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令git reset HEAD file，就回到了场景1，第二步按场景1操作。

场景3：已经提交了不合适的修改到版本库时，想要撤销本次提交，参考版本回退一节，不过前提是没有推送到远程库。
</code></pre><ol>
<li><p>远程仓库</p>
<p> 要关联一个远程库，使用命令git remote add origin git@server-name:path/repo-name.git；</p>
<p> 关联后，使用命令git push -u origin master第一次推送master分支的所有内容；</p>
<p> 此后，每次本地提交后，只要有必要，就可以使用命令git push origin master推送最新修改；</p>
</li>
<li><p>创建和合并分支</p>
<p> 查看分支：git branch</p>
<p> 创建分支：git branch <name></name></p>
<p> 切换分支：git checkout <name></name></p>
<p> 创建+切换分支：git checkout -b <name></name></p>
<p> 合并某分支到当前分支：git merge <name></name></p>
<p> 删除分支：git branch -d <name></name></p>
<p> 当Git无法自动合并分支时，就必须首先解决冲突。解决冲突后，再提交，合并完成。</p>
<p> 用git log –graph命令可以看到分支合并图。</p>
<p> 修复bug时，我们会通过创建新的bug分支进行修复，然后合并，最后删除；</p>
<p> 当手头工作没有完成时，先把工作现场git stash一下，然后去修复bug，修复后，再git stash pop，回到工作现场。</p>
<p> 开发一个新feature，最好新建一个分支；</p>
<p> 如果要丢弃一个没有被合并过的分支，可以通过git branch -D <name>强行删除。</name></p>
<p> 查看远程库信息，使用git remote -v；</p>
<p> 本地新建的分支如果不推送到远程，对其他人就是不可见的；</p>
<p> 从本地推送分支，使用git push origin branch-name，如果推送失败，先用git pull抓取远程的新提交；</p>
<p> 在本地创建和远程分支对应的分支，使用git checkout -b branch-name origin/branch-name，本地和远程分支的名称最好一致；</p>
<p> 建立本地分支和远程分支的关联，使用git branch –set-upstream branch-name origin/branch-name；</p>
<p> 从远程抓取分支，使用git pull，如果有冲突，要先处理冲突。</p>
<p> 命令git tag <name>用于新建一个标签，默认为HEAD，也可以指定一个commit id；</name></p>
</li>
<li><p>标签管理</p>
<p> git tag -a <tagname> -m “blablabla…”可以指定标签信息；</tagname></p>
<p> git tag -s <tagname> -m “blablabla…”可以用PGP签名标签；</tagname></p>
<p> 命令git tag可以查看所有标签。</p>
<p> 命令git push origin <tagname>可以推送一个本地标签；</tagname></p>
<p> 命令git push origin –tags可以推送全部未推送过的本地标签；</p>
<p> 命令git tag -d <tagname>可以删除一个本地标签；</tagname></p>
<p> 命令git push origin :refs/tags/<tagname>可以删除一个远程标签</tagname></p>
</li>
<li><p>Github使用</p>
<p> 在GitHub上，可以任意Fork开源仓库；</p>
<p> 自己拥有Fork后的仓库的读写权限；</p>
<p> 可以推送pull request给官方仓库来贡献代码。</p>
<p> 忽略某些文件时，需要编写.gitignore；</p>
<p> .gitignore文件本身要放到版本库里，并且可以对.gitignore做版本管理！</p>
</li>
</ol>
<p><strong>参考资料</strong></p>
<p><img src="http://7xritj.com1.z0.glb.clouddn.com/16-3-10/93928228.jpg" alt=""></p>
<p><a href="http://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="external">Git教程-廖雪峰的官方网站</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Approximation Algorithm]]></title>
      <url>http://buptldy.github.io/2016/01/12/2016-01-12-Approximation%20Algorithm/</url>
      <content type="html"><![CDATA[<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p>许多具有卖际意义的问题都是 <a href="http://buptldy.github.io/2016/01/11/NP%E5%AE%8C%E5%85%A8%E9%97%AE%E9%A2%98%E7%9A%84%E4%BB%8B%E7%BB%8D%E5%8F%8A%E8%AF%81%E6%98%8E/">NP 完全</a>问题。我们不知道如何在多项式时间内求得最优解。但是，这些问题通常又十分重要， 我们不能因此而放弃对它们的求解。即使一个问题是 NP 完全的，也有其求解方法。解决 NP 完全问题至少有三种方法：<br><a id="more"></a></p>
<ul>
<li>如果实际输入数据规模较小，则用指数级运行时的算法就能很好地解决问题；</li>
<li>对于一些能在多项式时间内解决的特殊情况，可以把它们单独列出来求解；</li>
<li>可以寻找一些能够在多项式时间内得到近似最优解 （near-optimal solution)的方法(最坏情况或平均情况)。</li>
</ul>
<p>在实际应用中，近似最优解一般都能满足要求， 返回近似最优解的算法就称为近似算法(approximation algorithm)。</p>
<p><strong>近似比：</strong></p>
<p>如果对规模为n的任意输人，近似算法所产生的近似解的代价C与最优解的代价C*只差一个因子$\rho (n)$:</p>
<p>$$max(\frac{C}{C*},\frac{C*}{C} )\leq \rho (n)$$</p>
<p>则称该近似算法有近似比$\rho (n)$。如果一个算法的近似比达到 $\rho (n)$，则称该算法为$\rho (n)$近似算法。近似比和$\rho (n)$近似算法的定义对求最大化和最小化问题都适用，一个近似算法的近似比不会小于1。</p>
<h1 id="一维装箱问题-Bin-Packing"><a href="#一维装箱问题-Bin-Packing" class="headerlink" title="一维装箱问题(Bin Packing)"></a>一维装箱问题(Bin Packing)</h1><p>问题如下：</p>
<p>Bin Packing is as follows: Given n items with sizes $a_1, \cdots , a_n ∈ (0, 1]$, find a packing in unit-sized bins that minimizes the number of bins used.</p>
<p>Give a 2-approximation algorithm for this problem and analysis the approximation factor.</p>
<p>装箱问题：有n个物品，每个物品的尺寸在0-1之间，每个箱子的容量为1，问最少要用多少的箱子能把所有的物品装下？</p>
<p>装箱问题可用整数规划描述如下，其中$y_i=1$表示箱子$i$被使用，否则表示没有使用，$x_{ij}=1$表示物品j放入箱子i中。</p>
<p><img src="http://i.imgur.com/VPewCSm.png" style="display:block;margin:auto"></p>
<p>其中约束条件(1)表示：一旦箱子i被使用，放入箱子i中的物品尺寸不能超过箱子的容量1。</p>
<p>约束条件(2)表示：每个物品刚好放入一个箱子中。</p>
<p>由<a href="http://buptldy.github.io/2016/01/11/NP%E5%AE%8C%E5%85%A8%E9%97%AE%E9%A2%98%E7%9A%84%E4%BB%8B%E7%BB%8D%E5%8F%8A%E8%AF%81%E6%98%8E/">前文</a>已知，整数线性规划问题是NP完全的，即不能找到多项式时间算法来求解，所以需要寻找一种近似算法。</p>
<blockquote>
<p>Next Fit算法：按顺序把物品放进当前箱子，如果放不下，则放下一个。</p>
</blockquote>
<p>举例：</p>
<table>
<thead>
<tr>
<th>物品</th>
<th>$J_1$</th>
<th>$J_2$</th>
<th>$J_3$</th>
<th>$J_4$</th>
<th>$J_5$</th>
<th>$J_6$</th>
</tr>
</thead>
<tbody>
<tr>
<td>尺寸</td>
<td>0.6</td>
<td>0.7</td>
<td>0.4</td>
<td>0.2</td>
<td>0.8</td>
<td>0.3</td>
</tr>
</tbody>
</table>
<p>根据Next Fit算法，解如下图所示：</p>
<center><br><img src="http://i.imgur.com/YDnGdKS.png" alt=""><br></center>


<blockquote>
<p>证明：Next Fit是Bin Packing问题近似比为2的近似算法</p>
</blockquote>
<ul>
<li><p>对所有的输入物品序列I有:$NF(I) \leq 2OPT(I)$</p>
<p>  如下图所示，任意考虑两个相邻的箱子，这两个箱子里面的物品的容量肯定要大于1，否则根据Next Fit算法会把这些物品放进第一个箱子，所以两个相邻箱子所占用的空间肯定是大于1的，即有$B_1+B_2&gt;1$，对于$B_3+B_4,\dots$都是这样。因此浪费的空间不达到一半，所以有$NF(I) \leq 2OPT(I)$。</p>
</li>
</ul>
<center><br>    <img src="http://i.imgur.com/A8GpRKX.png" alt=""><br></center>

<ul>
<li><p>存在一个输入物品序列I：$NF(I)\geq 2OPT(I)-2$</p>
<p>  考虑长度为n(n为4的倍数)的物品序列I，尺寸大小分别为：</p>
<p>  0.5，2/n，0.5，2/n，…，0.5，2/n</p>
<p>  则最佳装箱策略如下图所示，最少需要(n/4+1)个箱子。<br><center><br><img src="http://i.imgur.com/IPg1QX9.png" alt=""><br></center><br>  Next Fit策略如下图所示，需要(n/2)个箱子<br><center><br><img src="http://i.imgur.com/EkthQ3j.png" alt=""><br></center><br>  所以根据上述证明，Next Fit是Bin Packing问题近似比为2的近似算法。</p>
</li>
</ul>
<p><strong>复杂度分析</strong>：由于NF算法处理每个物品只检查一个箱子,所以其时间复杂度是线性的,但也正因为如此,使得前面箱子剩余空间再无利用的可能。该算法的时间复杂度是 $O(n)$ , 空间复杂度为 $O(1)$ 。</p>
<h1 id="Steiner-Tree-Problem"><a href="#Steiner-Tree-Problem" class="headerlink" title="Steiner Tree Problem"></a>Steiner Tree Problem</h1><p>问题如下：</p>
<p>Given an undirected graph G = (V, E) with edge costs and set T ⊆ V of required vertices, the Steiner Tree Problem is to find a minimum cost tree in G containing every vertex in T (vertices in V −T may or may not be used in T).</p>
<p>Give a 2-approximation algorithm if G is complete and the edge costs satisfy the triangle inequality.</p>
<p>所谓的Steine​​r tree problem是指在一无向图G(V,E)中, 给定一组V的子集合S, 我们要在其中找到一个minimum cost tree, 这个tree 必需包含S中所有的点, 另外也可包含一些非S中的点。这些非S的点我们称之为Steine​​r nodes, S中的点我们称之为terminals。</p>
<p>Steine​​r tree problem 是属于NP-complete 的间题, 代表着我们目前找不到一个算法, 能够在polynomial 的时间内解决这个问题。</p>
<p><strong>问题详述:</strong></p>
<center><br><img src="http://i.imgur.com/TF3lU8i.gif" alt=""><br></center><br>所谓的Steine​​r Tree Problem, 是一组无向图G(V,E)中, 给定一组terminals, 如图一的A和D, 然后我们必需在G上找到一个minimum spanning tree, 这个tree 必需满足下面要求<br><br>- 它必需span 所有的terminals<br>- 它可以包含非terminal 的点, 这些点称之为steine​​r node, 如图1的B, E, F<br>- 它的total cost必需为最小<br><br>在上图中我们可以知道, 如果不能包含非terminal 的点, 则找出来的spanning tree, cost为6, 而且有可能根本找不到这样的tree, 在包含了一些steine​​r node 之后, 所找出的cost为5。<br><strong>近似算法</strong><br><br>The Kou Markowsky and Berman algorithm<br><br>Input: a undirect graph G(V,E) and a subset S of V.<br><br>Output: The minimum cost Steine​​r tree T.<br><br>Step1:建构distance graph G1(S, E’). 对每一个E’中的edge (u, v),它的cost等于G中u到v的最短路径的cost<br><br>Step2: 找出minimum spaning tree T1 of G1<br><br>Step3: 建构G2(V’’, E’’), 将T1的每一个edge (u, v), 用它在Step1中所找的路径代入.<br><br>Step4: 将G2中的cycle去掉.<br><br><center><br>  <img src="http://i.imgur.com/CohPHo2.png" alt=""><br></center>



<p>如上图所示, 首先我们先建立一个包含所有terminal 的complete distance graph G1, 然后找出它的minimum spanning tree T1, 然后将原路径代回, 得到G2, 最后将G2 的cycle移去, 得到total cost 为14 的Steine​​r Tree T. 因为此一近似算法为approximate algorithm, 所以它得到的steine​​r tree并一定都是optimum, 此例子的minimum Steine​​r tree的cost为13。</p>
<p>复杂度和近似比：因为需要计算最短路径，所以时间复杂度为$O(M*N^2)$,其中 $|V|=N$ , $|S|=M$。</p>
<p>近似比为2，证明可以参考：<a href="http://www.csie.ntu.edu.tw/~kmchao/tree10fall/Steiner.pdf" title="http://www.csie.ntu.edu.tw/~kmchao/tree10fall/Steiner.pdf" target="_blank" rel="external">http://www.csie.ntu.edu.tw/~kmchao/tree10fall/Steiner.pdf</a></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>演算法设计与分析Term Project：<a href="http://par.cse.nsysu.edu.tw/~homework/algo01/9034811/Report/index.htm" title="演算法设计与分析Term Project" target="_blank" rel="external">http://par.cse.nsysu.edu.tw/~homework/algo01/9034811/Report/index.htm</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[NP-completeness Problem]]></title>
      <url>http://buptldy.github.io/2016/01/11/2016-01-11-NP-completeness%20Problem/</url>
      <content type="html"><![CDATA[<h2 id="NP完全性"><a href="#NP完全性" class="headerlink" title="NP完全性"></a>NP完全性</h2><p>到目前为止，我们讨论的几乎都是<strong>多项式时间算法</strong>：对于规模n的输入，在最坏情况下的运行时间是$O(n^k)$,其中k为某一确定常数。但还有很多问题在多项式时间内并不能求解，根据能否在多项式时间求解，定义如下几类问题：<br><a id="more"></a></p>
<ul>
<li>P: 能在多项式时间内解决的问题</li>
<li>NP: 不能在多项式时间内解决或不确定能不能在多项式时间内解决，但能在多项式时间验证的问题</li>
<li>NPC: NP完全问题，所有NP问题在多项式时间内都能约化(Reducibility)到它的NP问题，即解决了此NPC问题，所有NP问题也都得到解决。</li>
<li>NP hard:NP难问题，所有NP问题在多项式时间内都能约化(Reducibility)到它的问题(不一定是NP问题)。</li>
</ul>
<p>如果任何NP完全问题是可以多项式求解的，则P=NP，目前还不能证明P是否等于NP，这几个问题的关系如下：</p>
<center><br><img src="http://i.imgur.com/mAyE7SM.png" alt=""><br></center><br>规约的概念：若我们拥有一个已证明难以解决的问题，我们又获得另一个相似的新问题。我们可合理推想此新问题亦是难以解决的。我们可由下列谬证法得证：若此新问题本质上容易解答，且若我们可展示每个旧问题的实例可经由一系列转换步骤变成新问题的实例，则旧问题便容易解决，因此得到悖论。因此新问题可知亦难以解决。<br><br>如何证明某个问题是NP完全的？<br><br>如果我们有一个已经证明的NP完全问题，如果我们可以把已证明的NP完全问题的任何实例都能多项式的规约到要要证明问题的实例，则能证明这个问题是NP完全的。即如果我们要证明一个问题A是NPC问题，<strong>则只需要首先证明他是NP问题，然后只要找一个你所知道的NPC问题规约到A即可。</strong><br><br>## 常见的NPC问题<br>### 布尔可满足性问题（SAT）<br><br>对于一个确定的逻辑电路，是否存在一种输入使得输出为真。是第一个被证明的NPC问题，直观的看出这应该是一个NPC问题，因为当电路有k个输入，就会有$2^k$种情况的不同取值。<br>### 3SAT<br><br>3和取范式：公式中每个字句都恰好有三个不同的‘文字’，例如，布尔公式：<br><br>$$(l_1 ∨ l_2 ∨ x_2) ∧ (¬x_2 ∨ l_3 ∨ x_3) ∧ (¬x_3 ∨ l_4 ∨ x_4) ∧ … ∧ (¬x_{n − 3} ∨ l_{n − 2} ∨ x_{n − 2}) ∧ (¬x_{n − 2} ∨ l_{n − 1} ∨ l_n)$$<br>3SAT问题就是满足3和取范式的布尔公式是否可满足，3SAT问题可由SAT问题规约而来。<br><br>### 分团问题（clique problem）<br><br>无向图中的团是图中所有顶点的一个子集，团中的每一对顶点之间都有一条边相连，即一个团就是无向图中的一个完全子图。分团问题就是要寻找图中规模最大的团，判定条件：在图中是否存在一个给定规模为k的团。<br>### 独立集问题（Independent Set）<br><br>独立集：如果有一个顶点集合S，S中的任意两个顶点之间都没有边相连，则称S为一个独立集。<br>独立集问题和分团问题可相互规约，因为存在一个大小是k以上的分团，等价于它的补图中存在一个大小是k以上的独立集。<br>补图：一个图G的补图（complement）或者反面（inverse）是一个图有着跟G相同的点，而且这些点之间有边相连当且仅当在G里面他们没有边相连。在制作图的时候，你可以先建立一个有G所有点的完全图，然后清除G里面已经有的边来得到补图，这里的补图并不是图本身的补集。<br>### 顶点覆盖问题（Vertex Cover）<br><br>图的顶点覆盖是一些顶点的集合，使得图中的每一条边都至少接触集合中的一个顶点，如下图所示，图中红色顶点可以覆盖图中所有的边。寻找最小的顶点覆盖的问题称为顶点覆盖问题，它是一个NP完全问题。<br><br><center><br><img src="http://i.imgur.com/9wD7p7i.png" alt=""><br></center>

<h3 id="集合覆盖问题"><a href="#集合覆盖问题" class="headerlink" title="集合覆盖问题"></a>集合覆盖问题</h3><p>给定全集$\mathcal{U}$，以及一个包含n个集合且这n个集合的并集为全集的集合$\mathcal{S}$。集合覆盖问题要找到$\mathcal{S}$的一个最小的子集，使得他们的并集等于全集。<br>例如$\mathcal{U} = \{1, 2, 3, 4, 5\}，\mathcal{S} = \{\{1, 2, 3\}, \{2, 4\}, \{3, 4\}, \{4, 5\}\}$，虽然$\mathcal{S}$中所有元素的并集是$\mathcal{U}$，但是我们可以找到$\mathcal{S}$的一个子集$\{\{1, 2, 3\}, \{4, 5\}\}$，我们称其为一个集合覆盖。<br>集合覆盖问题的决定性问题为，给定$(\mathcal{U},\mathcal{S})$和一个整数k，求是否存在一个大小不超过k的覆盖。集合覆盖的最佳化问题为给定$(\mathcal{U},\mathcal{S})$，求使用最少的集合的一个覆盖。</p>
<h3 id="子集合问题（subset-sum-problem）"><a href="#子集合问题（subset-sum-problem）" class="headerlink" title="子集合问题（subset-sum problem）"></a>子集合问题（subset-sum problem）</h3><p>给定一个正整数的有限集S和一个整数目标t&gt;0,求是否存在S的一个子集，使得其元素之和为t。</p>
<h3 id="3-Coloring"><a href="#3-Coloring" class="headerlink" title="3 Coloring"></a>3 Coloring</h3><p>3Col is the problem of deciding whether there is a legal 3-Coloring of a graph (all edges bichromatic).</p>
<h3 id="哈密顿回路（Hamiltonian-Cycle）"><a href="#哈密顿回路（Hamiltonian-Cycle）" class="headerlink" title="哈密顿回路（Hamiltonian Cycle）"></a>哈密顿回路（Hamiltonian Cycle）</h3><p>G=(V,E)是一个图，若G中一条通路通过每一个顶点一次且仅一次，称这条通路为哈密尔顿通路。若G中一个圈通过每一个顶点一次且仅一次，称这个圈为哈密尔顿圈。若一个图存在哈密尔顿圈，就称为哈密尔顿图。</p>
<h2 id="NPC问题的证明"><a href="#NPC问题的证明" class="headerlink" title="NPC问题的证明"></a>NPC问题的证明</h2><p>问题如下：</p>
<p>Given an integer m × n matrix A and an integer m-vector b, the Integer programming problem asks whether there is an integer n-vector x such that Ax ≥ b. Prove that Integer-programming is in NP-complete.</p>
<p>证明整数线性规划是NP完全的。</p>
<p>假设上面所提到的常见的NPC问题使我们已知的NPC问题</p>
<p>我们可以把上面提到的顶点覆盖问题（Vertex Cover）规约成整数规划问题，举个简单的例子，简单无向图如下所示：</p>
<p><center><br><img src="http://i.imgur.com/GWPD8RB.png" alt=""><br></center><br>很显然就能看出顶点2能够覆盖所有的边，现在我们来讨论这个问题怎么用线性规划来表示，用$y_i$取1或0来表示是否选择结点i来覆盖边，则该问题用整数线性规划建模如下：<br>
$$\begin{align*}
         \min y_1+y_2+y_3 \\
         y_1 + y_2 & \ge 1 && \\
         y_2 + y_3 & \ge 1 && \\
         y_1,y_2,y_3 & \ge 0 && \\
         y_1,y_2,y_3 & \in \mathbb{Z} &&
\end{align*}$$
<br>如果网络中有结点之间相连，则构成一个不等式约束。从上面例子来看顶点覆盖问题能够规约成整数线性规划问题，一般来说，网络中有多少个结点对应整数线性规划有多少个变量，如果两个结点之间有边相连，则对应一个约束条件，最小化顶点覆盖则对应最小化所有变量之和。所以对无向图G=(V,E),定义整数线性规划如下：<br>
$$\begin{align*}
         \min \sum_{v \in V} y_v \\
         y_v + y_u & \ge 1 && \forall uv \in E\\
         y_v & \ge 0 && \forall v \in V\\
         y_v & \in \mathbb{Z} && \forall v \in V
\end{align*}$$
</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Maximum Flow Problem]]></title>
      <url>http://buptldy.github.io/2016/01/10/2016-01-10-Maximum%20Flow%20Problem/</url>
      <content type="html"><![CDATA[<h1 id="网络流的定义"><a href="#网络流的定义" class="headerlink" title="网络流的定义"></a>网络流的定义</h1><p>在图论中，网络流（Network flow）是指在一个每条边都有容量（capacity）的有向图上分配每条路劲流量，使一条边的流量不会超过它的容量。通常在运筹学中，有向图称为网络。顶点称为节点（node）而边称为弧（arc）。一道流必须符合一个结点的进出的流量相同的限制，除非这是一个源点（source）──有较多向外的流，或是一个汇点（sink）──有较多向内的流。一个网络可以用来模拟道路系统的交通量、管中的液体、电路中的电流或类似一些东西在一个结点的网络中游动的任何事物。<br><a id="more"></a></p>
<p>假设 G = (V,E) 是一个有限的有向图，它的每条边$  (u,v) \in E $ 都有一个非负值实数的容量$c(u, v)$。如果$(u, v) \not \in E$，我们假设 $c(u, v) = 0$。我们区别两个顶点：一个源点 s 和一个汇点 t 。一道网络流是一个对于所有结点 u 和 v 都有以下特性的实数函数 $f:$：</p>
<blockquote>
<p>容量限制（Capacity Constraints）：    $f(u, v) \le c(u, v)$一条边的流不能超过它的容量。</p>
<p>流量守恒（Flow Conservation）：    除非u = s或u = t，否则 $\sum_{w \in V} f(u, w) = 0$，中间结点的流入等于流出。</p>
</blockquote>
<p>即流守恒意味着： $\sum_{(u,v) \in E} f(u,v) = \sum_{(v,z) \in E} f(v,z) $，对每个顶点${v \in V\setminus{s,t}}$。</p>
<p><strong>最大流问题</strong>就是，给定一个流网络G，一个源节点s，一个汇点t，我们希望找到值最大的一个流。</p>
<p>一个网络流如下图所示：</p>
<center><br><img src="http://i.imgur.com/a8NFZl3.png" alt=""><br></center>

<h1 id="网络最大流算法"><a href="#网络最大流算法" class="headerlink" title="网络最大流算法"></a>网络最大流算法</h1><ul>
<li>Ford-Fulkerson算法</li>
</ul>
<p>残存网络的概念：</p>
<p>边的残存容量（residual capacity）是$ c_f(u, v) = c(u, v) - f(u, v)$。</p>
<p>定义 $G_f(V, E_f)$ 表示剩余网络（residual network），它显示当前网络可用的容量的多少。就算在原网络中由 u 到 v 没有边，在剩余网络仍可能有由 u 到 v 的边。因为残存网络允许相反方向的流抵消，减少由 v 到 u 的流相当于增加由 u 到 v 的流，因为我们是为了求最大流，之前走过的路可能是走错的。</p>
<p>增广路（augmenting path）是一条路径 $(u_1, u_2, \dots, u_k)$，而$u_1 = s , u_k = t $ 及$c_f(u_i , u_{i+1})&gt;0$，如果存在增广路，这表示沿这条路径还能够传送更多流。当且仅当剩余网络$G_f$ 没有增广路时处于最大流。</p>
<p>建立残存网络$ G_f$的步骤：</p>
<ol>
<li>$ G_f =  V $ 的顶点</li>
<li><p>定义如下的 $ G_f =  E_f$ 的边,对每条边 $ (x,y) \in E$</p>
<ul>
<li>若$ f(x,y) &lt; c(x,y)$，建立容量为$ c_f = c(x,y) - f(x,y)$ 的前向边$ (x,y) \in E_f$。</li>
<li>若$ f(x,y) &gt; 0$，建立容量为$ c_f =  f(x,y)$ 的后向边$ (y, x) \in E_f$。</li>
</ul>
</li>
</ol>
<p>上图中的残存网络如下图所示：</p>
<p><center><br><img src="http://i.imgur.com/RS7lQTw.png" alt=""><br></center><br>最小割的概念：</p>
<p>割的定义： 一个s-t 的割 C = (S, T) 把 所有的结点集合V分成两部分S和T，其中源节点s ∈ S 汇点 t ∈ T. 割 C 的集合表示如下：</p>
<p>$$\{(u,v)\in E\: u\in S,v\in T\}$$</p>
<p><strong>当割中的边被移掉时,则从源节点到汇结点的流量为0</strong>.</p>
<p>割容量的定义：</p>
<p>$$c(S,T)=\sum \nolimits_{(u,v)\in S\times T}c_{uv}$$</p>
<blockquote>
<p>最大流最小割定理：当残存网络中不含有任何增广路径时，网络中的流量f最大且等于最小割容量。</p>
</blockquote>
<p>如下图所示，网络的最大流为7，最小割由图中虚线组成，其中最小割的容量也为7.</p>
<p><center><br><img src="http://i.imgur.com/7Q718mT.png" alt=""><br></center><br>Ford-Fulkerson算法求网络的最大流就是在每次的迭代中，寻找某条增广路径p，然后使用p来对流f进行修改，直到残存网络中不含有任何增广路径时，求得最大流f。</p>
<p>Ford-Fulkerson算法伪代码：</p>
<p><center><br><img src="http://i.imgur.com/zsQkoBR.png" alt=""><br></center><br><a href="https://github.com/BUPTLdy/Algorithms/tree/master/Ford-Fulkerson" target="_blank" rel="external">Ford-Fulkerson算法Python实现</a></p>
<ul>
<li>Push-relabel algorithm</li>
</ul>
<p>Push-Relabel系的算法普遍要比Ford-Fulkerson系的算法快，但是缺点是相对难以理解。详细内容可以参考<a href="https://en.wikipedia.org/wiki/Push%E2%80%93relabel_maximum_flow_algorithm#Concepts" target="_blank" rel="external">维基百科 Push–relabel maximum flow algorithm</a></p>
<p><a href="https://github.com/BUPTLdy/Algorithms/tree/master/Push-relabel" target="_blank" rel="external">Push-relabel 算法Python实现</a></p>
<h1 id="网络最大流问题建模"><a href="#网络最大流问题建模" class="headerlink" title="网络最大流问题建模"></a>网络最大流问题建模</h1><p>问题如下：</p>
<p>Support the you are a matchmaker and there are some boys and girls. Since the boys are alway more than girls, you can assume that if a girl express her love to a boy , the boy will always accept her. Now you know every girl’s thought(a girl may like more than one boy) and you want to make as much pairs as you can. show that you can do this using maximum flow algorithm.</p>
<p>题目意思大概是有一些男孩和女孩。男孩的数量大于等于女孩的数量，如果女孩向男孩表达爱意，男孩必定接受，假设你是个媒婆，而且你知道女孩们喜欢那个男孩(一个女孩可能同时喜欢多个男孩)，要你用最大流的方法求最多能匹配多少对？</p>
<p>问题分析：最终一个女孩肯定只能和一个男孩配对，为了简单分析，我们假设有3个女孩$\{G_1,G_2,G_3\}$，3个男孩$\{B_1,B_2,B_3\}$,并且已知$G_1$喜欢$B_1$和$B_2$，$G_2$喜欢$B_2$,$G_3$喜欢$B_3$，我们可以构造出如下图所示的网络流：</p>
<p><center><br><img src="http://i.imgur.com/9hAGUgE.png" alt=""><br></center><br>求出上图所示网络的的最大流就是最大的匹配对数，根据这个思路，这个题抽象成最大流问题为：</p>
<pre><code>- 源节点，汇点，以及每个女孩男孩都构成一个节点
- 如果某个女孩喜欢某些男孩，则把这个女孩和那些男孩相连
- 把源节点和每个女孩相连，汇结点和每个男孩相连
- 网络中所有相连边的容量为1
</code></pre><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>维基百科 网络流：<a href="https://zh.wikipedia.org/wiki/%E7%BD%91%E7%BB%9C%E6%B5%81" title="维基百科 网络流" target="_blank" rel="external">https://zh.wikipedia.org/wiki/%E7%BD%91%E7%BB%9C%E6%B5%81</a></p>
<p>Wikipedia Max-flow min-cut theorem:<a href="https://en.wikipedia.org/wiki/Max-flow_min-cut_theorem" title="Wikipedia Max-flow min-cut theorem" target="_blank" rel="external">https://en.wikipedia.org/wiki/Max-flow_min-cut_theorem</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linear Programming]]></title>
      <url>http://buptldy.github.io/2016/01/09/2016-01-09-Linear%20Programming/</url>
      <content type="html"><![CDATA[<h1 id="什么是线性规划"><a href="#什么是线性规划" class="headerlink" title="什么是线性规划"></a>什么是线性规划</h1><p>线性规划（Linear Programming，简称LP）是指目标函数和约束条件皆为线性函数的最优化问题。</p>
<p>线性规划问题的常用的最直观形式是标准型。标准型包括以下三个部分：<br><a id="more"></a></p>
<ul>
<li><p>一个需要极大化的线性函数，例如：</p>
<p>  $$c_1 x_1 + c_2 x_2$$</p>
</li>
<li><p>以下形式的问题约束，例如：</p>
<p>  $$a_{11} x_1 + a_{12} x_2 \le b_1$$</p>
<p>  $$a_{21} x_1 + a_{22} x_2 \le b_2$$</p>
<p>  $$a_{31} x_1 + a_{32} x_2 \le b_3$$</p>
</li>
<li><p>非负变量，例如：</p>
<p>  $$x_1 \ge 0 $$</p>
<p>  $$x_2 \ge 0 $$</p>
</li>
</ul>
<p>几个概念和定理介绍：</p>
<p>可行域：下图中蓝色部分的点都是线性规划问题的解(可行解)，蓝色区域是可行解的集合，称为可行域。</p>
<center><br><img src="http://i.imgur.com/0QzN1q2.png" alt=""><br></center>

<p>基可行解：出现在可行域顶点的可行解</p>
<blockquote>
<p>定理1 若线性规划问题存在可行域，则其可行域是<a href="https://zh.wikipedia.org/wiki/%E5%87%B8%E9%9B%86" target="_blank" rel="external">凸集</a></p>
<p>定理2 若可行域有界，线性规划问题的目标函数一定可以在其可行域的顶点上达到最优</p>
</blockquote>
<p>通过上述定理得知，线性规划问题的解一定出现在可行域的顶点上，所以可以通过枚举所有的基可行解来找到最优解，但当变量个数很多时，这种办法是行不通的。</p>
<h1 id="单纯形法"><a href="#单纯形法" class="headerlink" title="单纯形法"></a>单纯形法</h1><p>单纯形法是求解线性规划问题的通用方法。单纯形是美国数学家G.B.丹齐克于1947年首先提出来的。它的理论根据是：线性规划问题的可行域是$n$维向量空间$R_n$中的多面凸集，其最优值如果存在必在该凸集的某顶点处达到。顶点所对应的可行解称为基本可行解。</p>
<p>单纯形法的基本思想是：先找出一个基本可行解，对它进行鉴别，看是否是最优解；若不是，则按照一定法则转换到另一改进的基本可行解，再鉴别；若仍不是，则再转换，按此重复进行。因基本可行解的个数有限，故经有限次转换必能得出问题的最优解。如果问题无最优解也可用此法判别。</p>
<p>单纯形法的一般解题步骤可归纳如下：</p>
<p>①把线性规划问题的约束方程组表达成典范型方程组，找出基本可行解作为初始基可行解。</p>
<p>②若基本可行解不存在，即约束条件有矛盾，则问题无解。</p>
<p>③若基本可行解存在，从初始基本可行解作为起点，根据最优性条件和可行性条件，引入非基变量取代某一基变量，找出目标函数值更优的另一基本可行解。</p>
<p>④按步骤③进行迭代,直到对应检验数满足最优性条件（这时目标函数值不能再改善），即得到问题的最优解。</p>
<p>⑤若迭代过程中发现问题的目标函数值无界，则终止迭代。</p>
<p>过程如下图所示：</p>
<center><br><img src="http://i.imgur.com/UfuthIQ.png" alt=""><br></center>

<p><a href="http://wenku.baidu.com/view/0edfb06aaf1ffc4ffe47acec.html" target="_blank" rel="external">单纯形法求解-动态演示</a></p>
<p>单纯形法伪代码：</p>
<p><center><br><img src="http://i.imgur.com/lhxj168.png" alt=""><br></center><br><a href="https://github.com/BUPTLdy/Algorithms/tree/master/Simplex%20Algorithm" target="_blank" rel="external">单纯形法Python实现</a></p>
<p>解线性规划问题也一些很好的工具，比如<a href="https://www.gnu.org/software/glpk/" target="_blank" rel="external">GLPK</a>和<a href="http://www.gurobi.com/" target="_blank" rel="external">Gurobi</a>等。</p>
<h1 id="线性规划解决实际问题"><a href="#线性规划解决实际问题" class="headerlink" title="线性规划解决实际问题"></a>线性规划解决实际问题</h1><p>问题如下：</p>
<p>with human lives at stake, an air traffic controller has to schedule the airplanes that are landing at an airport in order to avoid airplane collision. Each airplane $i$ has a time window $[s_i,t_i]$ during which it can safely land. You must compute the exact time of landing for each airplane that respects these time windows. Furthermore, the airplane landings should be stretched out as much as possible so that the minimum time gap between successive landings is as large as possible. For example, if the time window of landing three airplanes are [10:00-11:00], [11:20-11:40], [12:00-12:20], and they land at 10:00, 11:20, 12:20 respectively, then the smallest gap is 60 minutes, which occurs between the last two airplanes. Given n time windows, denoted as [s_1,t_1], [s_2,t_2], · · ·, [s_n,t_n] satisfying s_1 &lt;t_1 &lt; s_2 &lt; t_2 &lt; · · · &lt; s_n &lt; t_n, you are required to give the exact landing time of each airplane, in which the smallest gap between successive landings is maximized.</p>
<pre><code>Please formulate this problem as an LP.
</code></pre><p>题目的大概意思是每架飞机都只能在自己固定的时间窗内降落，为了安全起见两架飞机之间的降落时间间隔越大越好，然后给你n架飞机的降落时间窗口，要求n架飞机的最小降落间隔的最大值。</p>
<p>这个问题的建模起来很简单，令$x_i$表示第$i$架飞机的降落时间，则需要满足约束条件：</p>
<p>$$s_i\leq x_i \leq t_i$$</p>
<p>然后我们的目标是要求最小间隔的最大值，所以我们的目标函数为：</p>
<p>$$max(min(x_{i+1}-x_i))$$</p>
<p>那么现在问题来了，我们上面所说的线性规划的标准形式是不包括既有max又有min的，所以我们需要把这个min去掉，我们可以通过引入一个新变量，如果有$y\leq x_{i+1}-x_{i}$，那么$y$不就是$x_{i+1}-x_{i}$的最小值吗？</p>
<p>所以最终我们可以把问题形式化为:</p>

$$  \begin{align*}
    &max~y \\
    s.t.~ &s_i\leq x_i \leq t_i i=1,2,3 \cdots n \\
    &y \leq x_{i+1}-x_i & i=1,2,3 \cdots n
  \end{align*}$$

<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>维基百科 线性规划：<a href="https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92" title="维基百科 线性规划" target="_blank" rel="external">https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92</a></p>
<p>百度百科 单纯形法：<a href="http://baike.baidu.com/subview/471090/471090.htm" title="百度百科 单纯形法" target="_blank" rel="external">http://baike.baidu.com/subview/471090/471090.htm</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Greedy Algorithm]]></title>
      <url>http://buptldy.github.io/2016/01/08/2016-01-08-Greedy%20Algorithm/</url>
      <content type="html"><![CDATA[<h1 id="贪心算法-greedy-algorithm"><a href="#贪心算法-greedy-algorithm" class="headerlink" title="贪心算法(greedy algorithm)"></a>贪心算法(greedy algorithm)</h1><p>贪心算法通过做出一系列的选择来求出问题的最优解。 在每个决策点，它做出在当时看来是最佳的选择。贪心算法可以说是<a href="http://buptldy.github.io/2016/01/07/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/">动态规划</a>问题的一种特例，在学习贪心算法之前，必须先得了解动态规划算法。贪心算法总是做出局部最优的选择，并不能总能保证找到全局最优解，那我们怎么才能保证一个贪心算法能够求解一个最优化问题呢？<br><a id="more"></a><br>如果我们能够证明要解决的问题具有如下两个性质，那么我们向贪心算法迈出了重要一步。</p>
<ul>
<li>贪心选择性质</li>
</ul>
<p>最关键的要素是看问题是否具有贪心选择性质：如果我们可以通过做出局部最优选择来构造全局解。也就是说，在进行选择时，我们直接选择在当前问题看起来最优的情况，而不必考虑子问题的解。</p>
<p>这也是贪心算法与动态规划的不同之处，动态规划中每次的选择依赖于子问题的解，因此通常使用自底向上的方式求解动态规划问题。在贪心算法中，我们总是做出当时看起来是最佳的选择，然后求解剩下的<strong>唯一子问题</strong>。贪心算法在选择时可能依赖以前的选择，但不依赖任何将来的选择或者子问题的解。因此，<strong>动态规划算法是自底向上进行计算的，而每一个贪心算法通常时自顶向下的，进行一次又一次选择，将给定问题的实例变得更小。</strong></p>
<ul>
<li>最优子结构</li>
</ul>
<p>如果一个问题的最优解包含其子问题的最优解，则称此问题具有最优子结构性质。此性质是能否应用动态规划和贪心算法的关键要素。当应用于贪心算法时，我们可以假定，<strong>通过对原问题应用贪心选择就可以得到子问题</strong>，我们所要做的工作就是论证：将子问题的最优解与贪心选择组合在一起就能得到原始问题的最优解。</p>
<p>经过上述描述，我们可以按如下步骤设计贪心算法：</p>
<p>1.将最优化问题转化成这样的形式：对其做出一次选择后，只剩下一个子问题需要求解。</p>
<p>2.证明做出贪心选择后，原问题总是存在最优解，即贪心选择总是安全的。</p>
<p>3.证明做出贪心选择后，剩余的子问题满足性质:其最优解与贪心选择组合即可得到原问题的最优解。</p>
<h1 id="问题举例：Havel-Hakimi定理"><a href="#问题举例：Havel-Hakimi定理" class="headerlink" title="问题举例：Havel-Hakimi定理"></a>问题举例：Havel-Hakimi定理</h1><p>Given a list of $n$ natural numbers $d_1, d_2,…,d_n$, show how to decide in polynomial time whether there exists an undirected graph G = (V, E) whose node degrees are precisely the numbers $d_1, d_2, \cdots , d_n$. G should not contain multiple edges between the same pair of nodes, or “ loop” edges with both endpoints equal to the same node.</p>
<p>题目的大概意思是：给你一组数字序列，数字的大小为图的度，问这些数字的度能够构成一个图（无向无环图），如果可以，则称该序列是可图的。</p>
<p>怎么用贪心算法分析：</p>
<p>1.首先由无向图的性质分析，如果所有的度之和为奇数，显然不能构成无向图<br>2.如果最大的度比序列的长度还大，明显也不能构成图，因为就算所有的结点和它相连，度的值也为n-1小于n。<br>3.贪心选择:由<strong>非负数组成的非增序列</strong> $s:d_1,d_2,\cdots,d_n（n&gt;=2，d_1&gt;=1$是可图的，当仅当序列</p>
<p>$$s1:d_2-1,d_3-1,\cdots d_{d_1+1}-1,d_{d1+2},\cdots,d_n$$</p>
<p>是可图的。序列s1中有n-1个非负数，s序列排在$d_1$之后的前$d_1$个数减1后构成s1中的前$d_1$个数。</p>
<p>判定过程：一直循环直到当前序列出现负数（即不是可图的情况）或者当前序列全为0 （可图）时退出。</p>
<p>怎么理解这个贪心选择：如果一个序列是可图的，则我们选择一个度最小的结点，然后去掉这个结点以及和这个结点相连接的边，那么剩下的结点还是可图的，当然反过来也是可以的，所以这符合贪心算法的条件，去点一个结点以及和它连接的边之后，只要判断剩下的子问题是不是可图，逐渐的减少问题的规模，直到求解。</p>
<p>Havel-Hakimi定理伪代码:(注意下面伪代码中的sort()排序是从大到小排序)</p>
<p><img src="http://i.imgur.com/K3Qj1gw.png" alt=""></p>
<h1 id="问题举例：霍夫曼编码-Huffman-Coding"><a href="#问题举例：霍夫曼编码-Huffman-Coding" class="headerlink" title="问题举例：霍夫曼编码(Huffman Coding)"></a>问题举例：霍夫曼编码(Huffman Coding)</h1><p>霍夫曼编码的具体原理就不仔细介绍了，可以参考维基百科<a href="https://zh.wikipedia.org/wiki/%E9%9C%8D%E5%A4%AB%E6%9B%BC%E7%BC%96%E7%A0%81" title="霍夫曼编码" target="_blank" rel="external">霍夫曼编码</a>，主要在这里说说霍夫曼编码中怎么应用贪心算法。</p>
<p>贪心选择：构造霍夫曼树的关键之处在于每一步执行的时候，并不知道这个低频率的字符会编码为多少，只能保证它在树的最下面，使用最长的编码，这样就不会影响频率高的字符的编码。而每一步贪心的过程，都将两个频率低的字符合成一个父字符，减小了问题的规模，这样这个策略就在不影响其它字符编码的情况下，不断缩小问题的规模，直到最终求解。</p>
<p><a href="https://github.com/BUPTLdy/Algorithms/tree/master/huffman" target="_blank" rel="external">霍夫曼编码CPP代码下载</a></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>贪心算法如何体现在霍夫曼编码中？<a href="https://www.zhihu.com/question/22112710/answer/56030576" title="贪心算法如何体现在霍夫曼编码中？" target="_blank" rel="external">https://www.zhihu.com/question/22112710/answer/56030576</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Dynamic Programming]]></title>
      <url>http://buptldy.github.io/2016/01/07/2016-01-07-Dynamic%20Programming/</url>
      <content type="html"><![CDATA[<h1 id="动态规划-dynamic-programming"><a href="#动态规划-dynamic-programming" class="headerlink" title="动态规划(dynamic programming)"></a>动态规划(dynamic programming)</h1><p>动态规划与<a href="http://buptldy.github.io/2016/01/06/%E5%88%86%E6%B2%BB%E7%AD%96%E7%95%A5%5BDivide%20and%20Conquer%5D/">分治方法</a>相似，都是通过组合子问题的接来求解原问题，分治方法是将问题划分为互不相交的子问题，递归的求解子问题，再将它们的解组合起来，求出原问题的解。动态规划与之相反，应用于子问题重叠的情况，即不同的子问题具有公共的子子问题。在这种情况下，分治算法会反复的求解这些公共子问题，而动态规划对每个子问题只求解一次并保存结果，从而无需重复求解。<br><a id="more"></a><br>通常动态规划被用来求解<a href="https://en.wikipedia.org/wiki/Optimization_problem" target="_blank" rel="external">最优化问题</a>，即在可行解中寻找最优解。</p>
<p>设计一个动态规划算法通常有如下4个步骤：</p>
<pre><code>- (1).刻画一个最优解的结构特征
- (2).递归地定义最优解的值
- (3).采用自底向上的方法计算最优解的值
- (4).利用计算出的信息构造最优解
</code></pre><p>如果我们只需要得到这个最优解的结果，而不关注这个解是怎么得来的，则可以忽略步骤(4)。</p>
<p>上面叙述了动态规划方法的步骤，但是什么问题才能够使用动态规划法求解？使用动态规划方法求解的最优化问题应该具备两个要素：最优子结构和子问题重叠。</p>
<ul>
<li>最优子结构</li>
</ul>
<p>如果一个问题的最优解包含其子问题的最优解，则称此问题具有最优子结构性质。在动态规划方法中，我们通常自底向上地使用最优子结构，即首先求得子问题的最优解，然后求原问题的最优解。<strong>原问题的最优解的代价通常就是子问题最优解的代价再加上由此次选择直接产生的代价。</strong></p>
<ul>
<li>子问题重叠</li>
</ul>
<p>如果递归算法反复求解相同的子问题，就称最优化问题具有重叠子问题。动态规划算法通常这样利用重叠子问题性质：<strong>对每个子问题求解一次，将解存入一个表中，当再次需要这个子问题时直接查表，每次查表的代价为常量时间。</strong></p>
<h1 id="问题举例：Money-robbing"><a href="#问题举例：Money-robbing" class="headerlink" title="问题举例：Money robbing"></a>问题举例：Money robbing</h1><p>问题如下所示：</p>
<p>A robber planning to rob houses along a street. Each house has a certain amount of money stashed, the only constraint stopping you from robbing each of them is that adjacent houses have security system connected and it will automatically contact the police if two adjacent houses were broken into on the same night.</p>
<ol>
<li>Given a list of non-negative integers representing the amount of money of each house, determine the maximum amount of money you can rob tonight without alerting the police.</li>
<li>What if all houses are arranged in a circle?</li>
</ol>
<p>大概意思就是街上有一排房子，有个房子里有一定数量的钱，如果小偷在同一天晚上偷了两座相邻的房子，就会触发警报系统，第一问就是问在小偷不触发警报系统的前提下，怎样偷到的钱最多。</p>
<p>动态规划问题最重要的就是求出递归式，把原问题的最优化化为子问题的最优化。对这个问题来说，唯一的约束条件是不能同时抢劫相邻的两座房子，假设共有n座房子，分两种情况讨论：</p>
<p>(1). 如果你抢劫了第n座房子，很明显你能得到第n座房子的钱，但你只能抢劫第n-2座房子了，因为你不想被抓起来；</p>
<p>(2). 此时不选择抢劫第n座房子，当然待会你就可以抢劫第n-1座房子了</p>
<p>但这两种选择哪种是最优的了，很明显取决与这两个子问题的最优解，所以我们把原问题的最优解化成求解子问题的最优解，根据上述两种情况划分，可以很容易的写出递推公式：</p>
<p>$$dp(n)=MAX\{dp(n-1),dp(n-2)+money(n)\}$$</p>
<p>其中money(n)表示抢劫第n座房子得到的钱(注意在写程序是money[n-1],因为数组下标从0开始)，根据递推公式，采用自底向上的方法，就能计算出最优的结果。</p>
<p>伪代码如下所示：<br><img src="http://i.imgur.com/MW11e6T.png" alt=""></p>
<p>再来看第二问，第二问的意思就是如果房子不是排成一排而是围成一圈，该怎样才能偷到最多的钱？<br>我们随机的从一圈n座房子中选中一座房子，现在我们面临两个选择，抢还是不抢？</p>
<p>(1) 假设我们选择抢劫这座房子，当然我们能得到这座房子的钱，但是为了避免触发警报我们不能抢劫它周围的两座房子了，那么剩下的n-3座房子就没有构成一个圈了，那么问题也就规约成第一问的情况了；<br>(2) 假如我们没有抢劫这座房子，那么去掉这座房子，剩下的n-1座房子不是就不构成圈了吗，所以还是回到第一问的问题。</p>
<p>经过分析，所以我们得到递推公式为:</p>
<p>$$dp_{circle(n)}=MAX\{dp(n-1),dp(n-3)+money(n)\}$$</p>
<h1 id="问题举例：Maximum-profit-of-transactions"><a href="#问题举例：Maximum-profit-of-transactions" class="headerlink" title="问题举例：Maximum profit of transactions"></a>问题举例：Maximum profit of transactions</h1><p>问题如下：</p>
<p>Say you have an array for which the i-th element is the price of a given stock on day i.<br>Design an algorithm and implement it to find the maximum profit. You may complete at most two transactions.</p>
<p>Note: You may not engage in multiple transactions at the same time (ie,you must sell the stock before you buy again).</p>
<p>这个题目的意思是给你每天股票的价格，你能最多进行两次交易(一次交易包括买进和卖出)，怎样才能获得最大的收益。</p>
<p>我们先来讨论只在一次交易的情况下，怎么求得收益最大化，其实这个问题就是给你一个数组，求出这组数里面不是<a href="http://buptldy.github.io/2016/01/06/%E5%88%86%E6%B2%BB%E7%AD%96%E7%95%A5%5BDivide%20and%20Conquer%5D/">逆序数</a>(因为卖出肯定在买进之后)但相差最大的两个数的差,比如5，1，3，2，4中，以价钱1买进，价钱4卖出可以获得最大的收益3。</p>
<p>我们用动态规划法来分析这个问题，通过自底向上的方法，分析如何从前i天的最大收益推出前i+1天的最大收益，已知前i天的最大收益和前i天的最低价格：</p>
<ol>
<li>第i+1天的价格大于minPrice（已遍历数据的最低价），此时只要对max(i)（前i天的最大获益）和prices[i + 1] - minPrice（第i+1天卖出所能得到的获益）取大值就能得出max(i + 1)</li>
<li>第i+1天的价格小于等于minPrice，那么在第i+1天卖出所得到的获益必然是小于max(i)（这里哪怕考虑极端情况：给出的数据是整体递减的，那么最佳的卖出时机也是当天买当天卖，获益为0，所以不会存在获益是负值的情况），所以max(i + 1) = max(i)。而且，对于之后的数据而言，minPrice需要更新了，因为对于之后的数据，在第i+1天买进必然比在0到i天之间的任何时候买进的获益都要多（因为第i+1天是0到i+1区间内的最低价）。</li>
</ol>
<p>所以通过上述动态规划的方法可以求出只进行一次交易的最大收益，但我们题目中问的是最多进行两次交易的情况下，我们可以把Prices[] 分成两部分Prices[0…m] 和 Prices[m…length]  ，分别计算在这两部分内做交易的最大收益，方法就是上面所说的一次交易的方法，第一步扫描，先计算出子序列[0,…,i]中的最大利润，用一个数组保存下来，时间是O(n)。 第二步是逆向扫描，计算子序列[i,…,n-1]上的最大利润，这一步同时就能结合上一步的结果计算最终的最大利润了，这一步也是O(n)。 所以最后算法的复杂度就是O(n)。</p>
<p><a href="https://github.com/BUPTLdy/Algorithms/tree/master/Maximum%20profit%20of%20transactions" target="_blank" rel="external">算法代码下载(CPP)</a></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>Best Time to Buy and Sell Stock I II III IV@LeetCode：<a href="http://segmentfault.com/a/1190000002565570" title="Best Time to Buy and Sell Stock I II III IV@LeetCode" target="_blank" rel="external">http://segmentfault.com/a/1190000002565570</a><br>LeetCode-Best Time to Buy and Sell Stock系列：<a href="http://www.tuicool.com/articles/rMJZj2" title="LeetCode-Best Time to Buy and Sell Stock系列" target="_blank" rel="external">http://www.tuicool.com/articles/rMJZj2</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Divide and Conquer]]></title>
      <url>http://buptldy.github.io/2016/01/06/2016-01-06-Divide%20and%20Conquer/</url>
      <content type="html"><![CDATA[<h1 id="分治法简介："><a href="#分治法简介：" class="headerlink" title="分治法简介："></a>分治法简介：</h1><p>使用分治法的前提是问题在结构上是<strong>递归</strong>的，为了解决这一问题，算法一次或多次递归地调用自身以解决紧密相关的若干个子问题。</p>
<p>分治模式在每层的递归时有三个步骤：<br><a id="more"></a></p>
<pre><code>- **分解**原问题为若干子问题，这些子问题是原问题规模较小的实例。
- **解决**这些子问题，递归的求解各子问题，当子问题规模最够小时，则直接求解。
- **合并**这些子问题的解得到原始问题的解
</code></pre><h1 id="Karatsuba-算法"><a href="#Karatsuba-算法" class="headerlink" title="Karatsuba 算法"></a><a href="https://en.wikipedia.org/wiki/Karatsuba_algorithm" target="_blank" rel="external">Karatsuba</a> 算法</h1><p>Karatsuba算法是一种快速乘法算法，在1960年由<a href="https://en.wikipedia.org/wiki/Anatoly_Karatsuba" target="_blank" rel="external">Anatoly Karatsuba</a>提出。普通乘法的复杂度是$O(n^2)$,而Karatsuba算法的时间复杂度为$O(n^{1.585})$。</p>
<p>现在运用分治的思想来阐述下Karatsuba算法的基本原理：</p>
<ul>
<li>分解：原本要计算两个大数x，y的乘法,先把x,y分解成如下两部分，其中B为基。</li>
</ul>
<p>$$x = x_1B^m + x_0$$</p>
<p>$$y = y_1B^m + y_0$$</p>
<p>其中 $x_0$ 和 $y_0$ 要小于 $B^m$。 现在$xy$可以写为:</p>
<p>\begin{equation}<br>\begin{cases}<br>xy = (x_1B^m + x_0)(y_1B^m + y_0)\\<br>xy = z_2B^{2m} + z_1B^m + z_0\\<br>z_2 = x_1y_1\\<br>z_1 = x_1y_0 + x_0y_1\\<br>z_0 = x_0y_0<br>\end{cases}<br>\end{equation}</p>
<p>原本上面的式子中一共需要计算4次乘法（与基的幂次相乘只要进行移位操作就行），但是注意到有：</p>
<p>$$z_1 = (x_1 + x_0)(y_1 + y_0) - z_2 - z_0$$</p>
<p>所以每次只需要计算三次乘法即可。</p>
<ul>
<li>解决：通过上述分解步骤，我们每次还是需要3次乘法运算，然而这三次乘法运算我们可以继续递归的调用Karatsuba算法，直到数字足够小可以直接运用普通乘法求解。</li>
<li>合并：Karatsuba算法并没有涉及合并问题，通过公式$xy = z_2B^{2m} + z_1B^m + z_0$把最终结果求出来即可。</li>
</ul>
<p>根据上述分析，当m=n/2时(n为乘数的长度)，递归的效率最高，所以递归公式为：</p>
<p>$$T(n) = 3 T(\lceil n/2\rceil) + cn + d$$</p>
<p>由递归公式根据<a href="http://buptldy.github.io/2015/12/29/%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6%E6%B8%90%E8%BF%9B%E6%A0%87%E5%8F%B7/">主定理</a>得到算法的复杂度为：</p>
<p>$$T(n) = \Theta(n^{\log_2 3})$$</p>
<p>Karatsuba算法伪代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">procedure karatsuba(num1, num2)</span><br><span class="line">			if (num1 &lt; 10) or (num2 &lt; 10)</span><br><span class="line">		return num1*num2</span><br><span class="line">			/* calculates the size of the numbers */</span><br><span class="line">			m = max(size_base10(num1), size_base10(num2))</span><br><span class="line">		m2 = m/2</span><br><span class="line">			/* split the digit sequences about the middle */</span><br><span class="line">		high1, low1 = split_at(num1, m2)</span><br><span class="line">			high2, low2 = split_at(num2, m2)</span><br><span class="line">			/* 3 calls made to numbers approximately half the size */</span><br><span class="line">			z0 = karatsuba(low1,low2)</span><br><span class="line">			z1 = karatsuba((low1+high1),(low2+high2))</span><br><span class="line">			z2 = karatsuba(high1,high2)</span><br><span class="line">			return (z2*10^(2*m2))+((z1-z2-z0)*10^(m2))+(z0)</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/BUPTLdy/Algorithms/tree/master/Karatsuba" target="_blank" rel="external">Karatsuba算法代码下载(CPP)</a></p>
<h1 id="归并排序-MergeSort"><a href="#归并排序-MergeSort" class="headerlink" title="归并排序(MergeSort)"></a>归并排序(MergeSort)</h1><p>还是根据分治策略的思想，分为三个步骤：</p>
<ul>
<li>分解：把要排序的n个元素序列分解成两个含有n/2个元素的子序列<center><br><img src="http://i.imgur.com/umuDsOg.png" alt=""><br></center></li>
<li>解决：递归的调用分解，直到子序列能够直接排序</li>
<li>合并：归并排好序的子序列，直到得到原始问题的解</li>
</ul>
<p>简单例子演示：</p>
<center><br><img src="http://i.imgur.com/33u5yyl.gif" alt=""><br></center>

<h1 id="逆序数统计-Counting-Inversion"><a href="#逆序数统计-Counting-Inversion" class="headerlink" title="逆序数统计(Counting Inversion)"></a>逆序数统计(Counting Inversion)</h1><p>问题定义：输入n个不同的数字$a_1,a_2,\cdots ,a_n$，计算有多少对逆序数，逆序数的定义为数字下标$i<j$但是数字$a_i>a_j$。比如数字序列2，4，1，3，5，数字2在数字1的前面，但比数字1大，所以是一对逆序数，上述数字序列共有3组逆序数：（2，1），（4，1），（4，3）。</j$但是数字$a_i></p>
<p>统计逆序数要用到分治的思想，我们能很容易能想到先分解成两个子序列然后再递归求解每个子序列的逆序数。</p>
<p>现在主要的问题是解决，怎么计算两个不同子序列之间的逆序数，也就是怎么合并的问题，如下图所示，如果只是通过子序列之间每个数字的简单比较求解逆序数，则需要通过$n^2/4$次比较。所以时间复杂度为$T(n)=2T(n/2)+n^2/4=O(n^2)$(求解方法参考<a href="http://buptldy.github.io/2015/12/29/%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6%E6%B8%90%E8%BF%9B%E6%A0%87%E5%8F%B7/">主定理</a>)</p>
<center><br><img src="http://i.imgur.com/mDpKIUp.png" alt=""><br></center>

<p>所以这样和直接暴力解法相比，时间复杂度比没有降低，这时我们通过前面的归并排序想到，如果我们先对子序列进行排序(对子序列排序并不会影响两个子序列之间的逆序数对)，再统计子序列之间的逆序数。统计方法如下图所示，两个排好序的子序列之间进行逆序数统计，比如说第一个序列的第一个数字3比第二个序列的第一个数字2要大，则第一个序列3后面的数字肯定都要比2要大，所以直接统计出逆序数为6，根据这种思想可以把子序列之间的所有逆序数统计出来。</p>
<p><img src="http://i.imgur.com/wJtHUMG.gif" alt=""></p>
<p>(gif generated by <a href="https://screentogif.codeplex.com/" target="_blank" rel="external">ScreenToGif</a>)</p>
<p>上述合并求解逆序数的方法，先给子序列排序并同时计算逆序数，花费$O(n)$的时间，所以问题总的时间复杂度为：</p>
<p>$$T(n)=2T(n/2)+O(n)=O(nlogn)$$</p>
<p><a href="https://github.com/BUPTLdy/Algorithms/tree/master/Counting%20Inversion" target="_blank" rel="external">Counting Inversion算法代码下载(CPP)</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Heuristic Search Algorithm]]></title>
      <url>http://buptldy.github.io/2016/01/05/2016-01-05-Heuristic%20Search%20Algorithm/</url>
      <content type="html"><![CDATA[<h1 id="什么是启发式搜索"><a href="#什么是启发式搜索" class="headerlink" title="什么是启发式搜索"></a>什么是启发式搜索</h1><p><a href="http://buptldy.github.io/2016/01/04/%E5%9B%BE%E6%90%9C%E7%B4%A2/">无信息图搜索</a>一般需要产生大量的节点，因而效率较低。为提高效率，可以使用一些问题相关的信息，以减小搜索量，这些信息就称为启发式信息。使用启发式信息指导的搜索过程称为启发式搜索，所以启发式图搜索与无信息图搜索之间的区别就是启发式图搜索在OPEN表的排序过程中使用了与问题有关的知识。<br><a id="more"></a><br>在启发式搜索过程中对OPEN表进行排序，就需要定义一个评价函数f(n)，对当前的搜索状态进行评估，找出一个<em>最有希望</em>的节点来扩展。</p>
<h1 id="A算法"><a href="#A算法" class="headerlink" title="A算法"></a>A算法</h1><p>A算法是一种典型的启发式搜索算法，其基本思想为：定义一个评价函数f(n)，对当前的搜索状态进行评估，找出一个<em>最有希望</em>的节点来扩展。<br>评价函数的形式为：</p>
<pre><code>f(n)=g(n)+h(n)
</code></pre><p>其中n是被评价的结点。</p>
<p>为了了解f(n),g(n),h(n)的含义，我们先来介绍一下几个函数的定义：</p>
<ul>
<li>g*(n):从s（初始结点）到n的最短路径的耗散值（相当于一条路径的费用，代价）</li>
<li>h*(n):从n到g(目标结点)的最短路径的耗散值</li>
<li>f*(n)=g*(n)+h*(n)：从s经过n到g的最短路径的耗散值</li>
</ul>
<p>g(n)、 h(n)、 f(n)分别是g*(n)、 h*(n)、 f*(n)的估计值,是一种预测。A算法就是利用这种预测，来达到搜索的目的。<strong>它每次按照f(n)值的大小对OPEN表中的元素进行排序，f值小的放前面，f值大的放后面</strong>，这样每次在扩展结点时，总是选择当前f值最小的结点来优先扩展。</p>
<p>要想根据f对OPEN表中的结点排序，就需要计算f(n),g(n)和h(n)的值，根据搜索结果，g(n)就是初始结点s到结点n这条路径的耗散值；而h(n)依赖于启发信息，取决于具体的问题，通常称其为启发函数。</p>
<h2 id="A算法举例：八数码问题（Eight-Puzzle）"><a href="#A算法举例：八数码问题（Eight-Puzzle）" class="headerlink" title="A算法举例：八数码问题（Eight-Puzzle）"></a>A算法举例：八数码问题（Eight-Puzzle）</h2><p>八数码问题也称为九宫问题。在3×3的棋盘，摆有八个棋子，每个棋子上标有1至8的某一数字，不同棋子上标的数字不相同。棋盘上还有一个空格，与空格相邻的棋子可以移到空格中。要求解决的问题是：给出一个初始状态和一个目标状态，找出一种从初始转变成目标状态的移动棋子步数最少的移动步骤。</p>
<center><br><img src="http://i.imgur.com/XGw5X8W.png" alt=""><br></center>

<p>设评价函数f(n)形式如下：</p>
<pre><code>f(n)=d(n)+W(n)
</code></pre><p>其中，d(n)代表结点的深度，在单位耗散的情况下g(n)=d(n);取h(n)=W(n)表示以‘不在位’棋子个数作为启发函数的度量。如上图所示，初始状态和目标状态相比，初始状态中的数字“1”，“2”，“6”，“8”不在目标状态的位置上，所以初始状态的h值为4。</p>
<p>使用这种评价函数的搜索树如下所示，图中括弧中的数字表示该结点的评价函数值f;带圆圈的数字表示扩展结点的顺序。</p>
<center><br><img src="http://i.imgur.com/8DgjWdD.png" alt=""><br></center>

<p>根据目标结点L返回到s的指针，可得解路径为S(4)，B(4)，E(5)，I(5)，K(5)，L(5)。</p>
<h1 id="A-算法"><a href="#A-算法" class="headerlink" title="A*算法"></a>A*算法</h1><p>最佳图搜索算法A*(optimal search)，在A算法中，如果有h(n)&lt;=h*(n),则把这个算法称为A*算法。当问题有解时，<strong>A*算法一定能找到一条到达目标结点的最佳路径</strong>。例如，当h(n)恒为零时，满足条件，此时若取g为深度值，则算法等同于宽度优先算法，在<a href="http://buptldy.github.io/2016/01/04/%E5%9B%BE%E6%90%9C%E7%B4%A2/">无信息图搜索</a>中已提到过，宽度优先算法能够找到一条到目标结点的最短路径。</p>
<p>在使用A*算法求解问题时，定义的启发函数h，在满足A*的条件下，应尽可能的大一点，使其接近h*，这样才能提高搜索的效率，当h=h*时，搜索的效率最高。</p>
<p>对于八数码问题，取h(n)=W(n),容易看出，尽管我们不知道h*(n)具体为多少，但是它肯定至少要移动W(n)步才能达到目标状态，因为W(n)为此时和目标状态不相同的数字个数，所以有h(n)&lt;=h*(n),满足A*算法条件，所以上述A算法的例子也是A*算法。</p>
<h1 id="A-算法的改进"><a href="#A-算法的改进" class="headerlink" title="A*算法的改进"></a>A*算法的改进</h1><p>在A*算法中，扩展一个节点时，对已经在OPEN表或CLOSED表中的子节点，要调整指针，花时间和精力。如果在扩展节点n时，就已经找到了从根节点开始到它的最优路径，则不必调整指针, 可以大大提高效率。如果满足单调性限制，则可实现此愿望。</p>
<p>如果对每一个节点$n_i$以及它的后继节点$n_j$，满足：</p>
<p>$$h(n_i) - h(n_j) ≤ k(n_i,n_j)$$</p>
<p>则称启发式函数满足单调性限制。</p>
<p>如果A*满足单调性限制，则当它选择节点n扩展时，就已经发现了通向节点n的最佳路径,则不必进行结点的指针修正操作，因而改善了A*的效率。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Depth-First-Search and Breadth-First-Search]]></title>
      <url>http://buptldy.github.io/2016/01/04/2016-01-04-Depth-First-Search%20and%20Breadth-First-Search/</url>
      <content type="html"><![CDATA[<h1 id="图搜索策略简介"><a href="#图搜索策略简介" class="headerlink" title="图搜索策略简介"></a>图搜索策略简介</h1><p>之前所说的<a href="http://buptldy.github.io/2016/01/03/%E5%9B%9E%E6%BA%AF%E7%AD%96%E7%95%A5/">回溯搜索策略</a>是只保留了从初始状态到当前状态的一条路径，优点是节省存储空间，缺点是被回溯掉的已经搜索过的部分不能再使用。 与之相对应的是，将所有搜索过的状态都记录下来的搜索方法称为“图搜索”。<br><a id="more"></a><br>图搜索实际上是从一个隐含图中生成一部分确实含有一个目标结点的显示表示子图的搜索过程。</p>
<p>扩展一个结点：应用规则到已有结点上，生成其<strong>所有</strong>后继扩展结点的过程。扩展节点可使定义的隐含图生成为显式表示的状态空间图。</p>
<h1 id="2-图搜索算法"><a href="#2-图搜索算法" class="headerlink" title="2. 图搜索算法"></a>2. 图搜索算法</h1><p>图搜索中的两个表：</p>
<pre><code>- OPEN表：存放已经生成但未扩展的节点，最初只含初始结点
- CLOSED表：存放已经扩展的节点，其实设置为空表
</code></pre><p>图搜索过程：</p>
<center><br><img src="http://i.imgur.com/7EvIIuk.png" alt=""><br></center>

<p><strong>算法结束后，将生成一个图G，称为搜索图。同时由于每个节点都有一个指针指向父节点，这些指针指向的节点构成G的一个支撑树，称为搜索树。</strong></p>
<p>从目标节点开始，将指针指向的状态回串起来，即找到一条解路径。</p>
<p>例子：<br>下图中S结点为初始结点，把结点S放入OPEN表中，从S结点开始扩展，生成S结点的所有后继结点｛1，2，3｝，S结点已扩展，加入CLOSED表中，继续这一过程。</p>
<center><br><img src="http://i.imgur.com/2g5AKHd.png" alt=""><br></center><br>从上述结果可以看出，最终OPEN表中的结点都是搜索树中的端结点，而CLOSED表中的结点为搜索树中的非端结点。<br><br>对于搜索过程中3中的d）步骤，如果要搜索的隐含图是一棵树，则在它上一步中生成后继结点不可能是以前生成过的，即生成的后继结点不可能出现在OPEN，CLOSED表中，此时搜索图就是搜索树，因此不必进行指针的修改操作。如果要搜索的隐含图不是一棵树，则有可能存在生成的后继结点中的某一结点$m_k$已经在OPEN，CLOSED表中存在,<strong><em>这意味着此时又发现了达到$m_k$的新通路</em></strong>，这样就需要比较不用路径到达点$m_k$的代价，将指针修改到代价最小的路径上。<br><br># 无信息图搜索过程<br><br>无信息搜索过程是在图搜索算法过程中第3点的e）步中排列OPEN表中结点的顺序时，没有使用与问题有关的知识，任意排列，通常有<em>深度优先</em>和<em>宽度优先</em>两种排列方式。<br><br>- 深度优先<br><br>所谓深度优先搜索就是在每次扩展一个结点时，选择到目前为止深度最深的结点优先扩展。在算法中的实现为：<br><br>把后继结点中不在OPEN或CLOSED中的结点放在OPEN表的最前面，是深度大的结点优先扩展<br><br>因为新扩展出来的结点为子节点，子节点的深度要大于父节点的深度。一般情况下，深度优先搜索不但不能保证找到最优解，也不一定能保证找到解，这取决与问题的状态空间，如果问题的状态空间无限，可能会陷入“深渊”，而找不到解，因此也需要限制搜索的深度。<br><br>举例：<br><br>问题状态图G2如下图所示：<br><br><center><br><img src="http://i.imgur.com/B3dYJla.jpg" alt=""><br></center>

<p>根据深度优先原则，搜索过程为：（A–&gt;B–&gt;C–&gt;E–&gt;D–&gt;F–&gt;G）</p>
<p><center><br><img src="http://i.imgur.com/CrjoNGl.jpg" alt=""><br></center></p>
<ul>
<li>宽度优先</li>
</ul>
<p>宽度优先搜索与深度优先相反，每次选择深度最浅的结点优先扩展，实现方法为：</p>
<p>把后继结点中不在OPEN或CLOSED中的结点放在OPEN表的后面</p>
<p>当问题有解时，宽度优先算大一定能找到解，且在每段路径为单位代价时能找到最优解。</p>
<p>根据宽度优先原则，对图G2的搜索过程为：（A–&gt;B–&gt;C–&gt;E–&gt;F–&gt;D–&gt;G）</p>
<p><center><br><img src="http://i.imgur.com/LLE7pad.jpg" alt=""><br></center></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>&lt;&lt;人工智能&gt;&gt;.马少平，朱小燕编著</p>
<p>图的遍历之 深度优先搜索和广度优先搜索:<a href="http://www.cnblogs.com/skywang12345/p/3711483.html" title="图的遍历之 深度优先搜索和广度优先搜索" target="_blank" rel="external">http://www.cnblogs.com/skywang12345/p/3711483.html</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Backtracking Strategies]]></title>
      <url>http://buptldy.github.io/2016/01/03/2016-01-03-Backtracking%20Strategies/</url>
      <content type="html"><![CDATA[<h1 id="回溯策略（backtracking）简介"><a href="#回溯策略（backtracking）简介" class="headerlink" title="回溯策略（backtracking）简介"></a>回溯策略（backtracking）简介</h1><p>回溯法采用试错的思想，它尝试分步的去解决一个问题。在分步解决问题的过程中，当它通过尝试发现现有的分步答案不能得到有效的正确的解答的时候，它将取消上一步甚至是上几步的计算，再通过其它的可能的分步解答再次尝试寻找问题的答案。回溯法通常用最简单的递归方法来实现，在反复重复上述的步骤后可能出现两种情况：<br><a id="more"></a>    - 找到一个可能存在的正确的答案</p>
<ul>
<li>在尝试了所有可能的分步方法后宣告该问题没有答案</li>
</ul>
<p>在最坏的情况下，回溯法会导致一次复杂度为指数时间的计算。</p>
<h1 id="回溯策略应用：四皇后问题（Four-queens-puzzle）"><a href="#回溯策略应用：四皇后问题（Four-queens-puzzle）" class="headerlink" title="回溯策略应用：四皇后问题（Four queens puzzle）"></a>回溯策略应用：四皇后问题（Four queens puzzle）</h1><p>四皇后问题：在一个国际象棋中的 的棋盘上放置4个皇后， 为了使其中的任何2个皇后都不能相互“攻击”，希望寻求4个皇后的安全放置位置。 该问题的不能相互“攻击”相当于要求任意两个皇后不能在同一行、同一列或同一斜线上。</p>
<p>用回溯策略解决这一问题：</p>
<ul>
<li>首先放第一颗棋子</li>
</ul>
<center><br><img src="http://i.imgur.com/775UH8f.png" alt=""><br></center>

<ul>
<li><p>在符合规则的条件下摆放第二课棋子</p>
<center><br><img src="http://i.imgur.com/OYEeASd.png" alt=""><br></center>
</li>
<li><p>在规则下未找到解时，回溯</p>
</li>
</ul>
<center><br><img src="http://i.imgur.com/wIxeLH5.png" alt=""><br></center>

<ul>
<li>根据回溯策略，不断的试探，最终找到解为：</li>
</ul>
<center><br><img src="http://i.imgur.com/f1QnRh3.png" alt=""><br></center>

<h1 id="回溯搜索中知识的利用"><a href="#回溯搜索中知识的利用" class="headerlink" title="回溯搜索中知识的利用"></a>回溯搜索中知识的利用</h1><p>在回溯策略中，可以通过引入一些与问题有关的信息来加快搜索解的速度。对与N皇后问题来说，引入信息的基本思想是：</p>
<p>尽可能选取划去对角线上位置数最少的</p>
<p><center><br><img src="http://i.imgur.com/JCaTEEu.png" alt=""><br></center><br>可以想象，如果把一个皇后放在棋盘的某个位置后，它所影响的棋盘位置数少，那么给以后放置皇后剩下的余地就越大，找到解的可能性也越大。</p>
<h1 id="回溯算法存在的问题及解决方案"><a href="#回溯算法存在的问题及解决方案" class="headerlink" title="回溯算法存在的问题及解决方案"></a>回溯算法存在的问题及解决方案</h1><p>存在的问题：</p>
<pre><code>- 某一个分支具有无穷个状态，算法可能落入“深渊”，永远不能回溯
- 某一个分支上具有环路，搜索在环路中一直进行，同样不能回溯
</code></pre><p>解决方案：</p>
<pre><code>- 对搜索深度进行限制，当当前状态的深度达到了限制深度时，算法将进行回溯
- 记录从初始状态到当前状态的路径，如果出现过此路径，表明出现环路，算法回溯
</code></pre><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>维基百科回溯法：<a href="https://zh.wikipedia.org/wiki/%E5%9B%9E%E6%BA%AF%E6%B3%95" title="维基百科：回溯法" target="_blank" rel="external">https://zh.wikipedia.org/wiki/%E5%9B%9E%E6%BA%AF%E6%B3%95</a></p>
<p>四皇后问题：<a href="http://jpkc.onlinesjtu.com/CourseShare/DataStructure/FlashInteractivePage/exp7.htm" title="四皇后问题" target="_blank" rel="external">http://jpkc.onlinesjtu.com/CourseShare/DataStructure/FlashInteractivePage/exp7.htm</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Asymptotic Notation and Master theorem]]></title>
      <url>http://buptldy.github.io/2015/12/29/2015-12-29-Asymptotic%20Notation%20and%20Master%20theorem/</url>
      <content type="html"><![CDATA[<h1 id="算法渐进记号"><a href="#算法渐进记号" class="headerlink" title="算法渐进记号"></a>算法渐进记号</h1><ul>
<li><p>Big $\Theta$ ：$\Theta$ 记号渐进的给出一个函数的上界和下届，表示同阶的函数簇。</p>
</li>
<li><p>Big $O$：表示一个函数的渐进上界，用来限制算法的最坏情况运行时间。</p>
<a id="more"></a></li>
<li><p>Big $\Omega$：表示一个函数的渐进下界，算法运行的最好情况。</p>
</li>
<li><p>Litter $o$: 和big $O$ 定义相似，区别主要是 big$O$ 提供的上界可能和函数是同阶的，litter $o$ 表示非渐进紧确的上界。</p>
</li>
</ul>
<h1 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h1><center><br><img src="http://i.imgur.com/GgsO2DX.png" alt=""><br></center>

<h1 id="渐进记号与函数阶数的关系"><a href="#渐进记号与函数阶数的关系" class="headerlink" title="渐进记号与函数阶数的关系"></a>渐进记号与函数阶数的关系</h1><p>其中$a，b$分别为函数$g(n)，f(n)$的阶数</p>
<p><center><br><img src="http://i.imgur.com/wUDSXQf.png" alt=""><br></center><br>通常Big $\Theta$ 用来描述算法的最好和最坏的运行时间，Big $O$描述算法的最坏运行时间，Big $\Omega$描述算法的最好运行时间，经常使用的是Big $O$， 用来衡量算法的时间复杂度和空间复杂度。</p>
<h1 id="主定理-master-theorem-求解递归式"><a href="#主定理-master-theorem-求解递归式" class="headerlink" title="主定理(master theorem)求解递归式"></a>主定理(master theorem)求解递归式</h1><p>假设有递推关系式</p>
<p>$$T(n) = aT\left(\frac{n}{b}\right) + f(n)$$</p>
<p>其中$ a \geq 1 \mbox{, } b &gt; 1$，n为问题规模，a为递推的子问题数量，n/b为每个子问题的规模（假设每个子问题的规模基本一样），f(n)为递推以外进行的计算工作，包含了问题分解和子问题合并的代价。</p>
<ul>
<li>情况一<br>如果存在常数$\epsilon &gt; 0$，有$f(n) = O\left( n^{\log_b (a) - \epsilon} \right)$，并且是多项式意义上的小于，那么</li>
</ul>
<p>$$T(n) = \Theta\left( n^{\log_b a} \right)$$</p>
<ul>
<li>情况二<br>如果存在常数k ≥ 0，有$f(n) = \Theta\left( n^{\log_b a} \log^{k} n \right)$那么</li>
</ul>
<p>$$T(n) = \Theta\left( n^{\log_b a} \log^{k+1} n \right)$$</p>
<ul>
<li>情况三<br>如果存在常数$\epsilon &gt; 0$，有$f(n) = \Omega\left( n^{\log_b (a) + \epsilon} \right)$，并且是多项式意义上的大于，同时存在常数c &lt; 1以及充分大的n，满足</li>
</ul>
<p>$$a f\left( \frac{n}{b} \right) \le c f(n)$$</p>
<p>那么</p>
<p>$$T\left(n \right) = \Theta \left(f \left(n \right) \right)$$</p>
<p>简单举例：</p>
<p>$$T(n) = 9T\left(\frac{n}{3}\right) + n$$</p>
<p>对这个递归式，有a=9,b=3,f(n)=n,因此有$n^{log_b a}=n^2&gt;n$,所以复杂度为：</p>
<p>$$T(n)=\Theta(n^2)$$</p>
]]></content>
    </entry>
    
  
  
</search>
